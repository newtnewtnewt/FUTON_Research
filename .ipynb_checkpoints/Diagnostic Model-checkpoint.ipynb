{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  FUTON Model MDP + Q-Learning Creation Script\n",
    "#  A Research Project conducted by Noah Dunn \n",
    "###\n",
    "\n",
    "# Import the standard tools for working with Pandas dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import ctypes\n",
    "# Pickle provides easy Object Serialization for quick read + writes of data\n",
    "import pickle\n",
    "# Vector Quantization for Determining Cluster Centers\n",
    "from scipy.cluster.vq import vq\n",
    "# Skikit offers a solution to perform K-Means++ clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# Scipy provides a library to execute Z-Score Normalization\n",
    "from scipy.stats import zscore\n",
    "# We want to do type hinting for API clarification\n",
    "from typing import *\n",
    "# Import the MDP toolbox that contains a method for conducting Q-Learning\n",
    "# Tool can be found here: https://github.com/sawcordwell/pymdptoolbox\n",
    "# Documentation for the tool can be found here \n",
    "import mdptoolbox\n",
    "# Itertools provides an easy way to perform Cartesian product on multiple sets\n",
    "from itertools import product as cartesian_prod\n",
    "\n",
    "\n",
    "### Some repetitive type hinting\n",
    "int_matrix2D = np.array\n",
    "float_matrix2D = np.array\n",
    "int_matrix3D = np.array\n",
    "float_matrix3D = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Data File that will be used to conduct the experiments\n",
    "patientdata:pd.DataFrame = pd.read_csv(\"G:/MIMIC-ALL/MIMIC-PATIENTS/patient_data_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#  An MDP, or Markov Decision Process is used to model relationships between various states and actions.\n",
    "#  A state can be thought of in medical solution as a patient's diagnosis based on current vitals and state of being. \n",
    "#  An action can be thought of as a change in current diagnosis based on one of those vitals.\n",
    "#  The inspirations for the bulk of this code came from Komorowksi's AI Clinician which can be found \n",
    "#  here: https://github.com/matthieukomorowski/AI_Clinician/blob/master/AIClinician_core_160219.m\n",
    "###\n",
    "\n",
    "###\n",
    "# Begin by establishing some global variables for use in the MDP creation\n",
    "###\n",
    "mdp_count:int = 500            # The number of repititions we want/count of MDPs we need to create \n",
    "clustering_iter:int = 32       # The number of times clustering will be conducted\n",
    "cluster_sample:float = 0.25    # Proportion of the data used for clustering\n",
    "gamma:float = 0.99             # How close we desire clusters to be in similarity (Percentage)\n",
    "transition_threshold:int = 5   # The cutoff value for the transition matrix\n",
    "final_policies:int = 1         # The number of policies we would like to end up with\n",
    "state_count:int = 750          # The number of distinct states\n",
    "action_count:int = 5           # Number of actions per state (reccommended 2 to 10)\n",
    "crossval_iter:int = 5          # Number of crossvalidation runs (Default is 80% Train, 20% Test)\n",
    "# This will be replaced by the loop index at some point (Iterations of all the models)\n",
    "loop_index:int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Data structures to hold our interim data\n",
    "###\n",
    "\n",
    "###\n",
    "# The 30 and 15 constants here are used solely for the purpose of allotting enough space\n",
    "# to save information later on down in the pipeline\n",
    "## \n",
    "model_data:np.ndarray = np.empty((mdp_count*2, 30,))\n",
    "model_data[:] = np.nan\n",
    "\n",
    "bestmodels_data:np.ndarray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The extract_init_column_data function takes 6 arguments: \n",
    "# \n",
    "# patient_data:   The full DataFrame of all the patient data, raw and unfiltered\n",
    "# id_column:      The name of the column in the dataframe containing the patient IDs\n",
    "# binary_columns: A list containing the names of all the columns that are binary data (only 0s and 1s)\n",
    "# normal_columns: A list containing the names of all the columns that are regular data (require no function transformations)\n",
    "# log_columns:    A list containing the names of all the columns that are logarithmic data (Log has already been applied)\n",
    "# debug_flag:     A boolean flag that indicates whether or not print statements will be executed\n",
    "# \n",
    "# The function has 6 return values:\n",
    "#\n",
    "# colbin:       The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:      The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:       The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw:    The DataFrame containing the data of all desired columns and their values\n",
    "# id_count:     The total number of IDs to be used  \n",
    "# icu_ids:      The ids of all patients to be used\n",
    "# patient_idxs: \n",
    "\"\"\"\n",
    "\n",
    "def extract_init_column_data(patient_data:pd.DataFrame, id_column:str='icustayid', binary_columns:List[str]=None, normal_columns:List[str]=None,\n",
    "                            log_columns:List[str]=None, debug_flag:bool=False) -> (List[str], List[str], List[str], \n",
    "                                                                                   pd.DataFrame, pd.DataFrame, int, List[List[int]]):\n",
    "\n",
    "    # Grab list of unique patient ICU stay IDs\n",
    "    icu_ids:int = patient_data[id_column].unique()\n",
    "    # Number of patients to be used for states\n",
    "    id_count:int = icu_ids.size\n",
    "    if debug_flag:\n",
    "        print(id_count)\n",
    "\n",
    "    # Create a data structure to save patient data for report\n",
    "    patient_idxs:List[List[int]]= np.zeros((id_count, mdp_count,), dtype=np.int64)\n",
    "\n",
    "    # All our columns are broken up into 3 distinct categories:\n",
    "    # 1. Binary values (0 or 1)\n",
    "    # 2. Standard Ranges (Plain old Integers + Decimals)\n",
    "    # 3. Logarthmic Values (columnvalue = log(columnvalue))\n",
    "    colbin:List[str] = []\n",
    "    colnorm:List[str] = [] \n",
    "    collog:List[str] = []\n",
    "    \n",
    "    # Enables custom column selection\n",
    "    if binary_columns == None:\n",
    "        colbin = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag']\n",
    "    else:\n",
    "        colbin = binary_columns\n",
    "    \n",
    "    if normal_columns == None:\n",
    "        colnorm = ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', 'qSOFA'];\n",
    "    else:\n",
    "        colnorm = normal_columns\n",
    "    if log_columns == None:\n",
    "        collog = ['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly'];\n",
    "    else:\n",
    "        collog = log_columns\n",
    "    # Create seperate dataframes for each of the columns\n",
    "    colbin_df:pd.DataFrame = patient_data[colbin]\n",
    "    colnorm_df:pd.DataFrame = patient_data[colnorm]\n",
    "    collog_df:pd.DataFrame = patient_data[collog]\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Let's make sure we have what we need\n",
    "        print(colbin_df, \"\\n\", colnorm_df, \"\\n\", collog_df)\n",
    "    # Rearrange the dataframe in order of binary, normal, and log data from left to right\n",
    "    MIMIC_raw:pd.DataFrame = pd.concat([colbin_df, colnorm_df, collog_df], axis=1)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_raw) \n",
    "    return colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs = extract_init_column_data(patientdata, debug_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_zscores function takes 4 arguments: \n",
    "# \n",
    "# colbin:    The list of all columns representing binary values\n",
    "# colnorm:   The list of all columns representing normal values\n",
    "# collog:    The list of all columns representing log values\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "#\n",
    "# and returns \n",
    "# colbin:    The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:   The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:    The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "\"\"\"\n",
    "\n",
    "def construct_zscores(colbin:List[str], colnorm:List[str], collog:List[str], MIMIC_raw:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # We want a Z-Score for every item. This a measure of variance to see how far a value is from the mean\n",
    "\n",
    "    # We need to normalize binaries to -0.5 and 0.5 for later use\n",
    "    MIMIC_zscores:pd.DataFrame = MIMIC_raw\n",
    "\n",
    "    # No need for the zscore algorithm here, -0.5 and 0.5 suffice\n",
    "    MIMIC_zscores[colbin] = MIMIC_zscores[colbin] - 0.5\n",
    "\n",
    "    # Recall these columns are logarithmic, so they needed converted back for proper Z-Scoring (+ 0.1 to avoid log(0))\n",
    "    # Note that log(0.1) is essentially 0, Mathematically proved\n",
    "    \n",
    "    # zscore is the function pulled from the stats library in the initial import calls\n",
    "    MIMIC_zscores[collog] = np.log(MIMIC_zscores[collog] + 0.1).apply(zscore)\n",
    "\n",
    "    # Normal column requires no modifications. Z-Scores are calculated as normal\n",
    "    MIMIC_zscores[colnorm] = MIMIC_zscores[colnorm].apply(zscore)\n",
    "    print(MIMIC_zscores)\n",
    "\n",
    "    # We want the Re_Admission and fluid intake scaled Similarly to the other variables\n",
    "    MIMIC_zscores['re_admission'] = np.log(MIMIC_zscores['re_admission'] + 0.6)\n",
    "    # Apply a scalar to fluid intake\n",
    "    MIMIC_zscores['input_total'] = 2 * MIMIC_zscores['input_total']\n",
    "    \n",
    "    return MIMIC_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag  \\\n",
      "0         -0.5       0.5           -0.5          -0.5        0.5      -0.5   \n",
      "1         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "2         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "3         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "4         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "...        ...       ...            ...           ...        ...       ...   \n",
      "238325    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238326    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238327    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238328    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238329    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "\n",
      "             age  Weight_kg       GCS        HR  ...       BUN  Creatinine  \\\n",
      "0      -0.973100  -0.318414 -0.687933 -0.569736  ... -0.545751   -0.214781   \n",
      "1      -0.973100  -0.169199 -0.687933 -0.491800  ... -1.189188   -0.679473   \n",
      "2      -0.973100  -0.169199 -0.640362 -0.567776  ... -1.189188   -0.679473   \n",
      "3      -0.973100  -0.169199 -0.402506 -0.749137  ... -1.189188   -0.679473   \n",
      "4      -0.973100  -0.169199  0.510859  0.630189  ... -1.189188   -0.679473   \n",
      "...          ...        ...       ...       ...  ...       ...         ...   \n",
      "238325 -2.467202  -0.361999  0.739200  0.191981  ... -1.909177   -0.494229   \n",
      "238326 -2.467202   0.228708  0.739200 -0.072708  ... -1.909177   -0.494229   \n",
      "238327 -2.467202  -0.403020  0.739200 -0.205053  ... -1.909177   -0.494229   \n",
      "238328 -2.467202  -0.403020  0.739200 -1.102056  ... -1.909177   -0.494229   \n",
      "238329 -2.467202  -0.403020  0.739200 -0.695219  ... -1.909177   -0.494229   \n",
      "\n",
      "            SGOT      SGPT  Total_bili       INR  input_total  input_4hourly  \\\n",
      "0       0.977035  1.112233    0.334065 -0.692745     0.410209       0.096264   \n",
      "1      -0.268657 -0.443987    1.387969 -0.214109     0.412665       0.240649   \n",
      "2      -0.268657 -0.443987    1.387969  0.200505     0.415103       0.240649   \n",
      "3      -0.268657 -0.443987    1.387969 -0.692745     0.417521       0.240649   \n",
      "4      -0.268657 -0.443987    1.387969 -0.692745     0.418963       0.096264   \n",
      "...          ...       ...         ...       ...          ...            ...   \n",
      "238325  0.901093  0.140315   -1.185843 -0.962914     0.071129      -1.521065   \n",
      "238326 -0.892408 -0.604775   -1.185843 -0.692745     0.071129      -1.521065   \n",
      "238327  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "238328  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "238329  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "\n",
      "        output_total  output_4hourly  \n",
      "0           0.757770        0.733745  \n",
      "1           0.771262        0.731161  \n",
      "2           0.790024        0.847813  \n",
      "3           0.803524        0.767750  \n",
      "4           0.812873        0.670986  \n",
      "...              ...             ...  \n",
      "238325      0.530930        0.743862  \n",
      "238326      0.567943        0.789867  \n",
      "238327      0.567943       -1.852903  \n",
      "238328      0.587895        0.637420  \n",
      "238329      0.610678        0.701156  \n",
      "\n",
      "[238330 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "MIMIC_zscores:pd.DataFrame = construct_zscores(colbin, colnorm, collog, MIMIC_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The split_training_and_test function takes 4 arguments: \n",
    "# \n",
    "# id_count:     Total number of patient IDs to be used\n",
    "# icu_ids:      The IDs for all ICU visits\n",
    "# debug_flag:   A flag that enables/disables print statements \n",
    "# save_to_file: A flag that saves our deisred\n",
    "# \n",
    "# and returns 3 values:\n",
    "# \n",
    "# train_ids:  The ICU visit IDs to be used in the training set\n",
    "# test_ids:   The ICU visit IDs to be used in the testing set\n",
    "# train_flag: A list over all of MIMIC_raw that marks training rows as True\n",
    "\"\"\"\n",
    "\n",
    "def split_training_and_test(id_count:int, icu_ids:pd.DataFrame, debug_flag:bool, save_to_file:bool) -> (\n",
    "                                    pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    ### The main loop to generate all possible models\n",
    "\n",
    "    num_rows:int = id_count  # Total Number of Patients to divy data up\n",
    "    testing_flag:int = 1     # The random number we use to identify a patient used for testing\n",
    "\n",
    "    # This will allow the loop over a large about of models\n",
    "\n",
    "    train_ids:List[int] = []       # A list containing all training ids from the icu_ids list\n",
    "    test_ids:List[int] = []         # A list containing all testing ids from the icu_ids list\n",
    "\n",
    "    # We want approximate 20% test, 80% train, so we random numbers 1-5\n",
    "    # 1s Represent data points that will be used to test, 2-5 will be used to train\n",
    "    group_ids:pd.DataFrame = pd.DataFrame([int(np.floor(crossval_iter * np.random.random() + 1)) for i in range(1, id_count + 1)])\n",
    "    icu_pair_set:pd.DataFrame = pd.concat([pd.DataFrame(icu_ids), group_ids], axis=1, sort=False)\n",
    "    icu_pair_set.columns = ['id', 'fil_val']\n",
    "    train_ids =  icu_pair_set[icu_pair_set['fil_val'] != testing_flag]\n",
    "    test_ids = icu_pair_set[icu_pair_set['fil_val'] == testing_flag]\n",
    "\n",
    "    # We want to ensure that the testing patients + training patients = total patients\n",
    "    if (train_ids['id'].size + test_ids['id'].size) != id_count:\n",
    "        print(\"The testing and training set do not add up to the total set\")\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Percentage for testing should be about 20%, Training about 80%\n",
    "        print(\"Testing Percentage: \" + str((test_ids['id'].size / id_count)))\n",
    "        print(\"Training Percentage: \" + str((train_ids['id'].size / id_count)))\n",
    "\n",
    "    # After grabbing all the IDs, we want to flag all the rows that are train or test\n",
    "    train_flag:pd.DataFrame = patientdata['icustayid'].isin(train_ids['id'])\n",
    "    # Save the training ids for later if we need to use them\n",
    "    if save_to_file:\n",
    "        # Temporarily write train_flag for later use DELETE LATER\n",
    "        with open('sample_train.txt', 'wb') as fp:\n",
    "            pickle.dump(train_flag, fp)\n",
    "            \n",
    "    return train_ids, test_ids, train_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Percentage: 0.19959931044122442\n",
      "Training Percentage: 0.8004006895587755\n"
     ]
    }
   ],
   "source": [
    "train_ids, test_ids, train_flag = split_training_and_test(id_count, icu_ids, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 8 arguments: \n",
    "# \n",
    "# train_flag:       The dataframe representing whether or not a row is used in training or not\n",
    "# MIMIC_zscores:    The dataframe representing the zscores of the dataset\n",
    "# debug_flag:       A flag that determines if print statements are executed\n",
    "# save_flag:        A flag that determines if the training_zscores are saved\n",
    "# patient_data:     The raw MIMIC dataframe\n",
    "# bloc_name:        The name of the column that dictates a 'bloc' of time within an ICU visit\n",
    "# id_name:          The name of the column that dictates an ICU stay's ID\n",
    "# death_name:       The name of the column that dictates a patient's life status \n",
    "# \n",
    "# and returns 7 values:\n",
    "# \n",
    "# train_zscores:   The portion of MIMIC_zscores that is in the training set\n",
    "# test_zscores:    The portion of MIMIC_zscores that is in the testing set\n",
    "# train_blocs:     The rows of the MIMIC dataset that is in the training set\n",
    "# test_blocs:      The rows of the MIMIC dataset that is in the testing set\n",
    "# train_id_list:   The list of all IDs in the training set\n",
    "# train_90d:       A flag indicating if a given patient died in the training set after 90d\n",
    "# test_90d:        A flag indicating if a given patient died in the testing set after 90d\n",
    "\"\"\"\n",
    "\n",
    "def zscores_for_train_and_test(train_flag:pd.DataFrame, MIMIC_zscores:pd.DataFrame, debug_flag:bool, save_flag:bool,\n",
    "                               patient_data:pd.DataFrame, bloc_name:str, id_name:str, death_name:str) -> (pd.DataFrame,\n",
    "                               pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    # Seperate the Z-Scores for the training set and the testing set\n",
    "    train_zscores:pd.DataFrame = MIMIC_zscores[train_flag]\n",
    "    test_zscores:pd.DataFrame = MIMIC_zscores[~train_flag]\n",
    "\n",
    "    # Validate all data is selected\n",
    "    if(train_zscores.size + test_zscores.size != MIMIC_zscores.size):\n",
    "        print(\"The Z-Scores are all evenly distributed\")\n",
    "\n",
    "\n",
    "    # The blocs of relevance in order based on the train and test set\n",
    "    # These will be used to build relevant data frames later down\n",
    "    train_blocs:List[int] = patientdata[train_flag][bloc_name]\n",
    "    test_blocs:List[int] = patientdata[~train_flag][bloc_name]\n",
    "\n",
    "    # Doing the same with the patient ids\n",
    "    train_id_list:pd.DataFrame = patientdata[train_flag][id_name]\n",
    "    # We modify a column later to use for the test_id_list, not necesssary here.\n",
    "\n",
    "    # Grabbing the boolean values for the patients who died within 90 days in the training set\n",
    "    train_90d:pd.DataFrame = patientdata[train_flag][death_name]\n",
    "    test_90d:pd.DataFrame = patientdata[~train_flag][death_name]\n",
    "    # Next, we want to sample the existing training set to only pick cluster_sample percent to use\n",
    "    \n",
    "    # We want to flag all the data points in the train_zscores set that will be used to create the MDP\n",
    "    \n",
    "    # Note: len(train_zscores.index) is the fastest way to get the number of rows in a dataframe in pandas\n",
    "\n",
    "    # Additional Note: np.floor(np.random.random() + cluster_sample) is a computationally speedy way to get an approximate\n",
    "    # percentage sample from a proportion value (cluster_sample). If cluster sample is 0.25, approximately 25% of the values\n",
    "    # will be flagged as a 1, making it into the sample training set\n",
    "    sample_train_flags:List[bool] = [bool(np.floor(np.random.random() + cluster_sample)) for i in range(len(train_zscores.index))]\n",
    "    \n",
    "    if debug_flag:\n",
    "        # It's good to know how much of the data was selected as sample\n",
    "        print(\"Proportion of Train Data used for the Sample: \" + str(sample_train_flags.count(True)/len(sample_train_flags)))\n",
    "\n",
    "    # The actual set to use\n",
    "    sample_train_set:pd.DataFrame = train_zscores[sample_train_flags]\n",
    "    if save_flag:\n",
    "        # Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('train_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(train_zscores, fp)\n",
    "    return train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, train_90d, test_90d, sample_train_flags, sample_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Train Data used for the Sample: 0.25102186261974135\n"
     ]
    }
   ],
   "source": [
    "train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, train_90d, test_90d, sample_train_flags, sample_train_set = \\\n",
    "zscores_for_train_and_test(train_flag, MIMIC_zscores, True, True, patientdata, 'bloc', 'icustayid', 'mortality_90d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 5 arguments: \n",
    "# data_set:               The training dataset used to generate the various clusters\n",
    "# state_count:            Total number of clusters desired in the end result\n",
    "# num_loops_per_iter:     The number of loops ran per 1 iteration to approximate clusters\n",
    "# max_num_iter:           Number of iterations to run the algorithm\n",
    "# debug_flag:             True indicates print statements are included, False indicates no prints\n",
    "# regenerate_flag:        True indictates the kmeans will be run again. False means the cached/previous file value will be used\n",
    "# \n",
    "# and returns 4 values:\n",
    "# \n",
    "# cluster_values:         A value generated based on the Z-Scores of all the patient's data relative to each other\n",
    "# cluster_labels:         A number generated for the correspond state -> value relationship. Range is 0 to state_count\n",
    "# train_zscores:          The Z-Scores of the training set as created earlier\n",
    "# closest_clusters:       A list representing which state each patient datapoint is closest to\n",
    "# \n",
    "\"\"\"\n",
    "def kmeans_cluster_calculations(data_set:pd.DataFrame, state_count:int=500, num_loops_per_iter:int=10000, max_num_iter:int=32,\n",
    "                               debug_flag:bool=True, regenerate_flag=False) -> (List[List[np.float64]], List[int], pd.DataFrame, np.ndarray):\n",
    "    # In order to prepare a proper set of states, we want to use k-means clustering to group various patients into \n",
    "    # distinct states based on Z-Scores\n",
    "\n",
    "    # K-Means or K-Means++ is a technique used to condense very diverse and sparse data into similar groups called 'clusters'\n",
    "    # The K-means algorithm will create k clusters from N data points. In the case of this research,\n",
    "    # the algorithm divides patients into groups that have similar data (age, blood pressure, etc..) and creates a faux 'point'\n",
    "    # at the center of that particular clustering of data\n",
    "\n",
    "    # The KMeans takes three 'settings' arguments\n",
    "    # 1. n_clusters: The number of clusters (later to be used as states), that we desire the algorithm to produce\n",
    "    # this value has been preset to state_count which is 750\n",
    "    # 2. max_iter: How many times each round of k-means clustering will make adjustments, set at 10,000 in my case\n",
    "    # 3. n_init: The number of max_iter batches that will be conducted in a row. The best of these will be chosen\n",
    "    # and saved in the variable clusters_models\n",
    "    if regenerate_flag:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(sample_train_set)\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('cluster_labels.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.labels_, fp)\n",
    "        with open('cluster_centers.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.cluster_centers_, fp)\n",
    "        if debug_flag:\n",
    "            print(clusters_models.labels_)\n",
    "            print(clusters_models.cluster_centers_)\n",
    "    \n",
    "\n",
    "    # Read these values back in from being saved to file\n",
    "    cluster_values:List[List[np.float64]] = []\n",
    "    cluster_labels:List[int] = [] \n",
    "    train_zscores:pd.DataFrame = []\n",
    "\n",
    "    with open ('cluster_centers.txt', 'rb') as fp:\n",
    "        cluster_values = pickle.load(fp)\n",
    "    with open ('cluster_labels.txt', 'rb') as fp:\n",
    "        cluster_labels = pickle.load(fp)\n",
    "    with open ('train_zscores.txt', 'rb') as fp:\n",
    "        train_zscores = pickle.load(fp)\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(cluster_values, \"\\n\", \"Dimensions: \", len(cluster_values),\" x \", len(cluster_values[0]), \"\\n\", train_zscores)\n",
    "    \n",
    "    # We now want to use the clusters to determine their nearest real data point neighbors\n",
    "    # As a visual of this. Suppose we have 4 flags of different colors scattered over a park. The K-Means++ algorithm\n",
    "    # is what planted the flags in the middle of groups of people that are similar. The KNN Search (K nearest neighbor search)\n",
    "    # can be used in MatLab as a simple point finder instead of as a more complicated Supervised Learning algorithm. In Python \n",
    "    # we can make use of the Vector Quanization (vq) package to assign each point to a centroid\n",
    "    \n",
    "    closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "\n",
    "    # Check to make sure each cluster has a value\n",
    "    print(len(closest_clusters[0]))\n",
    "\n",
    "    # As an aside, closest_clusters[1] contains the distance between each point's values (in this case 50 of them)\n",
    "    # and their closest cluster's values.\n",
    "    # Ex: If a point is [1, 1, 1] and it's closest cluster is the point [3, 3, 3]  closest_clusters[1] would contain the vector\n",
    "    # [abs(3 - 1), abs(3 - 1), abs(3 - 1)] or [2, 2, 2]\n",
    "\n",
    "    # Validate that all the points are in the range 0-749 (since there are only 750 clusters as specified previously)\n",
    "    for i in closest_clusters[0]:\n",
    "        if(i > (state_count - 1) or i < 0):\n",
    "            print(\"The clusters you are searching for are not configured properly and are out of bounds\")\n",
    "            print(\"Did you modify the cluster_count variable without changing this error configuration?\")\n",
    "    \n",
    "    return cluster_values, cluster_labels, train_zscores, closest_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.55737705e-01  4.34426230e-01 -3.85754098e-01 ...  3.80228529e-01\n",
      "   7.31333868e-01  7.12742180e-01]\n",
      " [ 3.15217391e-01  1.30434783e-01 -4.71336957e-01 ... -1.47334846e-01\n",
      "   8.37547692e-01  5.49701284e-01]\n",
      " [ 6.93889390e-18 -2.75000000e-01 -4.95625000e-01 ... -6.38592514e-01\n",
      "  -1.32594951e+00 -1.24390028e+00]\n",
      " ...\n",
      " [-4.90196078e-02 -3.23529412e-01 -4.90843137e-01 ... -2.94132415e-01\n",
      "   3.32300837e-01  4.33748198e-01]\n",
      " [-6.09756098e-02 -4.51219512e-01 -5.00000000e-01 ... -1.49388817e+00\n",
      "  -2.31206609e+00 -1.85290276e+00]\n",
      " [ 6.66666667e-02 -4.50000000e-01 -4.98866667e-01 ... -1.18806659e+00\n",
      "  -3.76091639e-01 -1.59309354e+00]] \n",
      " Dimensions:  750  x  50 \n",
      "         gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag  \\\n",
      "0         -0.5       0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "1         -0.5       0.5           -0.5     -2.302585        0.5       0.5   \n",
      "2         -0.5       0.5           -0.5     -2.302585        0.5       0.5   \n",
      "3         -0.5       0.5           -0.5     -2.302585        0.5       0.5   \n",
      "4         -0.5       0.5           -0.5     -2.302585        0.5       0.5   \n",
      "...        ...       ...            ...           ...        ...       ...   \n",
      "238325    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238326    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238327    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238328    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238329    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "\n",
      "             age  Weight_kg       GCS        HR  ...       BUN  Creatinine  \\\n",
      "0      -0.973100  -0.318414 -0.687933 -0.569736  ... -0.545751   -0.214781   \n",
      "1      -0.973100  -0.169199 -0.687933 -0.491800  ... -1.189188   -0.679473   \n",
      "2      -0.973100  -0.169199 -0.640362 -0.567776  ... -1.189188   -0.679473   \n",
      "3      -0.973100  -0.169199 -0.402506 -0.749137  ... -1.189188   -0.679473   \n",
      "4      -0.973100  -0.169199  0.510859  0.630189  ... -1.189188   -0.679473   \n",
      "...          ...        ...       ...       ...  ...       ...         ...   \n",
      "238325 -2.467202  -0.361999  0.739200  0.191981  ... -1.909177   -0.494229   \n",
      "238326 -2.467202   0.228708  0.739200 -0.072708  ... -1.909177   -0.494229   \n",
      "238327 -2.467202  -0.403020  0.739200 -0.205053  ... -1.909177   -0.494229   \n",
      "238328 -2.467202  -0.403020  0.739200 -1.102056  ... -1.909177   -0.494229   \n",
      "238329 -2.467202  -0.403020  0.739200 -0.695219  ... -1.909177   -0.494229   \n",
      "\n",
      "            SGOT      SGPT  Total_bili       INR  input_total  input_4hourly  \\\n",
      "0       0.977035  1.112233    0.334065 -0.692745     0.820417       0.096264   \n",
      "1      -0.268657 -0.443987    1.387969 -0.214109     0.825330       0.240649   \n",
      "2      -0.268657 -0.443987    1.387969  0.200505     0.830205       0.240649   \n",
      "3      -0.268657 -0.443987    1.387969 -0.692745     0.835042       0.240649   \n",
      "4      -0.268657 -0.443987    1.387969 -0.692745     0.837926       0.096264   \n",
      "...          ...       ...         ...       ...          ...            ...   \n",
      "238325  0.901093  0.140315   -1.185843 -0.962914     0.142258      -1.521065   \n",
      "238326 -0.892408 -0.604775   -1.185843 -0.692745     0.142258      -1.521065   \n",
      "238327  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "238328  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "238329  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "\n",
      "        output_total  output_4hourly  \n",
      "0           0.757770        0.733745  \n",
      "1           0.771262        0.731161  \n",
      "2           0.790024        0.847813  \n",
      "3           0.803524        0.767750  \n",
      "4           0.812873        0.670986  \n",
      "...              ...             ...  \n",
      "238325      0.530930        0.743862  \n",
      "238326      0.567943        0.789867  \n",
      "238327      0.567943       -1.852903  \n",
      "238328      0.587895        0.637420  \n",
      "238329      0.610678        0.701156  \n",
      "\n",
      "[190828 rows x 50 columns]\n",
      "190828\n"
     ]
    }
   ],
   "source": [
    "cluster_values, cluster_labels, train_zscores, closest_clusters = \\\n",
    "kmeans_cluster_calculations(data_set=sample_train_set, state_count=state_count, num_loops_per_iter=10000, \n",
    "                            max_num_iter=clustering_iter, debug_flag=True, regenerate_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The generate_action_column function takes 4 arguments: \n",
    "#\n",
    "# column_values: A series of column values from a dataframe that we want to turn into action states\n",
    "# num_groups: How many groups or distinct actions we want to split the data into\n",
    "# column_name: The name of the column used for print debug statements\n",
    "# num_rows: The total number of rows in the full column before modifications (This is normally patientdata[column_name].size)\n",
    "# \n",
    "# This function returns column_actions, a series that represents the 'action', or group that each row of data falls under.\n",
    "#\n",
    "# An example is found down below, but in words, this function takes a full column of data, groups \n",
    "# the values for that data into num_groups distinct actions, and returns a series representing actions based on row\n",
    "# \n",
    "# Ex: Patients' blood pressure might be grouped into 5 categories (Action 1: < 20 mmHg, Action 2: > 20 mmHg && < 60 mmHg... etc)\n",
    "\"\"\"\n",
    "\n",
    "def generate_action_column(column_values:pd.DataFrame, num_groups:int, column_name:str, num_rows:int, debug_flag:bool=True) -> pd.Series:\n",
    "    # Determine minimum and maxium to scale data appropriately\n",
    "    if debug_flag:\n",
    "        print(\"Old Lowest \", column_name, \" Rank: \", min(column_values.rank()))\n",
    "        print(\"Old Highest \" , column_name,  \" Rank: \", max(column_values.rank()))\n",
    "    # Now we want to rank these actions in order of their value (lowest to highest)\n",
    "    # Normalizing according to lowest and highest rank\n",
    "    \n",
    "    # Moving the minimum to zero\n",
    "    column_ranks:pd.Series = (column_values.rank() - min(column_values.rank()))\n",
    "    # Shifting the max to approximately 1.0\n",
    "    column_ranks:pd.Series = column_ranks / max(column_ranks)\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Validate that the range is indeed 0 to 1\n",
    "        print(\"New Lowest \", column_name, \" Rank: \", min(column_ranks))\n",
    "        print(\"New Highest \", column_name, \" Rank: \", max(column_ranks))\n",
    "\n",
    "    # The Max of all column values needs to be nearly 1, and the min of all column\n",
    "    # values needs to be nearly 0 \n",
    "    if round(max(column_ranks), 3) != 1 or round(min(column_ranks), 3) != 0:\n",
    "        print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "        print(\"Current max: \", round(max(column_ranks), 3))\n",
    "        print(\"Curret min: \", round(min(column_ranks), 3))\n",
    "    # Normalize the rank values to even intevals of ranks\n",
    "    old_values:List[float] = np.sort(column_ranks.unique()).tolist()\n",
    "    even_intervals:List[float] = [i/column_ranks.unique().size for i in range(0, column_ranks.unique().size)]\n",
    "    # Iterate over the Series to apply the normalized values\n",
    "    for i in range(0, column_ranks.size):\n",
    "        old_value_index:float = old_values.index(column_ranks[i])\n",
    "        column_ranks[i] = even_intervals[old_value_index] \n",
    "    \n",
    "    # This is a mathematics trick to seperate all the values into {num_groups} distinct groups based on their rank.\n",
    "    # Given different columns of interest this can take different forms. For IV fluids, this number is 5.\n",
    "    column_groups:pd.Series = np.floor(((column_ranks + 1.0/float(num_groups)) * (num_groups - 1))) + 1\n",
    "    \n",
    "    # Validate that groups are all associated with desired group split\n",
    "    if not(column_groups.isin([i for i in range(1, num_groups + 1)]).any()):\n",
    "        print(\"Groups chosen fall outside the desired 1-\" + num_groups + \" window\")\n",
    "\n",
    "    column_actions:pd.Series = pd.Series([1 for i in range(0, num_rows)])\n",
    "\n",
    "    # If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "    for index in column_groups.index:\n",
    "        column_actions[index] = column_groups[index]\n",
    "        \n",
    "    return column_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This function takes two arguments:\n",
    "# actions_column: A column of action groups generated by the above function (generate_action_column())\n",
    "# real_values: The actual values from the dataset corresponding to the same column as actions_column\n",
    "# and returns a list that contains the real median values for each 'group' actions.\n",
    "#\n",
    "# Ex: We apply the function to the action_column \"IV_Fluid\", which has split the data into 4 different groups of \n",
    "# IV_Fluid actions. This function will produce a list containing the median amount of IV_Fluid administered for each of those\n",
    "# groups (Group 1 -> Adminster 20 mL, Group 2 -> Administer 40 mL, Group 3 -> Administer 60 mL, Group 4 -> Administer 80 mL\n",
    "\"\"\"\n",
    "\n",
    "def median_action_values(actions_column: pd.DataFrame, real_values:pd.DataFrame) -> List[np.float64]:\n",
    "    # Grab all the unique actions for a column and sort them\n",
    "    all_groups:List[np.float64] = np.sort(actions_column.unique())\n",
    "    # Concatanate the group number and real value for each row\n",
    "    action_set:pd.DataFrame = pd.concat([actions_column, real_values], axis=1, sort=False)\n",
    "    # Name the columns for accurate querying\n",
    "    action_set.columns = ['group_id', 'data_val']\n",
    "    # Grab the median value for each group based on group number using python list comprehension\n",
    "    median_values:List[np.float64] = [np.median(action_set[action_set['group_id'] == i]['data_val']) for i in all_groups]\n",
    "    return median_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# This function takes one argument\n",
    "# list_action_columns: This is a Pandas dataframe that contains all the action_columns we desir to be grouped by index\n",
    "# This can be retrieving using the previously defined 'median action' function \n",
    "# \n",
    "# and returns two items:\n",
    "# list_action columns: The 'keys' or integers that represent every permutation of actions\n",
    "# chosen_action: The key that was chosen based on the action values in each column\n",
    "\"\"\"\n",
    "def generate_action_matrix(list_action_columns:pd.DataFrame) -> (List[int], List[int]):\n",
    "    # Grabs the list of columns the user has provided for use\n",
    "    desired_columns:List[str] = [column for column in list_action_columns]\n",
    "    # Drops all group combinations that are duplicates\n",
    "    list_action_columns_indexes:pd.DataFrame = list_action_columns.drop_duplicates(desired_columns)\n",
    "    # Sorts all combinations in order\n",
    "    list_action_columns_indexes = list_action_columns_indexes.sort_values(desired_columns)\n",
    "    # Create a dictionary based on the values from the dataframe \n",
    "    list_action_columns_indexes:List[int] = list_action_columns_indexes.values.tolist() \n",
    "    # Determine which index in the list each row corresponds to \n",
    "    # Ex: For an 2-D action permutation list of [1,1] thru [5,5], there are 5 x 5 possibilities\n",
    "    # {1..5}, {1..5}, so there are 25 possible permutations, the indexes will run 1 - 25\n",
    "    chosen_action:List[int] = [list_action_columns_indexes.index(val_pair) for val_pair in list_action_columns.values.tolist()]\n",
    "    # Return the keys first, and then the true values for the dataset\n",
    "    return list_action_columns_indexes, chosen_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The build_dataset_actions function takes 7 arguments: \n",
    "# patientdata: A Dataframe representing all the patients and their data\n",
    "# first_column: Our first column that we desire to build our action matrix with (first_column x second_column) (SOFA by default)\n",
    "# second_column: Our second column that we desire to build or action matrix with (first_column x second_column) (qSOFA by default)\n",
    "# num_groups_first_column: The number of groups we want to divide our first column into (5 by default)\n",
    "# num_groups_second_column: The number of groups we want to divide our second column into (4 by default)\n",
    "# debug_flag: A boolean that determines whether we want debug prints presents\n",
    "# graph_flag: Whether we want to print the graph or not\n",
    "# \n",
    "# and returns 2 values:\n",
    "# \n",
    "# train_chosen_actions:        The actions for each row of the training dataset\n",
    "# train_action_values:         The matrix representing (first_column x second_column)\n",
    "# action_list:                 A list of actions taken at every given datapoint\n",
    "\"\"\"\n",
    "def build_dataset_actions(patientdata:pd.DataFrame, first_column:str=\"SOFA\", second_column:str=\"qSOFA\", num_groups_first_column:int=5,\n",
    "                         num_groups_second_column:int=4, debug_flag:bool=True, graph_flag:bool=True) -> (pd.Series, \n",
    "                                                                                                         List[List[int]],\n",
    "                                                                                                         List[int]):\n",
    "    # Generate the actions for a the first desired column of data\n",
    "    first_col:pd.DataFrame = patientdata[first_column]\n",
    "    first_col_actions:pd.Series = generate_action_column(column_values = first_col, num_groups = num_groups_first_column, \n",
    "                                                        column_name = first_column, \n",
    "                                                         num_rows = patientdata[first_column].size, debug_flag=True)\n",
    "    if debug_flag:\n",
    "        print(first_col_actions.unique())    \n",
    "    # Now do the same for the second desired column of data\n",
    "    second_col:pd.DataFrame = patientdata[second_column]\n",
    "    second_col_actions:pd.Series = generate_action_column(column_values = second_col, num_groups = num_groups_second_column,  \n",
    "                                                         column_name = second_column, \n",
    "                                                         num_rows = patientdata[second_column].size, debug_flag=True)\n",
    "    if debug_flag:\n",
    "        print(second_col_actions.unique())\n",
    "    \n",
    "    # Obtain the median real values that each division represents\n",
    "    first_median_actions:List[np.float64] = median_action_values(actions_column = first_col_actions, real_values = patientdata[first_column])\n",
    "    second_median_actions:List[np.float64] = median_action_values(actions_column = second_col_actions, real_values = patientdata[second_column])\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(first_column,\" Action Median Values:\", str(first_col_actions), \"\\n\" + second_column + \":\", second_median_actions, \"\\n\")\n",
    "    ###\n",
    "    # FINISH CONSTRUCTION OF ALL ACTIONS AND THEIR VALUES\n",
    "    ###\n",
    "    # Combine the columns that we desire to observe (iv_fluid_actions, vasopressor_actions)\n",
    "    combined_groups:pd.DataFrame = pd.concat([first_col_actions, second_col_actions], axis=1, sort=False)\n",
    "    # Name the columns for proper usage in the function\n",
    "    combined_groups.columns = [first_column, second_column]\n",
    "    \n",
    "    # The Key value pair for every datapoint and the corresponding action taken at that point\n",
    "    action_keys, action_list = generate_action_matrix(list_action_columns = combined_groups)\n",
    "    # Plot the distribution of actions\n",
    "    if graph_flag:\n",
    "        plt.hist(action_list, density=False, bins=20)  # `density=False` would make counts\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Index of Action Chosen: 1 through 24\")\n",
    "    train_chosen_actions:List[np.float64] = []\n",
    "    with open ('sample_train.txt', 'rb') as fp:\n",
    "        train_chosen_actions = pickle.load(fp)\n",
    "\n",
    "    # Grab a Series representing the action taken by the train data only\n",
    "    train_chosen_actions:pd.Series = pd.Series(action_list)[train_chosen_actions]\n",
    "\n",
    "    # Assign all action choices to their corresponding median values as shown previously\n",
    "    if debug_flag:\n",
    "        print(first_median_actions, second_median_actions)\n",
    "\n",
    "\n",
    "    # This gives us the representative median values for a patient's vitals present in various action groups\n",
    "    # action_keys[i] corresponds to train_action_values[i]\n",
    "    # So, if the patient falls into group [1, 1] or no iv fluid given, no vasopressor administered,\n",
    "    # The corresponding median values for this group will be represented by train_action_values (0.0, 0.0).\n",
    "    # A patient in group [1, 2] (no iv fluid, a little vasopressor) will have a median real value of (0.0, 0.04)\n",
    "    train_action_values:List[List[int]] = list(cartesian_prod(first_median_actions, second_median_actions))\n",
    "\n",
    "    if len(train_action_values) != len(first_median_actions) * len(second_median_actions):\n",
    "        print(\"Something went wrong in determining the Cartesian product\")\n",
    "    \n",
    "    return train_chosen_actions, train_action_values, action_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Lowest  SOFA  Rank:  2962.5\n",
      "Old Highest  SOFA  Rank:  238328.0\n",
      "New Lowest  SOFA  Rank:  0.0\n",
      "New Highest  SOFA  Rank:  1.0\n",
      "[2 3 1 4 5]\n",
      "Old Lowest  qSOFA  Rank:  30048.0\n",
      "Old Highest  qSOFA  Rank:  235188.0\n",
      "New Lowest  qSOFA  Rank:  0.0\n",
      "New Highest  qSOFA  Rank:  1.0\n",
      "[3 2 1 4]\n",
      "SOFA  Action Median Values: 0         2\n",
      "1         2\n",
      "2         2\n",
      "3         2\n",
      "4         2\n",
      "         ..\n",
      "238325    1\n",
      "238326    1\n",
      "238327    2\n",
      "238328    1\n",
      "238329    1\n",
      "Length: 238330, dtype: int64 \n",
      "qSOFA: [0.0, 1.0, 2.0, 3.0] \n",
      "\n",
      "[1.0, 5.0, 9.0, 15.0, 20.0] [0.0, 1.0, 2.0, 3.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_chosen_actions, train_action_values, action_list = build_dataset_actions(patientdata=patientdata, \n",
    "                      first_column=\"SOFA\", second_column=\"qSOFA\", \n",
    "                      num_groups_first_column=5, num_groups_second_column=4, \n",
    "                      debug_flag=True, graph_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_prestate_matrix_train function takes 5 arguments: \n",
    "# train_90d:              A column representing whether or not a patient was dead or alive at the end of 90days\n",
    "# train_blocs:            All the rows of data for a patient for each individual hospital stay\n",
    "# closest_clusters:       The closest data cluster that a given data point falls near\n",
    "# train_chosen_actions:   The action that is represented by two of the patient's characteristics, calculated previously\n",
    "# is_debug:               Whether or not statements are printed over time.\n",
    "# \n",
    "# and returns 1 value:\n",
    "# \n",
    "# qlearning_dataset_mod: The full training dataset configured to be converted into states and actions\n",
    "# all_lower_ranges:      The lower bound for the reward values\n",
    "# all_upper_ranges:      The upper bound for the reward values\n",
    "\"\"\"\n",
    "def construct_prestate_matrix_train(train_90d:pd.DataFrame, train_blocs:pd.DataFrame, closest_clusters:List[List[int]],\n",
    "                                    train_chosen_actions:pd.DataFrame, is_debug:bool = True, action_count:int = 20, \n",
    "                                    state_count:int = 750) -> (pd.DataFrame, List[int], List[int]):\n",
    "    ###\n",
    "    # BEGIN CONSTRUCTION OF PRE-STATE MATRIX\n",
    "    # This will be used to build the full state/action matrix\n",
    "    ### \n",
    "\n",
    "    # Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "    # If they have died, the range is [-100, 100]\n",
    "    # If they are alive, the range is [100, -100]\n",
    "    range_vals:List[int] = [100, -100]\n",
    "    # Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "    # This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "    train_90d_polarity:List[int] = (2 * (1 - train_90d) - 1)\n",
    "    range_matrix:List[int] = [np.multiply(polarity, range_vals) for polarity in train_90d_polarity]\n",
    "    # Grab the lower range limit and upper range limit seperately in order to build the\n",
    "    # full range of reward values\n",
    "    all_lower_ranges:List[int] = [i[0] for i in range_matrix]\n",
    "    all_upper_ranges:List[int] = [i[1] for i in range_matrix]\n",
    "        \n",
    "    # The qlearning_dataset prior to modification contains 6 columns and ~190885 rows (around 75% of the data)\n",
    "    # The columns are as follows:\n",
    "    #\n",
    "    # training_bloc: time_series stamps for a patient's state over time, very in range from {1..?}\n",
    "    #\n",
    "    # closest_cluster_index: The index of the nearest cluster to the z-scores of the patient's data, \n",
    "    # corresponding actual data for each cluster's index (i) can be found in cluster_values[i]\n",
    "    #\n",
    "    # chosen_action_index: The chosen action or representation of a patient's IV_Fluid and Vasopressor status [0 - 24]\n",
    "    # \n",
    "    # 90d_mortality_status: 0 means the patient is alive 90 days after discharge from ICU\n",
    "    #                      1 means the patient is dead  90 days after discharge from ICU\n",
    "    #\n",
    "    # lower_range + upper_range: An index to be used later on, gathered from the range index\n",
    "    print(\"Training Blocs Length: \", str(len(train_blocs)), \"\\nClosest Clusters Length: \", str(len(closest_clusters[0])), \n",
    "          \"\\nAction List Length: \", str(len(train_chosen_actions)), \"\\nTrain 90d Length\", str(len(train_90d)), \n",
    "          \"\\nRange Matrix Length: \", len(range_matrix))\n",
    "    qlearning_dataset:pd.DataFrame = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters[0]), \n",
    "                                   pd.Series(train_chosen_actions.tolist()), \n",
    "                                   pd.Series(train_90d.tolist()), \n",
    "                                   pd.Series(all_lower_ranges), \n",
    "                                   pd.Series(all_upper_ranges)], \n",
    "                                  axis=1, sort=False)\n",
    "    qlearning_dataset.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', '90d_mortality_status', 'lower_range', 'upper_range']\n",
    "    if is_debug: \n",
    "        print(qlearning_dataset)\n",
    "    \n",
    "    # Modify the set for the final time in order to construct the final life + death states for each patient\n",
    "    qlearning_dataset_mod = modify_qlearning_dataset(ql_dataset = qlearning_dataset, is_training = False)# Print some important details of the set\n",
    "    if is_debug:\n",
    "        # Total patients being observed in the test\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['training_bloc'] == 1]['training_bloc']))\n",
    "        # Show that we now have end states established \n",
    "        print(len(qlearning_dataset[qlearning_dataset['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == state_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == state_count + 1]['chosen_action_index']))\n",
    "        print(qlearning_dataset_mod, \"\\n\")\n",
    "    return qlearning_dataset_mod, all_lower_ranges, all_upper_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# modify_qlearning_dataset is a function that takes a dataframe intended for qlearning and modifies\n",
    "# it in preparation for the ML. In essence, it runs through creating the life and and death states in preparation\n",
    "# for constructing the MDP\n",
    "#\n",
    "# Parameters: \n",
    "# ql_dataset:  The Dataset we want to modify in preparation\n",
    "# is_training: Whether we are building the training set or not.\n",
    "# is_debug:    Whether prints are included or not\n",
    "#       \n",
    "# \n",
    "# Returns: qlearning_dataset_mod - The modified dataset\n",
    "\"\"\"\n",
    "\n",
    "def modify_qlearning_dataset(ql_dataset:pd.DataFrame, is_training:bool) -> pd.DataFrame:\n",
    "    # The base qlearning_dataset does not account for endpoints in either life or death\n",
    "    # These states have not been established yet, which is what this step corrects\n",
    "    qlearning_dataset:pd.DataFrame = ql_dataset[:]\n",
    "    qlearning_dataset_len:int = len(qlearning_dataset.index)\n",
    "    # We need space to add a death/life state for every patient, about a 20% increase in size from the original MDP\n",
    "    # We will cut the excess off by the end of the loop\n",
    "    qlearning_dataset_len_mod:float = int(np.floor(qlearning_dataset_len * 1.2))\n",
    "    qlearning_dataset_mod:np.ndarray = []\n",
    "    if is_training:\n",
    "        qlearning_dataset_mod:np.ndarray = np.array([[0 for i in range(0, 8)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "    else:\n",
    "        qlearning_dataset_mod:np.ndarray = np.array([[0 for i in range(0, 4)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "    # Start construction of modified data\n",
    "    row:int = 0\n",
    "    # In Markov theory, an absorbing state is one which can be entered, but cannot be left. (Similar to the Hotel California)\n",
    "    # In the case of this experiment, those states are either life (state_count) or death (state_count + 1) per patient as\n",
    "    # defined by me (750, 751)\n",
    "    absorbing_states:List[int] = [state_count, state_count + 1]\n",
    "\n",
    "    # Start the loop to begin capping the markov chain off at life and death states\n",
    "    for i in range(0, qlearning_dataset_len - 1):\n",
    "        # Use the already gathered data for each row\n",
    "        if is_training:\n",
    "            ql_row = qlearning_dataset.iloc[i]\n",
    "            qlearning_dataset_mod[row, :] = np.concatenate([ql_row[0:3].values, [ql_row[4]], [ql_row[6]], [ql_row[6]], [ql_row[6]], [ql_row[6]]])\n",
    "        else:\n",
    "            qlearning_dataset_mod[row, :] = qlearning_dataset.iloc[i][0:4]\n",
    "        # If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\n",
    "        if qlearning_dataset.iloc[i + 1]['training_bloc'] <= qlearning_dataset.iloc[i]['training_bloc']:\n",
    "            # Grab the row\n",
    "            whole_row:pd.DataFrame = qlearning_dataset.iloc[i]\n",
    "            # Set most of the row to the original data's values, except set the action to be either state 750 or 751\n",
    "            # Life or death respectively\n",
    "            row = row + 1\n",
    "            if is_training:\n",
    "                # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "                qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], 25,  whole_row['reward_value'], 0, 0, 0, whole_row['training_set_id']]\n",
    "            else:\n",
    "                # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "                qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], 25,  whole_row['lower_range']]\n",
    "        row = row + 1\n",
    "    # Add in the last row\n",
    "    whole_row:pd.DataFrame = qlearning_dataset.iloc[len(qlearning_dataset.index) - 1]\n",
    "    if is_training:\n",
    "        qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], 25,  whole_row['reward_value'], 0, 0, 0, whole_row['training_set_id']]\n",
    "    else:\n",
    "        qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], 25,  whole_row['lower_range']]\n",
    "    \n",
    "    row = row + 1\n",
    "    # Get rid of the unneeded rows\n",
    "    qlearning_dataset_mod:pd.DataFrame = pd.DataFrame(qlearning_dataset_mod[0:row, :])\n",
    "    if is_training:\n",
    "        qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', 'optimal_policy' , 'behavior_policy', 'optimal_action','training_set_id']\n",
    "    else:\n",
    "        qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value']\n",
    "    if is_training:\n",
    "        qlearning_dataset_mod['behavior_policy'] = qlearning_dataset_mod['behavior_policy'].astype(float)\n",
    "        qlearning_dataset_mod['optimal_policy'] = qlearning_dataset_mod['optimal_policy'].astype(float)\n",
    "    return qlearning_dataset_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Blocs Length:  190828 \n",
      "Closest Clusters Length:  190828 \n",
      "Action List Length:  190828 \n",
      "Train 90d Length 190828 \n",
      "Range Matrix Length:  190828\n",
      "        training_bloc  closest_cluster_index  chosen_action_index  \\\n",
      "0                   1                    369                    6   \n",
      "1                   2                    610                    5   \n",
      "2                   3                    610                    5   \n",
      "3                   4                    425                    5   \n",
      "4                   5                    502                    6   \n",
      "...               ...                    ...                  ...   \n",
      "190823              9                    572                    0   \n",
      "190824             10                    372                    1   \n",
      "190825             11                    716                    4   \n",
      "190826             12                    572                    0   \n",
      "190827             13                    572                    0   \n",
      "\n",
      "        90d_mortality_status  lower_range  upper_range  \n",
      "0                          1         -100          100  \n",
      "1                          1         -100          100  \n",
      "2                          1         -100          100  \n",
      "3                          1         -100          100  \n",
      "4                          1         -100          100  \n",
      "...                      ...          ...          ...  \n",
      "190823                     0          100         -100  \n",
      "190824                     0          100         -100  \n",
      "190825                     0          100         -100  \n",
      "190826                     0          100         -100  \n",
      "190827                     0          100         -100  \n",
      "\n",
      "[190828 rows x 6 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f7d1b0938cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                                             \u001b[0mis_debug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                                                             \u001b[0maction_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                                                                             state_count=state_count)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-2374dad48ef1>\u001b[0m in \u001b[0;36mconstruct_prestate_matrix_train\u001b[1;34m(train_90d, train_blocs, closest_clusters, train_chosen_actions, is_debug, action_count, state_count)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Modify the set for the final time in order to construct the final life + death states for each patient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mqlearning_dataset_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodify_qlearning_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mql_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqlearning_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Print some important details of the set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_debug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# Total patients being observed in the test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-51c6357bd057>\u001b[0m in \u001b[0;36mmodify_qlearning_dataset\u001b[1;34m(ql_dataset, is_training)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mqlearning_dataset_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mql_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mqlearning_dataset_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqlearning_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;31m# If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mqlearning_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_bloc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mqlearning_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_bloc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    752\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qlearning_dataset_mod, all_upper_ranges, all_lower_ranges = construct_prestate_matrix_train(train_90d = train_90d, \n",
    "                                                                                            train_blocs = train_blocs, \n",
    "                                                                                            closest_clusters = closest_clusters,\n",
    "                                                                                            train_chosen_actions = train_chosen_actions, \n",
    "                                                                                            is_debug=True,\n",
    "                                                                                            action_count=action_count, \n",
    "                                                                                            state_count=state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Now that we officially have some a valid bloc for actions, and a valid set of states, it's time \n",
    "#  to begin building the transitions matrix.\n",
    "###\n",
    "\n",
    "### If the matrix is bidirectional (S1 -> S2, S2 -> S1 are both valid, we can build two matrices)\n",
    "\n",
    "### \n",
    "# The MDP Toolbox we are going to be using requires Transition and Reward Matrices to be in the form\n",
    "# M(action, state1, state2)\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "# The create_transition_matrix method takes 4 arguments:\n",
    "# num_actions: The total number of possible actions (calculated by action_count ^ 2 or in py, action_count ** 2)\n",
    "# num_states:  Number of states the model uses\n",
    "# qlearning_dataset: The dataset that will be used for the qlearning process\n",
    "# transition_threshold: How many actions do we want to deem as scarce and not worth keeping (default = 5)\n",
    "# reverse: If false, the matrix that is created is represented as transition[A][S1][S2], if true: transition[A][S2][S1]\n",
    "# \n",
    "# and returns 2 values:\n",
    "# transition_matrix: The counts of which actions were chosen in which states\n",
    "# physician_policy:  The transition_matrix that has been turned into probabilties by dividing counts in each state by \n",
    "# total counts\n",
    "# \n",
    "\"\"\"\n",
    "def create_transition_matrix(num_actions:int, num_states:int, ql_data_input:pd.DataFrame, \n",
    "                             transition_threshold:int = 5, reverse:bool = False) -> (int_matrix2D, float_matrix2D):\n",
    "    # The transition matrix is a 3D construct, involving a transition between two states\n",
    "    # and an action. The dimensions for the matrix are (state_count * 2) * (state_count + 2) * action_count\n",
    "    transition_matrix:float_matrix2D = [[[0 for i in range(0, num_states + 2)] for i in range(0, num_states + 2)] for i in range(0, num_actions)]\n",
    "    # NP Arrays allow for more compact and efficient slicing\n",
    "    transition_matrix = np.array(transition_matrix).astype(float)\n",
    "    # We also need a matrix to denote the policy that corresponds with taken a particular action from a state\n",
    "    transition_policy_count:int_matrix2D = [[0 for i in range(0, num_states + 2 )] for i in range(0, num_actions)]\n",
    "    transition_policy_count = np.array(transition_policy_count).astype(float)\n",
    "    # Iterate over the actual data in order to form the actual states and their corresponding actions\n",
    "    # As soon as we hit the next patient (the next row has a training bloc value of 1), we stop processing actions for that patient\n",
    "    for i in range(0, len(qlearning_dataset_mod) - 1):\n",
    "        # Since 1 is our 'endpoint' for each patient, there are no actions we can take from this point on\n",
    "        if ql_data_input.iloc[i + 1]['training_bloc'] > ql_data_input.iloc[i]['training_bloc']:\n",
    "            S1:int = ql_data_input.iloc[i]['closest_cluster_index']\n",
    "            S2:int = ql_data_input.iloc[i + 1]['closest_cluster_index'] \n",
    "            action_id:int = ql_data_input.iloc[i]['chosen_action_index']\n",
    "            if not(reverse):\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S1][S2] = transition_matrix[action_id][S1][S2] + 1\n",
    "            else:\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S2][S1] = transition_matrix[action_id][S2][S1] + 1\n",
    "                \n",
    "            # Count the number of times action A is used to transition from S1\n",
    "            transition_policy_count[action_id][S1] = transition_policy_count[action_id][S1] + 1        \n",
    "\n",
    "    # In order to avoid drastically altering our model, we fix a constant\n",
    "    # value (set by default to 5), in order to declare sparse actions \n",
    "    # as essentially not happening (make their count 0)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if transition_policy_count[i][j] <= transition_threshold:\n",
    "                transition_policy_count[i][j] = 0 \n",
    "    # Now, we want to prevent transitions from state -> state using\n",
    "    # a certain action if that action is sparse or nonexistant\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if not(reverse):\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,j,:] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,j,:] = transition_matrix[i,j,:]/np.float64(transition_policy_count[i][j])\n",
    "            else:\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,:,j] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,:,j] = transition_matrix[i,:,j]/np.float64(transition_policy_count[i][j])\n",
    "    \n",
    "    # Ensure no divisions create NaNs or infinities\n",
    "    transition_matrix = np.nan_to_num(transition_matrix)\n",
    "    # Determine the phyisican's policy based on total count\n",
    "    # This comes in handy later when comparing model ability\n",
    "    total_transitions:float = sum(transition_policy_count)\n",
    "    physician_policy:float_matrix2D = np.divide(transition_policy_count, total_transitions)\n",
    "    return transition_matrix, physician_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 752 752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Constructing Transition Matrix(A, State1, State2)\n",
    "total_actions:int = len(train_action_values)     \n",
    "# Execute the function call\n",
    "transition_mat, physician_policy = create_transition_matrix(num_actions = total_actions, num_states = state_count,ql_data_input = qlearning_dataset_mod, transition_threshold = transition_threshold, reverse = False)\n",
    "# Should be 25, 752, 752\n",
    "print(len(transition_mat), len(transition_mat[0]), len(transition_mat[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The build_and_check_reward_matrix function takes 5 arguments: \n",
    "# train_90d:              A column representing whether or not a patient was dead or alive at the end of 90days\n",
    "# train_blocs:            All the rows of data for a patient for each individual hospital stay\n",
    "# closest_clusters:       The closest data cluster that a given data point falls near\n",
    "# train_chosen_actions:   The action that is represented by two of the patient's characteristics, calculated previously\n",
    "# is_debug:               Whether or not statements are printed over time.\n",
    "# \n",
    "# and returns 1 value:\n",
    "# \n",
    "# qlearning_dataset_mod: The full training dataset configured to be converted into states and actions\n",
    "\"\"\"\n",
    "def build_and_check_reward_matrix(transition_mat:int_matrix2D, physician_policy:float_matrix2D, total_actions:int,\n",
    "                                       is_debug:bool = True, check_errors:bool = True) -> (float_matrix2D):\n",
    "    \n",
    "    # With the MDP built and the probabilities evaluated, it is time to construct the reward matrix \n",
    "    reward_mat:int_matrix2D = np.array([[[0 for i in range(0, state_count + 2)] for i in range(0, state_count + 2)] for i in range(0, total_actions)])\n",
    "    # We want to fix initial penalities and rewards like before at -100 and +100\n",
    "    # Note, we allot the 2 extra columns for the reward and penalty\n",
    "    for i in range(0, total_actions):\n",
    "        for j in range(0, state_count + 2):\n",
    "            # If the patient hits the final state of death, we penalize the model\n",
    "            reward_mat[i][j][state_count] = -100\n",
    "            # If the patient hits the final state of life, we reward the model\n",
    "            reward_mat[i][j][state_count + 1] = 100\n",
    "            \n",
    "    # Perform Matrix multiplication to multiply reward value by probability of that reward\n",
    "    # This gives us the 'real reward' where sparse events are not weighted as heavily \n",
    "    # as more frequent events.\n",
    "    # I.E\n",
    "    # An action that occurs 2% of the time resulting in death should suffer a much lower penalty than a death route that occurs 50% of the time\n",
    "    # Likewise, an action occurs 80% of the time yielding life should be looked at more closely than a 2% chance\n",
    "    reward_mat_final:float_matrix2D = np.multiply(transition_mat, reward_mat)\n",
    "    \n",
    "    if is_debug:\n",
    "        reward_count:int = 0\n",
    "        for i in range(0, total_actions):\n",
    "            for j in range(0, state_count + 2):\n",
    "                for k in range(0, state_count + 2):\n",
    "                    if reward_mat_final[i][j][k] != 0:\n",
    "                        reward_count = reward_count + 1\n",
    "\n",
    "            # Reward Count should be somewhere in the 2000 - 3500 count\n",
    "            print(reward_count)\n",
    "\n",
    "    if check_errors:\n",
    "        # Evaluate whether or not the Sum of all actions from a given state is either 0.0 or 1.0\n",
    "        # Where 1 indicates there are further actions to take, and 0 means we have hit the end of potential actions to take\n",
    "        for i in range(0, total_actions):\n",
    "            for j in range(0, state_count + 2):\n",
    "                total_prob:float = sum(transition_mat[i][j])\n",
    "                if (abs(total_prob - 1.0) > 0.001):\n",
    "                    print(\"All probabilities should be either 0.0 or almost 1.0 (0.999....), check your arguments to transition_mat\")\n",
    "        # Evaluate whether or not the total sum of all physician policy probabilities \n",
    "        # is 1\n",
    "        for i in range(0, state_count + 2):\n",
    "            total_prob:float = 0\n",
    "            for j in range(0, total_actions):\n",
    "                total_prob = total_prob + physician_policy[j][i]\n",
    "            if (abs(total_prob - 1.0) > 0.001):\n",
    "                    print(\"One of your states does not have an added action probability total of 1\")\n",
    "\n",
    "        # State 750 is life, State 751 is death\n",
    "        # This provides a count of how many states in the finalized MDP end in life and death\n",
    "        life_count:int = 0\n",
    "        death_count:int = 0\n",
    "\n",
    "        for i in range(0, total_actions):\n",
    "            for j in range(0, state_count + 2):\n",
    "                if transition_mat[i, j, state_count] != 0:\n",
    "                    life_count = life_count + 1\n",
    "                if transition_mat[i, j, state_count + 1] != 0:\n",
    "                    death_count = death_count + 1\n",
    "\n",
    "        print(\"Life Endpoints:\", life_count, \"\\nDeath Endpoints\", death_count)\n",
    "\n",
    "        # Validate that all rewards are in the range -100 to 100 \n",
    "        for i in range(0, total_actions):\n",
    "            for j in range(0, state_count + 2):\n",
    "                for k in range(0, state_count + 2):\n",
    "                    if reward_mat_final[i][j][k] > 100 or reward_mat_final[i][j][k] < -100:\n",
    "                        print(\"The entry at\", i + \",\" + j, \"has a reward value of \", reward_mat_other[i][j], \"which is not in the range (-100, 100)\")\n",
    "        \n",
    "        return reward_mat_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "331\n",
      "363\n",
      "365\n",
      "971\n",
      "1913\n",
      "2487\n",
      "2549\n",
      "2840\n",
      "3645\n",
      "4192\n",
      "4299\n",
      "4305\n",
      "4395\n",
      "4511\n",
      "4548\n",
      "4551\n",
      "4558\n",
      "4561\n",
      "4563\n",
      "Life Endpoints: 2725 \n",
      "Death Endpoints 1838\n"
     ]
    }
   ],
   "source": [
    "reward_mat_final:float_matrix2D = build_and_check_reward_matrix(transition_mat = transition_mat, \n",
    "                                        physician_policy = physician_policy, total_actions = total_actions,\n",
    "                                       is_debug = True, check_errors = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The qlearning_for_optimal_actions function takes 12 arguments: \n",
    "# transition_mat:        Matrix of state -> action pairs\n",
    "# reward_mat_final:      Matrix of action -> state -> reward groups\n",
    "# train_blocs:           Patient Training Data\n",
    "# closest_clusters:      Clusters that are closest to an individual row of patient data\n",
    "# train_chosen_actions:  All the actions for the training set of the data\n",
    "# all_lower_ranges:      All lower reward thresholds\n",
    "# train_id_list:         The ID of every datapoint in the training set\n",
    "# discount_val:          Discount to be applied to future decisions\n",
    "# num_of_iterations_val: How many steps are desired in the Q-Learning algorithm   \n",
    "# scheck_val:            A bool to perform checks on the matrix at each step (False for performance speedup)\n",
    "# is_debug:              If print statements are included or not\n",
    "# \n",
    "# and returns 2 values:\n",
    "# \n",
    "# optimal_actions:              A matrix (actions x state) that represents the optimal action at a given state\n",
    "# qlearning_dataset_train_mod:  The modified final qlearning_dataset\n",
    "# \n",
    "# \n",
    "\"\"\"\n",
    "def qlearning_for_optimal_actions(transition_mat:int_matrix3D, reward_mat_final:float_matrix3D,  \n",
    "                                  train_blocs:pd.DataFrame, closest_clusters:List[List[int]],\n",
    "                                  train_chosen_actions:pd.DataFrame, all_lower_ranges:List[int],\n",
    "                                  train_id_list:pd.DataFrame, discount_val:float = 0.99, \n",
    "                                  num_of_iterations_val:int = 200000, scheck_val:bool = False, \n",
    "                                  is_debug:bool = True) -> (List[int], pd.DataFrame, np.ndarray):\n",
    "    ###\n",
    "    #  The Full MDP is now finished. The Transition Matrix exists, the reward matrix that corresponds to it also exists.\n",
    "    #  Now is the time to actually perform Q-Learning\n",
    "    ###\n",
    "\n",
    "    # The initial MDP matrix\n",
    "    # We need the values of weights that determines how much the model\n",
    "    # prefers transitioning from one state (medical conditional), to another\n",
    "    # The Matrix must be in the form [[A][S1][S2]] Where S1 is initial state, S2 is the second state, and\n",
    "    # A is the action taken to get from S1 to S2. \n",
    "    transitions:int_matrix3D = transition_mat\n",
    "\n",
    "    # We need to determine the reward value for predicting an outcome leading to survival (+)\n",
    "    # and a penalty for an outcome that will yield death (-)\n",
    "    # The Matrix must be in the form [[R][S1][S2]] Where S1 is initial state, S2 is the second state, and\n",
    "    # R is the reward for taking the action from S1 to S2. \n",
    "    reward:float_matrix3D = reward_mat_final\n",
    "\n",
    "    # We need to determine the discount value to influence the model to continue changing\n",
    "    # when outcomes are not desired, This value should be kept in the range 0 < discount < 1\n",
    "    discount:float = discount_val\n",
    "\n",
    "    # The Q-Learning algorithm will run a fixed number of times\n",
    "    num_of_iterations:int = num_of_iterations_val\n",
    "\n",
    "    # We need to determine whether or not we want to validate that the transitions and rewards matrix\n",
    "    # to make sure they are valid, this option will only be turned off for speed\n",
    "    scheck:bool = scheck_val\n",
    "    \n",
    "    ql_runner = mdptoolbox.mdp.QLearning(transitions, reward, discount, n_iter = num_of_iterations)\n",
    "    ql_runner.run()\n",
    "    ql_runner.Q\n",
    "    \n",
    "    if is_debug:\n",
    "        # Given that the Q-Learning has taken place, we can record the best action\n",
    "        # For each state\n",
    "        optimal_actions:List[int] = [0 for i in range(0, len(ql_runner.Q))]\n",
    "        for i in range(0, len(ql_runner.Q)):\n",
    "            optimal_actions[i] = np.argmax(ql_runner.Q[i])\n",
    "        print(optimal_actions)\n",
    "    \n",
    "        # This displays the number of actions in which the optimal action to state\n",
    "        # is not zero. In one run, the original set had 578/750 as non zero. Due\n",
    "        # to seeding and random number generation, the Q-Learning can differentiate.\n",
    "        total_count:int = 0 \n",
    "        for i in range(0, 752):\n",
    "            if optimal_actions[i] != 0:\n",
    "                total_count = total_count + 1\n",
    "        print(total_count)\n",
    "        \n",
    "    ####\n",
    "    # Q-Learning validation using the Q Model, and the Train and Test sets\n",
    "    ####\n",
    "\n",
    "    # We have the optimal actions determined by the Q-Equation, as to which\n",
    "    # action each state take to derive optimal outcome. We can use the initial\n",
    "    # train and test set that has been pre determined to figure out\n",
    "    # how accurate the model is\n",
    "    \n",
    "    # First we have to configure the set to be trainable\n",
    "    qlearning_dataset_train:pd.DataFrame = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters[0]), \n",
    "                                   pd.Series(train_chosen_actions.tolist()), \n",
    "                                   pd.Series(train_90d.tolist()), \n",
    "                                   pd.Series([0 for i in range(0, len(closest_clusters[0]))]),\n",
    "                                   pd.Series(all_lower_ranges), \n",
    "                                   pd.Series(train_id_list.tolist())],\n",
    "                                   axis=1, sort=False)\n",
    "    qlearning_dataset_train.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', '90d_mortality_status', 'zero_value', 'reward_value', 'training_set_id']\n",
    "    # Modify the initial Q-Learning dataset to be applicable as a training set\n",
    "    qlearning_dataset_train_mod = modify_qlearning_dataset(ql_dataset = qlearning_dataset_train, is_training=True)\n",
    "    if is_debug:\n",
    "        print(qlearning_dataset_train_mod)\n",
    "    return optimal_actions, qlearning_dataset_train_mod, ql_runner.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 0, 1, 9, 1, 8, 8, 9, 0, 0, 9, 0, 0, 1, 1, 0, 5, 0, 0, 8, 0, 5, 5, 10, 5, 0, 0, 1, 1, 8, 0, 0, 4, 17, 8, 0, 9, 0, 0, 10, 8, 13, 9, 0, 0, 8, 17, 4, 9, 0, 0, 6, 0, 4, 8, 10, 4, 0, 0, 7, 1, 14, 0, 6, 6, 0, 0, 0, 14, 1, 0, 0, 0, 8, 0, 9, 6, 0, 9, 5, 5, 4, 0, 9, 0, 9, 15, 0, 5, 6, 0, 0, 0, 5, 0, 4, 4, 4, 5, 0, 1, 0, 2, 8, 8, 0, 10, 10, 8, 0, 5, 6, 0, 14, 0, 1, 0, 9, 0, 10, 9, 4, 4, 5, 10, 5, 4, 10, 6, 10, 10, 1, 9, 4, 0, 6, 6, 1, 0, 4, 11, 14, 9, 1, 15, 11, 0, 0, 0, 0, 0, 6, 6, 10, 11, 13, 0, 5, 14, 9, 10, 10, 10, 5, 10, 0, 8, 8, 10, 0, 9, 6, 9, 1, 5, 0, 8, 0, 8, 9, 14, 6, 17, 0, 0, 5, 5, 5, 5, 5, 0, 0, 4, 0, 0, 10, 13, 10, 2, 10, 13, 10, 5, 13, 9, 4, 9, 4, 7, 0, 1, 1, 0, 5, 14, 6, 0, 17, 0, 10, 9, 8, 12, 4, 6, 0, 10, 4, 9, 0, 10, 14, 5, 6, 7, 0, 6, 0, 8, 0, 0, 0, 5, 1, 0, 2, 4, 0, 0, 5, 1, 9, 5, 0, 9, 5, 1, 5, 0, 2, 0, 10, 5, 10, 9, 0, 10, 6, 6, 12, 0, 9, 0, 5, 6, 9, 8, 10, 9, 15, 0, 2, 5, 0, 0, 0, 5, 0, 0, 6, 5, 10, 5, 0, 6, 1, 5, 0, 3, 9, 7, 0, 0, 0, 8, 1, 9, 14, 10, 4, 4, 0, 9, 0, 5, 10, 0, 0, 9, 5, 10, 10, 4, 1, 0, 1, 11, 4, 0, 1, 5, 1, 9, 0, 2, 0, 0, 0, 0, 6, 9, 0, 0, 0, 9, 11, 0, 5, 1, 10, 0, 5, 0, 5, 8, 8, 5, 9, 5, 0, 7, 0, 1, 0, 9, 0, 9, 0, 0, 9, 10, 0, 0, 11, 0, 5, 9, 6, 10, 5, 5, 0, 5, 11, 14, 6, 5, 0, 11, 7, 4, 1, 0, 0, 0, 0, 0, 5, 0, 8, 0, 4, 8, 0, 0, 0, 0, 0, 6, 1, 5, 10, 14, 19, 4, 0, 2, 6, 4, 14, 0, 0, 5, 10, 5, 8, 5, 10, 2, 6, 8, 9, 1, 0, 11, 9, 14, 11, 0, 0, 0, 8, 4, 9, 4, 5, 9, 0, 4, 0, 6, 6, 0, 1, 10, 5, 8, 0, 0, 10, 0, 6, 0, 4, 10, 2, 0, 0, 1, 5, 4, 10, 5, 9, 5, 5, 1, 0, 9, 6, 15, 10, 16, 4, 0, 0, 0, 9, 5, 10, 0, 10, 10, 0, 11, 0, 0, 0, 4, 0, 2, 1, 0, 0, 0, 13, 6, 5, 0, 0, 1, 0, 1, 0, 6, 4, 6, 1, 0, 0, 5, 0, 1, 13, 0, 0, 4, 0, 0, 17, 5, 8, 5, 0, 5, 1, 0, 11, 5, 0, 0, 10, 2, 9, 9, 0, 5, 18, 0, 5, 0, 10, 9, 0, 1, 5, 4, 10, 5, 5, 14, 1, 6, 5, 10, 1, 5, 13, 4, 4, 10, 11, 0, 0, 9, 4, 0, 15, 6, 5, 0, 13, 10, 0, 2, 0, 0, 14, 1, 0, 6, 0, 0, 0, 0, 0, 2, 1, 5, 0, 0, 8, 9, 10, 8, 0, 10, 5, 10, 5, 0, 10, 0, 0, 7, 8, 5, 1, 5, 9, 5, 8, 4, 0, 9, 5, 10, 14, 6, 0, 0, 10, 9, 1, 10, 11, 6, 0, 0, 6, 0, 10, 5, 1, 1, 0, 14, 0, 10, 5, 9, 9, 0, 14, 6, 6, 6, 6, 15, 9, 0, 0, 3, 9, 0, 0, 13, 0, 9, 0, 9, 0, 2, 4, 9, 0, 1, 9, 6, 0, 0, 15, 10, 0, 0, 0, 6, 2, 9, 2, 0, 6, 9, 0, 0, 14, 0, 6, 9, 9, 14, 1, 1, 0, 9, 2, 13, 0, 9, 5, 5, 6, 6, 9, 7, 0, 4, 0, 5, 11, 0, 15, 5, 0, 4, 12, 0, 4, 13, 10, 4, 6, 2, 0, 4, 2, 9, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 10, 0, 11, 0]\n",
      "503\n",
      "        training_bloc  closest_cluster_index  chosen_action_index  \\\n",
      "0                   1                    369                    6   \n",
      "1                   2                    610                    5   \n",
      "2                   3                    610                    5   \n",
      "3                   4                    425                    5   \n",
      "4                   5                    502                    6   \n",
      "...               ...                    ...                  ...   \n",
      "208001              9                    572                    0   \n",
      "208002             10                    372                    1   \n",
      "208003             11                    716                    4   \n",
      "208004             12                    572                    0   \n",
      "208005             14                    750                   25   \n",
      "\n",
      "        reward_value  optimal_policy  behavior_policy  optimal_action  \\\n",
      "0                  0             3.0              3.0               3   \n",
      "1                  0             3.0              3.0               3   \n",
      "2                  0             3.0              3.0               3   \n",
      "3                  0             3.0              3.0               3   \n",
      "4                  0             3.0              3.0               3   \n",
      "...              ...             ...              ...             ...   \n",
      "208001             0         99995.0          99995.0           99995   \n",
      "208002             0         99995.0          99995.0           99995   \n",
      "208003             0         99995.0          99995.0           99995   \n",
      "208004             0         99995.0          99995.0           99995   \n",
      "208005          -100             0.0              0.0               0   \n",
      "\n",
      "        training_set_id  \n",
      "0                     3  \n",
      "1                     3  \n",
      "2                     3  \n",
      "3                     3  \n",
      "4                     3  \n",
      "...                 ...  \n",
      "208001            99995  \n",
      "208002            99995  \n",
      "208003            99995  \n",
      "208004            99995  \n",
      "208005            99995  \n",
      "\n",
      "[208006 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "optimal_actions, qlearning_dataset_train_mod, q_equation_output = qlearning_for_optimal_actions(transition_mat = transition_mat, \n",
    "                                  reward_mat_final = reward_mat_final, \n",
    "                                  train_blocs = train_blocs,\n",
    "                                  closest_clusters = closest_clusters,\n",
    "                                  train_chosen_actions = train_chosen_actions,\n",
    "                                  train_id_list = train_id_list,\n",
    "                                  all_lower_ranges = all_lower_ranges,\n",
    "                                  discount_val = 0.99, \n",
    "                                  num_of_iterations_val = 200000, \n",
    "                                  scheck_val = False, \n",
    "                                  is_debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Preparing the Physican's policy for comparison\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "# soften_policy_probabilites is a method that takes 'physician_policy' (The phyiscian's MDP)\n",
    "# and a random_factor (random_factor = 0.01) in order to insure that policies are \n",
    "# not guarenteed to be followed. It constructs a modified version of the physician\n",
    "# policy with those parameters. Included also are the total number of states and actions,\n",
    "# as well as a list of optimal actions from a state\n",
    "#\n",
    "# Parameters: \n",
    "# physician_policy: 2D Matrix of State -> Action reward values\n",
    "# state_count: Total Number of States in the MDP (Default 750)\n",
    "# number_actions: Total Number of Actions per a State in the MDP (Default 25)\n",
    "# optimal_state_actions: Optimal actions determined by Q-Learning for a given state\n",
    "# qlearning_dataset: The set used in the Q-Learning operations\n",
    "# random_factor: How often we want to assume Phyisicians are going to make improper choices\n",
    "# \n",
    "# Returns:\n",
    "# A DataFrame of all the softened policies\n",
    "\"\"\"\n",
    "def soften_policy_probabilites(physician_policy:float_matrix2D, state_count:int, number_actions:int, optimal_state_actions:List[int],\n",
    "                                  qlearning_dataset:pd.DataFrame,  random_factor:float = 0.01) -> pd.DataFrame:\n",
    "    # A constant that soften's the impact of the phyisican's individual choices\n",
    "    optimal_physician_policy:pd.DataFrame = physician_policy[:]\n",
    "    empty_actions:List[bool] = [False for i in range(0, number_actions)]\n",
    "\n",
    "    for i in range(0, state_count):\n",
    "        for j in range(0, number_actions):\n",
    "            empty_actions[j] = abs(0 - optimal_physician_policy[j][i]) <= 0.0001\n",
    "        # A random probability factor for all the zero actions present\n",
    "        empty_action_count:float = sum(empty_actions)\n",
    "        zero_softener:float = 0.0\n",
    "        non_zero_softener:float = 0.0\n",
    "        # These are for the cases where all actions are filled or none of them are for a given state\n",
    "        if empty_action_count != 0:\n",
    "            zero_softener = random_factor / empty_action_count\n",
    "        if empty_action_count != state_count:\n",
    "            # A softening factor for non-zero actions present\n",
    "            non_zero_softener = random_factor / (number_actions - empty_action_count)\n",
    "        # Set the physician policies based on the softening values\n",
    "        for j in range(0, number_actions):\n",
    "            # If the action is a 'zero' action, set it to the softened zero policy\n",
    "            if empty_actions[j]:\n",
    "                optimal_physician_policy[j][i] = zero_softener\n",
    "            else:\n",
    "            # If the action is a non-zero action, soften the probability\n",
    "            # of the phyisican \n",
    "                optimal_physician_policy[j][i] = optimal_physician_policy[j][i] - non_zero_softener\n",
    "            \n",
    "    # Optimal Policy = Target Policy = Evaluation Policy\n",
    "    behavior_physician_policy:float_matrix2D = [[random_factor/24.0 for i in range(state_count + 2)] for j in range(0, number_actions)]\n",
    "    \n",
    "    # The optimal policy should be taken almost all the time (1 - random_factor)\n",
    "    for i in range(0, state_count):\n",
    "        behavior_physician_policy[optimal_state_actions[i]][i] = 1 - random_factor\n",
    "    # Use the probabilities of policies in combination with the\n",
    "    # Known QL Data to make the model practical and realistic\n",
    "    for i in range(0, len(qlearning_dataset.index)):\n",
    "        # As long as it's not the death or life condition\n",
    "        if qlearning_dataset.iloc[i]['closest_cluster_index'] < state_count:\n",
    "            state_val = int(qlearning_dataset.iloc[i]['closest_cluster_index'])\n",
    "            action_val = int(qlearning_dataset.iloc[i]['chosen_action_index'])\n",
    "            # Save the optimal policy, behavior policy, and optimal action in the modified location\n",
    "            qlearning_dataset.at[i, 'optimal_policy'] = optimal_physician_policy[action_val][state_val]\n",
    "            qlearning_dataset.at[i, 'behavior_policy'] = behavior_physician_policy[action_val][state_val]\n",
    "            qlearning_dataset.at[i, 'optimal_action'] = optimal_state_actions[state_val]\n",
    "    \n",
    "    return qlearning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearning_train_dataset_final = soften_policy_probabilites(physician_policy = physician_policy.astype('float64'), state_count = state_count, number_actions = total_actions, optimal_state_actions = optimal_actions, qlearning_dataset=qlearning_dataset_train_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# At this point. we have determined a programmatic Q-equation based on our\n",
    "# data, we have created a probablistic matrix based on the occurence of\n",
    "# 'policies' or actions determined based on state. Now, we need to evaluate\n",
    "# how our model performs against the physician's choices\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_Q_learning_eval is a method that takes 6 arguments and returns 2 items\n",
    "# This method evaluates the performance of the MDP determined previously\n",
    "# \n",
    "# Parameters:\n",
    "# ql_train_set_Q: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# alpha: A hyperparameter that weights our reward function at each step\n",
    "# numtraces: Number of Q-Learning iterations we would like to perform\n",
    "# num_actions: Total number of actions in the set (For Sepsis: 25)\n",
    "# num_clusters: Total number of states in the set (For Sepsis: 752)\n",
    "# \n",
    "# Returns:\n",
    "# Q_Equation = The set of Q-Values obtained by the algorithm's performance\n",
    "# sum_Q_values = The Q-Equation's performance at a given step in the algorithm\n",
    "\"\"\"\n",
    "def offpolicy_Q_learning_eval(ql_train_set_Q: pd.DataFrame, gamma: float, alpha: float, numtraces: int,\n",
    "                        num_actions: int, num_clusters: int, stopping_difference: float) -> (float_matrix2D, List[float]):\n",
    "    # We need to save the Q-value for each run \n",
    "    sum_Q_values:List[float] = np.zeros((numtraces))\n",
    "    # Where the Q-Values are saved at each given run\n",
    "    Q_Equation:int_matrix2D = np.zeros((num_actions, num_clusters))\n",
    "    # A perfect Q-value for a given action is the max that can be obtained\n",
    "    max_avg_Q:int = 1\n",
    "    # How often do we want to check if we are making progress\n",
    "    modulus_val:int = 5000\n",
    "    # The list of all starting patient states \n",
    "    first_index_list:List[int] = ql_train_set_Q[ql_train_set_Q['training_bloc'] == 1].index\n",
    "    # A seperate index running in parallel with i for the sum_Q_values\n",
    "    jj:int = 0\n",
    "    # We iterate for the total number of times we want to do this process\n",
    "    for i in range(0, numtraces):\n",
    "        # Select a random patient starting point from the data\n",
    "        patient_starter_index:int = random.choice(first_index_list)\n",
    "        # As Q-learning progreses, we need a data structure to track progress\n",
    "        full_trace:List[Tuple[float, int, int]] = []\n",
    "        # While we are still working on a single patient\n",
    "        num_ql_rows = len(ql_train_set_Q.index)\n",
    "        # We run until we hit the end of the patient or the end of the dataset\n",
    "        while patient_starter_index + 1 < num_ql_rows and ql_train_set_Q.iloc[patient_starter_index + 1]['training_bloc'] != 1:\n",
    "            # Grab state (Initial State at this point)\n",
    "            state_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['closest_cluster_index']\n",
    "            # Grab action taken from this point\n",
    "            action_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['chosen_action_index']\n",
    "            # Grab reward provided by taken an action from this state to the next\n",
    "            reward_value:float = ql_train_set_Q.iloc[patient_starter_index + 1]['reward_value']\n",
    "            # A 'step' in the trace, a single data point snapshot\n",
    "            trace_step:Tuple[float, int, int] = (reward_value, state_index, action_index)\n",
    "            # Add the step to the full trace\n",
    "            full_trace.append(trace_step)\n",
    "            # Increment the current data row\n",
    "            patient_starter_index = patient_starter_index + 1\n",
    "        # Full length of the trace path\n",
    "        trace_length:int = len(full_trace)\n",
    "        # Grab the final reward (final reward for last state)\n",
    "        return_reward:float = full_trace[trace_length - 1][0]\n",
    "        # Walk the trace stack backwards\n",
    "        for j in range(trace_length - 2, -1, -1):\n",
    "            # Grab the state, action, and reward at each step\n",
    "            step_state:int = full_trace[j][1]\n",
    "            step_action:int = full_trace[j][2]\n",
    "            # Using alpha blending (where we take a portion of the old value and blend it with the new)\n",
    "            # We blend part of the old value of this state with the new value\n",
    "            Q_Equation[step_action, step_state] = (1 - alpha) * Q_Equation[step_action, step_state] + alpha * return_reward\n",
    "            # Recall we have a gamma value to determine the impact of previous decisions on future ones\n",
    "            # Note: this is a Hyperparameter (a parameter on the model itself)\n",
    "            return_reward = return_reward * gamma  + full_trace[j][0]\n",
    "        # Save the overall value based on the current states and actions avaiable at the \n",
    "        # current iteration\n",
    "        sum_Q_values[jj] = np.sum(Q_Equation)\n",
    "        jj = jj + 1\n",
    "        # If we haven't hit our max iterations, we still want to see if we should keep pushing forward\n",
    "        # If there is no noticable progress, we want to stop\n",
    "        \n",
    "        # Perform a check every modulus_val runs\n",
    "        if i % modulus_val == 0:\n",
    "            # Grab the current slice of unchecked {modulus_val} values\n",
    "            slice_mean:float = np.mean(sum_Q_values[j - modulus_val:j])\n",
    "            # Calculate the difference between current and last average \n",
    "            max_difference:float =(slice_mean - max_avg_Q)/max_avg_Q\n",
    "            # Check if the average of this {modulus_val} values is less than 0.001 away from the previous\n",
    "            if abs(max_difference) < stopping_difference:\n",
    "                break\n",
    "            max_avg_Q = slice_mean\n",
    "            \n",
    "    # Trim off the portion of the list we did not use\n",
    "    sum_Q_values = sum_Q_values[0:jj]\n",
    "    return Q_Equation, sum_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# parallel_bootql_creation is a function intended to be used\n",
    "# with the multithraeding package availabe in Python\n",
    "# It takes arguments passed in from it's parent function: offpolicy_eval_tdlearning\n",
    "#\n",
    "# unique_training_set_ids: All the patient ids for the training set\n",
    "# proportion: The proportion of the data to be used for training\n",
    "# gamma:  A hyperparameter that determines how much future values are valued\n",
    "# qlearning_train_dataset_final: The full dataset of value\n",
    "# physician_policy: A 25 x 750 matrix of the proabalities of actions a phyiscian chosen given a state (physician_policy[A][S])\n",
    "# distribution_values: A 750 long array that stores frequency of state appears\n",
    "# alpha: A hyperparameter for determining the amount we consider previous weights\n",
    "# numtraces: How many runs of algorithm are conducted\n",
    "# num_actions: How many actions are possible in the MDP\n",
    "# num_clusters: How many clusters are in the MDP\n",
    "# \n",
    "# and returns\n",
    "# \n",
    "# boot_ql_results: The rewards for each state weighted by frequency\n",
    "\"\"\"\n",
    "def parallel_bootql_creation(unique_training_set_ids: pd.DataFrame, proportion: float, \n",
    "                             gamma: float, qlearning_train_dataset_final: pd.DataFrame, \n",
    "                             phys_pol: float_matrix2D, distribution_values: List[float],\n",
    "                             alpha:float,  numtraces:int, num_actions:int, num_clusters:int) -> float_matrix2D:\n",
    "    # Grab a random sample of the ids to use for the Q-LEARNING step\n",
    "    train_len:int = len(unique_training_set_ids)\n",
    "    # We are going to randomly mark around (proportion) number of IDs\n",
    "    # and mark them as 1's to filter\n",
    "    random_number_set:List[float] = np.random.random_sample(train_len)\n",
    "    id_flags:List[float] = [np.floor(random_number_set[i] + proportion) == 1 for i in range(0, train_len)]\n",
    "    full_count:int = 0\n",
    "    chosen_ids:pd.DataFrame = unique_training_set_ids[id_flags]\n",
    "    # Choose the rows with training_ids begin at the 1\n",
    "    valid_training_set_ids:pd.DataFrame = qlearning_train_dataset_final['training_set_id'].isin(chosen_ids)\n",
    "    ql_train_set_proto:pd.DataFrame = qlearning_train_dataset_final[valid_training_set_ids]\n",
    "    ql_train_set:pd.DataFrame = pd.concat((ql_train_set_proto['training_bloc'],\n",
    "                             ql_train_set_proto['closest_cluster_index'], \n",
    "                             ql_train_set_proto['chosen_action_index'], \n",
    "                             ql_train_set_proto['reward_value']),\n",
    "                             axis=1,\n",
    "                             keys=['training_bloc',\"closest_cluster_index\",\"chosen_action_index\",\"reward_value\"]\n",
    "                            )\n",
    "    # Reset the indexes to start counting from 0\n",
    "    ql_train_set.reset_index(inplace=True, drop=True)\n",
    "    # Use the train set to achieve optimal Q-Equation\n",
    "    # Defaults are gamma = 0.99, alpha = 0.1, numtraces = 30000, num_actions = 25, num_clusters=750, \n",
    "    # stopping difference = 0.001\n",
    "    Q_equation, _ = offpolicy_Q_learning_eval(ql_train_set_Q=ql_train_set, gamma=gamma, alpha=alpha, numtraces=numtraces, \n",
    "                                         num_actions=num_actions, num_clusters=num_clusters, stopping_difference=0.001)\n",
    "    # Value the phyisican's decision based on Q-Learning probabilistic \n",
    "    # distribution and the actual chosen policy at each step\n",
    "    value_matrix = np.zeros((num_actions, num_clusters))\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_clusters):\n",
    "            value_matrix[i][j] = phys_pol[i][j] * Q_equation[i][j]\n",
    "    # The reward of a given state is the sum of the rewards of all possible actions \n",
    "    # That can be taken from that state\n",
    "    value_sums = np.zeros(num_clusters)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_clusters):\n",
    "            value_sums[j] = value_sums[j] + value_matrix[i][j]\n",
    "    # Return the rewards for each state, weighted by their probability of\n",
    "    # occuring (gathered from frequency in the actual data)\n",
    "    return np.nansum(np.multiply(value_sums, distribution_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_eval_tdlearning is a method that takes 4 arguments and returns 1 item\n",
    "# \n",
    "# Parameters:\n",
    "# qlearning_train_dataset_final: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# num_iter: Number of Q-Learning iterations we would like to perform\n",
    "# alpha: A hyperparameter for determining the amount we consider previous weights\n",
    "# numtraces: How many runs of algorithm are conducted\n",
    "# num_actions: How many actions are possible in the MDP\n",
    "# num_clusters: How many clusters are in the MDP\n",
    "# \n",
    "# Returns:\n",
    "# boot_ql_results = The set of Q-Values obtained by the algorithm's performance\n",
    "\"\"\"\n",
    "def offpolicy_eval_tdlearning(qlearning_train_dataset_final: pd.DataFrame, \n",
    "                              phys_pol: float_matrix2D, gamma: float, \n",
    "                              num_iter: int, num_patients_used: int,\n",
    "                              alpha:float,  numtraces:int, num_actions:int, num_clusters:int) -> float_matrix2D:\n",
    "    # physician_policy is a 25 x 752 matrix\n",
    "    unique_training_set_ids:List[int] = np.unique(qlearning_train_dataset_final['training_set_id'])\n",
    "    # 5000 total patient samples are going to be used for this training\n",
    "    proportion:float = num_patients_used/len(unique_training_set_ids)   \n",
    "    # At max, 75% of all samples are to be used\n",
    "    true_proportion:float = min(proportion, 0.75)    \n",
    "    first_blocs:pd.DataFrame = qlearning_train_dataset_final['training_bloc'] == 1\n",
    "    first_clusters:pd.DataFrame = qlearning_train_dataset_final[first_blocs]['closest_cluster_index']\n",
    "    distribution_values = np.zeros((num_clusters))\n",
    "    for i in range(0, num_clusters):\n",
    "        distribution_values[i] = sum(first_clusters == i)\n",
    "    distribution_values = np.divide(distribution_values, sum(distribution_values))\n",
    "    \n",
    "    boot_ql_results:float_matrix2D = parallel_bootql_creation(unique_training_set_ids=unique_training_set_ids, \n",
    "                                                              proportion=true_proportion, \n",
    "                                                              gamma=gamma, \n",
    "                                                              qlearning_train_dataset_final=qlearning_train_dataset_final, \n",
    "                                                              phys_pol=phys_pol, \n",
    "                                                              distribution_values=distribution_values,\n",
    "                                                              alpha=alpha,\n",
    "                                                              numtraces=numtraces,\n",
    "                                                              num_actions=num_actions,\n",
    "                                                              num_clusters=num_clusters)\n",
    "    return boot_ql_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to load the ctypes library file I custom baked for speedup on these runs\n",
    "wislib = ctypes.windll.LoadLibrary(\"g:/JuptyerScripts/FUTON_Research/libwis.dll\")\n",
    "# All ctypes functions have to have their parameters defined in the context of c_type variables\n",
    "wislib.performanceEvaluation.argtypes = [ctypes.c_double, ctypes.c_double, \n",
    "                                         ctypes.c_double, ctypes.c_double, \n",
    "                                         ctypes.POINTER(ctypes.c_double), ctypes.c_int, \n",
    "                                         ctypes.POINTER(ctypes.c_double), ctypes.c_int, \n",
    "                                         ctypes.POINTER(ctypes.c_double), ctypes.c_int, \n",
    "                                         ctypes.c_int, ctypes.c_int]\n",
    "# The return type of functions have to be defined in the context of c_type variables\n",
    "wislib.performanceEvaluation.restype = ctypes.POINTER(ctypes.c_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Weighted Importance Sampling (WIS) is a modern off-policy learning approach that \n",
    "# makes use of linear function approximation. \n",
    "# https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf\n",
    "# In this research, it is a useful approximation that can be used to evaluate the physician's policy perfomance\n",
    "# This will be used to measure performance against the AI policy\n",
    "# \n",
    "# Parameters:\n",
    "# ql_train_set: The full dataset to be used in the training procedure\n",
    "# gamma: A hyperparameter for evaluating how much the model considers previous decisions\n",
    "# iter_wis: How many iterations of the WIS algorithm are going to be conducted\n",
    "# \n",
    "# Returns: \n",
    "# A float_matrix2D which contains the rewards for the physician choosing invidual actions \n",
    "# given a particular state\n",
    "\"\"\"\n",
    "def offpolicy_eval_wis(ql_train_set: pd.DataFrame, gamma: float, iter_wis: int, num_patients_used: int, cpp_speedup:bool) -> float_matrix2D:\n",
    "    phys_wis_policy_results:List[float] = np.zeros(iter_wis)\n",
    "    unique_training_set_ids:List[int] = np.unique(qlearning_train_dataset_final['training_set_id'])\n",
    "    # Iterate iter_wis times\n",
    "    for i in range(0, iter_wis):\n",
    "        # 25000 total patient samples are going to be used for this training\n",
    "        proportion:float = num_patients_used/len(unique_training_set_ids)   \n",
    "        # At max, 75% of all samples are to be used\n",
    "        true_proportion:float = min(proportion, 0.75)   \n",
    "        # Grab a random sample of the ids to use for the Q-LEARNING step\n",
    "        train_len:int = len(unique_training_set_ids)\n",
    "        # We are going to randomly mark around (proportion) number of IDs\n",
    "        # and mark them as 1's to filter\n",
    "        random_number_set:List[float] = np.random.random_sample(train_len)\n",
    "        id_flags:List[int] = [np.floor(random_number_set[i] + proportion) == 1 for i in range(0, train_len)]\n",
    "        full_count:int = 0\n",
    "        chosen_ids = unique_training_set_ids[id_flags]\n",
    "        # Choose the rows with training_ids begin at the 1\n",
    "        valid_training_set_ids:pd.DataFrame = qlearning_train_dataset_final['training_set_id'].isin(chosen_ids)\n",
    "        wis_train_set:pd.DataFrame = qlearning_train_dataset_final[valid_training_set_ids]\n",
    "        # Reset the indexes to start counting from 0\n",
    "        wis_train_set.reset_index(inplace=True, drop=True)\n",
    "        # Grabbing a list variant of behavior, optimal_policy, reward_value\n",
    "        behavior_policy_values:List[np.float64] = wis_train_set['behavior_policy']\n",
    "        optimal_policy_values:List[np.float64] = wis_train_set['optimal_policy']\n",
    "        reward_policy_values:List[np.float64] = wis_train_set['reward_value']\n",
    "        # Create the values outside the scope to be used later\n",
    "        behParam:ctypes.POINTER(ctypes.c_double) = []\n",
    "        optParam:ctypes.POINTER(ctypes.c_double) = []\n",
    "        rewParam:ctypes.POINTER(ctypes.c_double) = []\n",
    "        behParamLen:ctypes.c_int = 0   \n",
    "        optParamLen:ctypes.c_int = 0\n",
    "        rewParamLen:ctypes.c_int = 0\n",
    "        if cpp_speedup:\n",
    "            # Run the code using C-Types to speed FLOPS up\n",
    "            behParamLen = len(behavior_policy_values.index)\n",
    "            optParamLen = len(optimal_policy_values.index)\n",
    "            rewParamLen = len(reward_policy_values.index)\n",
    "            behVals:ctypes.POINTER(ctypes.c_double) = ctypes.c_double * behParamLen\n",
    "            optVals:ctypes.POINTER(ctypes.c_double) = ctypes.c_double * optParamLen\n",
    "            rewVals:ctypes.POINTER(ctypes.c_double) = ctypes.c_double * rewParamLen\n",
    "            behParam:ctypes.POINTER(ctypes.c_double) = behVals(*behavior_policy_values)\n",
    "            optParam:ctypes.POINTER(ctypes.c_double) = optVals(*optimal_policy_values)\n",
    "            rewParam:ctypes.POINTER(ctypes.c_double) = rewVals(*reward_policy_values)\n",
    "        # A trial will be performed for every chosen ID\n",
    "        num_of_trials:int = len(chosen_ids)\n",
    "        # We need to store the results of every trial\n",
    "        trial_results:List[np.float64] = np.zeros(num_of_trials)\n",
    "        # The list of all starting patient states \n",
    "        first_index_list:List[int] = wis_train_set[wis_train_set['training_bloc'] == 1].index\n",
    "        # We need to store the likelihood of state-actions (rho values) distrubtions that are determined programmatically\n",
    "        distribution_values = np.zeros(num_of_trials)\n",
    "        # Run through the trials to evaluate individual performance for a patient\n",
    "        for j in range(0, num_of_trials - 1):\n",
    "            # Some initial state values to prepare for a given trial\n",
    "            current_trial_evaluation:ctypes.c_double = 0.0\n",
    "            rho_value:np.float64 = 1.0\n",
    "            discount_value:np.float64 = 1/gamma\n",
    "            if cpp_speedup:\n",
    "                resultPointer = wislib.performanceEvaluation(ctypes.c_double(current_trial_evaluation), ctypes.c_double(rho_value), \n",
    "                                            ctypes.c_double(discount_value), ctypes.c_double(gamma), \n",
    "                                            behParam, behParamLen, optParam, \n",
    "                                             optParamLen, rewParam, \n",
    "                                             rewParamLen, first_index_list[j], first_index_list[j+1] - 1)\n",
    "                distribution_values[j] = resultPointer[0]\n",
    "                trial_results[j] = resultPointer[0] * resultPointer[1]\n",
    "            else:\n",
    "                # Iterate over all the data for an individual patient except for the last two (Terminal state and next step)\n",
    "                for k in range(first_index_list[j], first_index_list[j+1] - 1):\n",
    "                    rho_value = rho_value * (behavior_policy_values[k] / optimal_policy_values[k])\n",
    "                    # Apply our new discount over every iteration, constantly encouraging growth weighting\n",
    "                    discount_value = discount_value * gamma\n",
    "                    # Add our new reward/penalty\n",
    "                    current_trial_evaluation = current_trial_evaluation + discount_value * reward_policy_values[k + 1]\n",
    "                distribution_values[j] = rho_value\n",
    "                trial_results[j] = current_trial_evaluation * rho_value\n",
    "        # Store the average performance of the runs in the global results\n",
    "        phys_wis_policy_results[i] = np.nansum(trial_results)/np.nansum(distribution_values)\n",
    "    return phys_wis_policy_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_multiple_eval_parent is a method that takes 6 arguments and returns 2 items\n",
    "# \n",
    "# Parameters:\n",
    "# qlearning_train_dataset_final: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# do_ql: Whether or not we want to perform Q-Learning or use a previously determined average value\n",
    "# iter_ql: How many iterations of the Q-Learning algorithm we want to do\n",
    "# iter_wis: How many iterations of the Wis algorithm we want to do\n",
    "# alpha: A hyperparameter for determining the amount we consider previous weights\n",
    "# numtraces: How many runs of algorithm are conducted\n",
    "# num_actions: How many actions are possible in the MDP\n",
    "# num_clusters: How many clusters are in the MDP\n",
    "# num_patients_used: How many patients do we want to run the performance evaluations on\n",
    "# \n",
    "# Returns:\n",
    "# bootql = The set of Q-Values obtained by the algorithm's performance\n",
    "# bootwis = The set of WIS values obtained by the algorithm's performance \n",
    "\"\"\"\n",
    "def offpolicy_multiple_eval_parent(qlearning_train_dataset_final: pd.DataFrame, phys_pol: float_matrix2D, \n",
    "                                   gamma: float, do_ql: bool, iter_ql: int, \n",
    "                                   iter_wis: int, cpp_speedup:bool, \n",
    "                                   alpha:float, numtraces:int, num_actions:int, \n",
    "                                   num_clusters:int, num_patients_used:int) -> (float_matrix2D, float_matrix2D):\n",
    "    phys_ql_results = []\n",
    "    # A boolean flag to determine if q_learning is going to be performed or not\n",
    "    if do_ql:\n",
    "        phys_ql_results = offpolicy_eval_tdlearning(qlearning_train_dataset_final=qlearning_train_dataset_final, \n",
    "                                                    phys_pol=phys_pol, gamma=gamma, num_iter=iter_ql,\n",
    "                                                    alpha=alpha,  numtraces=numtraces, num_actions=num_actions, \n",
    "                                                    num_clusters=num_clusters, num_patients_used=num_patients_used)\n",
    "    else:\n",
    "        # This is an abitrary average value determined by the previous research group\n",
    "        phys_ql_results = [55]\n",
    "    print(\"Mean value of physicians' policy by TD Learning : \" + str(np.nanmean(phys_ql_results)) + \"\\n\")\n",
    "    \n",
    "    phys_wis_results = offpolicy_eval_wis(ql_train_set=qlearning_train_dataset_final, gamma=gamma, \n",
    "                                          iter_wis=iter_wis, num_patients_used=num_patients_used, \n",
    "                                          cpp_speedup=cpp_speedup)\n",
    "    \n",
    "    print(\"Mean value of AI policy by WIS : \" + str(np.nanmean(phys_wis_results)) + \"\\n\")\n",
    "    return phys_ql_results, phys_wis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value of physicians' policy by TD Learning : -45.201540099008795\n",
      "\n",
      "Mean value of AI policy by WIS : -74.75009390975558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normal value for iter_ql is 6, iter_wis is 750\n",
    "# Q-Learning results is usually a positive value\n",
    "# WIS Value should be Negative\n",
    "\n",
    "offpolicy_return_values:(float_matrix2D, float_matrix2D) = offpolicy_multiple_eval_parent(\n",
    "        qlearning_train_dataset_final=qlearning_train_dataset_final, \n",
    "        phys_pol=physician_policy.astype('float64'), \n",
    "        gamma=0.99, \n",
    "        do_ql=True, \n",
    "        iter_ql=6, \n",
    "        iter_wis=10,\n",
    "        cpp_speedup=True,\n",
    "        alpha=0.1,\n",
    "        numtraces=30000,\n",
    "        num_actions=total_actions,\n",
    "        num_clusters=state_count,\n",
    "        num_patients_used=25000)\n",
    "    \n",
    "phys_ql_results:float_matrix2D = offpolicy_return_values[0]\n",
    "phys_wis_results:float_matrix2D = offpolicy_return_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data for each model needs to be saved\n",
    "\n",
    "# Save the number of model we tried\n",
    "model_data[loop_index, 0] = loop_index\n",
    "# Save the average and 99% quantile for the results of the Q-Learning for train data\n",
    "model_data[loop_index, 3] = np.nanmean(phys_ql_results)\n",
    "model_data[loop_index, 4] = np.quantile(phys_ql_results, 0.99)\n",
    "# Save the average and 99% quantile for the results of the WIC validation for train data\n",
    "model_data[loop_index, 5] = np.nanmean(phys_wis_results)\n",
    "model_data[loop_index, 6] = np.quantile(phys_wis_results, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that initial performance for the training dataset has been evaluated, it is time to begin\n",
    "# doing the same steps to generated data that we want to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix_test(test_zscores:pd.DataFrame, cluster_values:List[List[int]], patientdata:pd.DataFrame,\n",
    "                          test_90d:pd.DataFrame, train_flag:pd.DataFrame, patient_idxs:pd.DataFrame, loop_index:int,\n",
    "                          action_list:List[int], is_debug:bool = False, id_tag:str = \"icustayid\") -> (pd.DataFrame):\n",
    "    # Now that initial performance for the training dataset has been evaluated, it is time to begin\n",
    "    # doing the same steps to generated data that we want to test \n",
    "    # First, the QL dataset must be reconstructed for the test data\n",
    "\n",
    "    # Using the clusters created by KNN, and the ZScores of the test data, get the closest clusters for the test\n",
    "    # data\n",
    "    closest_clusters_test:List[int] = vq(test_zscores, cluster_values)[0]\n",
    "    # Flag patient IDs based on if they are in the test group and save them for evaluation later\n",
    "    test_patient_flags = pd.DataFrame(patientdata[id_tag].unique()).isin(patientdata[~train_flag][id_tag].unique()).values.tolist()\n",
    "    # Denote what IDs were used for a given loop\n",
    "    test_cluster_index:int = 0\n",
    "    for i in range(0, len(test_patient_flags)):\n",
    "        # Have to use [0] here because tolist in dataframe does not automatically flatten the list\n",
    "        if test_patient_flags[i][0] == True:\n",
    "            # Save the cluster for a given state for the flagged test data\n",
    "            patient_idxs[i][loop_index] = closest_clusters_test[test_cluster_index]\n",
    "            test_cluster_index = test_cluster_index + 1\n",
    "\n",
    "    # Generate the actions for the test set of data\n",
    "    test_chosen_actions:pd.Series = pd.Series(action_list)[~train_flag]\n",
    "\n",
    "    # Fix the reward ranges\n",
    "    # Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "    # If they have died, the range is [-100, 100]\n",
    "    # If they are alive, the range is [100, -100]\n",
    "    range_vals:List[int] = [100, -100]\n",
    "    # Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "    # This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "    test_90d_polarity:List[int] = (2 * (1 - test_90d) - 1)\n",
    "    range_matrix:List[int] = [np.multiply(polarity, range_vals) for polarity in test_90d_polarity]\n",
    "    # Grab the lower range limit and upper range limit seperately in order to build the \n",
    "    all_lower_ranges:List[int] = [i[0] for i in range_matrix]\n",
    "    all_upper_ranges:List[int] = [i[1] for i in range_matrix]\n",
    "    # Generate the new dataset\n",
    "    qlearning_dataset_test:pd.DataFrame = pd.concat([pd.Series(test_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters_test), \n",
    "                                   pd.Series(test_chosen_actions.tolist()), \n",
    "                                   pd.Series(test_90d.tolist()), \n",
    "                                   pd.Series(all_lower_ranges), \n",
    "                                   pd.Series(all_upper_ranges)], \n",
    "                                  axis=1, sort=False)\n",
    "    qlearning_dataset_test.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', '90d_mortality_status', 'lower_range', 'upper_range']\n",
    "    # Convert to the proper datatypes\n",
    "    qlearning_dataset_test[\"training_bloc\"] = qlearning_dataset_test[\"training_bloc\"].astype(np.int64)\n",
    "    qlearning_dataset_test[\"closest_cluster_index\"] = qlearning_dataset_test[\"closest_cluster_index\"].astype(np.int64)\n",
    "    qlearning_dataset_test[\"chosen_action_index\"] = qlearning_dataset_test[\"chosen_action_index\"].astype(np.int64)\n",
    "    if is_debug:\n",
    "        print(qlearning_dataset_test)\n",
    "    return qlearning_dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearning_dataset_test = construct_matrix_test(test_zscores = test_zscores, cluster_values = cluster_values, \n",
    "                                               patientdata = patientdata,test_90d = test_90d, \n",
    "                                               train_flag = train_flag, patient_idxs = patient_idxs, \n",
    "                                               loop_index = loop_index, action_list = action_list, \n",
    "                                               is_debug = False, id_tag = \"icustayid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for qlLearning \n",
    "qlearning_dataset_test_mod:pd.DataFrame = modify_qlearning_dataset(qlearning_dataset_test, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soften the policies based on physician input \n",
    "qlearning_test_dataset_final = soften_policy_probabilites(physician_policy.astype('float64'), state_count, total_actions, optimal_actions, qlearning_dataset=qlearning_dataset_test_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value of physicians' policy by TD Learning : 55.0\n",
      "\n",
      "Mean value of AI policy by WIS : -80.12852600963062\n",
      "\n",
      "136.13140201568604\n"
     ]
    }
   ],
   "source": [
    "# Now that the tester set is finished, the same evaluation can be run on the test data, as was done on the train data\n",
    "# Normal value for iter_ql is 6, iter_wis is 750\n",
    "# Q-Learning results is usually a positive value\n",
    "# WIS Value should be Negative\n",
    "start_time = time.time()\n",
    "offpolicy_return_values_test:(float_matrix2D, float_matrix2D) = offpolicy_multiple_eval_parent(\n",
    "        qlearning_train_dataset_final=qlearning_test_dataset_final, \n",
    "        phys_pol=physician_policy.astype('float64'), \n",
    "        gamma=0.99, \n",
    "        do_ql=False, \n",
    "        iter_ql=6, \n",
    "        iter_wis=750,\n",
    "        cpp_speedup=True,\n",
    "        alpha=0.1,\n",
    "        numtraces=30000,\n",
    "        num_actions=total_actions,\n",
    "        num_clusters=state_count,\n",
    "        num_patients_used=25000)\n",
    "print(time.time() - start_time)\n",
    "    \n",
    "phys_ql_results_test:float_matrix2D = offpolicy_return_values_test[0]\n",
    "phys_wis_results_test:float_matrix2D = offpolicy_return_values_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all important and desired metrics into memory\n",
    "\n",
    "# Model's Q-Learning performance against Physician at the 95% quantile mark\n",
    "model_data[loop_index, 18] = np.quantile(phys_ql_results_test, 0.95)\n",
    "# Model's Q-learning Mean performance against Physician overall\n",
    "model_data[loop_index, 19] = np.nanmean(phys_ql_results_test)\n",
    "# Model's Q-Learning performance against Physician at the 95% quantile mark\n",
    "model_data[loop_index, 20] = np.quantile(phys_ql_results_test, 0.99)\n",
    "# Model's WIS performance against Physician overall\n",
    "model_data[loop_index, 21] = np.nanmean(phys_wis_results_test)\n",
    "# Model's WIS performance against Physician at 1% quantile\n",
    "model_data[loop_index, 22] = np.quantile(phys_wis_results_test, 0.01)\n",
    "# Model's WIS performance against Physician at 5% quantile\n",
    "model_data[loop_index, 23] = np.quantile(phys_wis_results_test, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Memory to file every 5 runs\n",
    "if loop_index % 5 == 0:\n",
    "    with open('model_data.txt', 'wb') as fp:\n",
    "                pickle.dump(model_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file back into Memory\n",
    "model_data:pd.DataFrame = []\n",
    "with open ('model_data.txt', 'rb') as fp:\n",
    "    train_zscores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model doesn not perform well at the 95% Quantile, discard the\n",
    "# result, otherwise, we want to keep it\n",
    "bestmodels_data.append((loop_index,\n",
    " q_equation_output,\n",
    " physician_policy,\n",
    " transition_mat,\n",
    " reward_mat_final,\n",
    " cluster_values,\n",
    " train_ids,\n",
    " qlearning_train_dataset_final,\n",
    " qlearning_test_dataset_final,\n",
    " optimal_actions))\n",
    "\n",
    "\n",
    "# Left out reverse Transition Matrix\n",
    "# Left out Alternate dataset \n",
    "loop_index = loop_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
