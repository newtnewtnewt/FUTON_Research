{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  FUTON Model MDP + Q-Learning Creation Script\n",
    "#  A Research Project conducted by Noah Dunn \n",
    "###\n",
    "\n",
    "# Import the standard tools for working with Pandas dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve\n",
    "# Import the MDP toolbox that contains a method for conducting Q-Learning\n",
    "# Tool can be found here: https://github.com/sawcordwell/pymdptoolbox\n",
    "# Documentation for the tool can be found here \n",
    "import mdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Data File that will be used to conduct the experiments\n",
    "patientdata = pd.read_csv(\"G:/MIMIC-ALL/MIMIC-PATIENTS/patient_data_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#  An MDP, or Markov Decision Process is used to model relationships between various states and actions.\n",
    "#  A state can be thought of in medical solution as a patient's diagnosis based on current vitals and state of being. \n",
    "#  An action can be thought of as a change in current diagnosis based on one of those vitals.\n",
    "#  The inspirations for the bulk of this code came from Komorowksi's AI Clinician which can be found \n",
    "#  here: https://github.com/matthieukomorowski/AI_Clinician/blob/master/AIClinician_core_160219.m\n",
    "###\n",
    "\n",
    "###\n",
    "# Begin by establishing some global variables for use in the MDP creation\n",
    "###\n",
    "mdp_count = 500            # The number of repititions we want/count of MDPs we need to create \n",
    "clustering_iter = 32       # The number of times clustering will be conducted\n",
    "cluster_sample = 0.25      # Proportion of the data used for clustering\n",
    "gamma = 0.99               # How close we desire clusters to be in similarity (Percentage)\n",
    "transition_threshold = 5   # The cutoff value for the transition matrix\n",
    "final_policies = 1         # The number of policies we would like to end up with\n",
    "state_count = 750          # The number of distinct states\n",
    "action_count = 5           # Number of actions per state (reccommended 2 to 10)\n",
    "crossval_iter = 10         # Number of crossvalidation runs (Default is 80% Train, 20% Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Data structures to hold our interim data\n",
    "###\n",
    "\n",
    "# Create the structures and fill them with NaN values\n",
    "optimal_actions = np.empty((state_count + 2, mdp_count,))  # Not sure the significance of the 2 yet\n",
    "optimal_actions[:] = np.nan\n",
    "\n",
    "\n",
    "model_data = np.empty((mdp_count*2, 30,))\n",
    "model_data[:] = np.nan\n",
    "\n",
    "bestmodels_data = np.empty((mdp_count, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21463\n"
     ]
    }
   ],
   "source": [
    "# Grab list of unique patient ICU stay IDs\n",
    "icu_ids = patientdata['icustayid'].unique()\n",
    "# Number of patients to be used for states\n",
    "id_count = icu_ids.size\n",
    "print(id_count)\n",
    "\n",
    "# Create a data structure to representing all patients\n",
    "patient_idxs = np.empty((id_count, mdp_count,))\n",
    "patient_idxs[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag\n",
      "0            0         1            0.0             0          1         0\n",
      "1            0         1            0.0             0          1         1\n",
      "2            0         1            0.0             0          1         1\n",
      "3            0         1            0.0             0          1         1\n",
      "4            0         1            0.0             0          1         1\n",
      "...        ...       ...            ...           ...        ...       ...\n",
      "238325       0         0            0.0             0          1         0\n",
      "238326       0         0            0.0             0          1         0\n",
      "238327       0         0            0.0             0          1         0\n",
      "238328       0         0            0.0             0          1         0\n",
      "238329       0         0            0.0             0          1         0\n",
      "\n",
      "[238330 rows x 6 columns] \n",
      "                 age  Weight_kg        GCS         HR       SysBP     MeanBP  \\\n",
      "0       17639.82644  75.062500  10.000000  77.800000  119.000000  82.000000   \n",
      "1       17639.82644  78.699997  10.000000  79.125000  111.375000  78.000000   \n",
      "2       17639.82644  78.699997  10.166667  77.833333  104.000000  76.000000   \n",
      "3       17639.82644  78.699997  11.000000  74.750000  121.500000  96.000000   \n",
      "4       17639.82644  78.699997  14.200000  98.200000  131.100000  98.600000   \n",
      "...             ...        ...        ...        ...         ...        ...   \n",
      "238325   8538.73934  74.000000  15.000000  90.750000  142.500000  85.500000   \n",
      "238326   8538.73934  88.400002  15.000000  86.250000  149.000000  87.000000   \n",
      "238327   8538.73934  73.000000  15.000000  84.000000  147.333333  98.111111   \n",
      "238328   8538.73934  73.000000  15.000000  68.750000  146.000000  99.333333   \n",
      "238329   8538.73934  73.000000  15.000000  75.666667  139.333333  89.000000   \n",
      "\n",
      "            DiaBP         RR     Temp_C  FiO2_1  ...      paCO2  Arterial_BE  \\\n",
      "0       63.200000  23.800000  37.422223    0.40  ...  38.750000       -1.375   \n",
      "1       59.500000  21.000000  37.319443    0.40  ...  40.000000        5.000   \n",
      "2       59.333333  20.333333  37.240740    0.40  ...  40.000000        5.000   \n",
      "3       80.000000  19.750000  37.333332    0.40  ...  40.000000        5.000   \n",
      "4       80.300000  23.400000  37.438887    0.61  ...  36.000000        3.000   \n",
      "...           ...        ...        ...     ...  ...        ...          ...   \n",
      "238325  70.500000  19.250000  37.083333    0.21  ...  38.000000        1.000   \n",
      "238326  70.666667  23.750000  36.999999    0.21  ...  34.000000        0.000   \n",
      "238327  78.333333  18.000000  36.822222    0.21  ...  51.363636        0.000   \n",
      "238328  83.500000  17.750000  36.555557    0.21  ...  51.363636        0.000   \n",
      "238329  76.333333  20.000000  36.555557    0.21  ...  51.363636        0.000   \n",
      "\n",
      "             HCO3  Arterial_lactate  SOFA  SIRS  Shock_Index    PaO2_FiO2  \\\n",
      "0       21.000000          1.900000     3     1     0.653782   434.687493   \n",
      "1       27.000000          1.300000     7     2     0.710438   207.499997   \n",
      "2       27.000000          1.900000     7     2     0.748397   207.499997   \n",
      "3       27.000000          1.200000     7     1     0.615226   207.499997   \n",
      "4       32.000000          2.200000     6     3     0.749047   165.573772   \n",
      "...           ...               ...   ...   ...          ...          ...   \n",
      "238325  22.333333          2.600000     1     2     0.636842   385.714286   \n",
      "238326  25.000000          2.600000     0     2     0.578859  1390.476190   \n",
      "238327  27.000000          1.354545     5     1     0.570136   314.718615   \n",
      "238328  27.000000          1.354545     1     1     0.470890   314.718615   \n",
      "238329  27.000000          1.354545     1     2     0.543062   314.718615   \n",
      "\n",
      "        cumulated_balance  qSOFA  \n",
      "0               -4690.000      2  \n",
      "1               -5215.000      1  \n",
      "2               -6015.000      1  \n",
      "3               -6615.000      1  \n",
      "4               -7055.000      2  \n",
      "...                   ...    ...  \n",
      "238325          -2546.417      0  \n",
      "238326          -3246.417      1  \n",
      "238327          -3246.417      0  \n",
      "238328          -3666.417      0  \n",
      "238329          -4186.417      0  \n",
      "\n",
      "[238330 rows x 33 columns] \n",
      "              SpO2    BUN  Creatinine   SGOT   SGPT  Total_bili  INR  \\\n",
      "0       98.400000  15.75       0.975  151.2  145.4        1.48  1.1   \n",
      "1       97.000000  10.00       0.700   38.0   23.0        4.90  1.3   \n",
      "2       97.166667  10.00       0.700   38.0   23.0        4.90  1.5   \n",
      "3       97.000000  10.00       0.700   38.0   23.0        4.90  1.1   \n",
      "4       97.400000  10.00       0.700   38.0   23.0        4.90  1.1   \n",
      "...           ...    ...         ...    ...    ...         ...  ...   \n",
      "238325  98.000000   6.00       0.800  139.0   46.0        0.20  1.0   \n",
      "238326  96.666667   6.00       0.800   19.0   19.0        0.20  1.1   \n",
      "238327  98.000000   6.00       0.800   60.0   31.0        0.80  1.7   \n",
      "238328  99.000000   6.00       0.800   60.0   31.0        0.80  1.7   \n",
      "238329  98.000000   6.00       0.800   60.0   31.0        0.80  1.7   \n",
      "\n",
      "        input_total  input_4hourly  output_total  output_4hourly  \n",
      "0          6297.000           30.0       10987.0           580.0  \n",
      "1          6347.000           50.0       11562.0           575.0  \n",
      "2          6397.000           50.0       12412.0           850.0  \n",
      "3          6447.000           50.0       13062.0           650.0  \n",
      "4          6477.000           30.0       13532.0           470.0  \n",
      "...             ...            ...           ...             ...  \n",
      "238325     2113.583            0.0        4660.0           600.0  \n",
      "238326     2113.583            0.0        5360.0           700.0  \n",
      "238327     2113.583            0.0        5360.0             0.0  \n",
      "238328     2113.583            0.0        5780.0           420.0  \n",
      "238329     2113.583            0.0        6300.0           520.0  \n",
      "\n",
      "[238330 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# All our columns are broken up into 3 distinct categories:\n",
    "# 1. Binary values (0 or 1)\n",
    "# 2. Standard Ranges (Plain old Integers + Decimals)\n",
    "# 3. Logarthmic Values (columnvalue = log(columnvalue))\n",
    "\n",
    "colbin = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag']\n",
    "colnorm = ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "    'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "    'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "    'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', 'qSOFA'];\n",
    "collog=['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly'];\n",
    "\n",
    "# Create seperate dataframes for each of the columns\n",
    "colbin_df = patientdata[colbin]\n",
    "colnorm_df = patientdata[colnorm]\n",
    "collog_df = patientdata[collog]\n",
    "\n",
    "# Let's make sure we have what we need\n",
    "print(colbin_df, \"\\n\", colnorm_df, \"\\n\", collog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag  \\\n",
      "0            0         1            0.0             0          1         0   \n",
      "1            0         1            0.0             0          1         1   \n",
      "2            0         1            0.0             0          1         1   \n",
      "3            0         1            0.0             0          1         1   \n",
      "4            0         1            0.0             0          1         1   \n",
      "...        ...       ...            ...           ...        ...       ...   \n",
      "238325       0         0            0.0             0          1         0   \n",
      "238326       0         0            0.0             0          1         0   \n",
      "238327       0         0            0.0             0          1         0   \n",
      "238328       0         0            0.0             0          1         0   \n",
      "238329       0         0            0.0             0          1         0   \n",
      "\n",
      "                age  Weight_kg        GCS         HR  ...    BUN  Creatinine  \\\n",
      "0       17639.82644  75.062500  10.000000  77.800000  ...  15.75       0.975   \n",
      "1       17639.82644  78.699997  10.000000  79.125000  ...  10.00       0.700   \n",
      "2       17639.82644  78.699997  10.166667  77.833333  ...  10.00       0.700   \n",
      "3       17639.82644  78.699997  11.000000  74.750000  ...  10.00       0.700   \n",
      "4       17639.82644  78.699997  14.200000  98.200000  ...  10.00       0.700   \n",
      "...             ...        ...        ...        ...  ...    ...         ...   \n",
      "238325   8538.73934  74.000000  15.000000  90.750000  ...   6.00       0.800   \n",
      "238326   8538.73934  88.400002  15.000000  86.250000  ...   6.00       0.800   \n",
      "238327   8538.73934  73.000000  15.000000  84.000000  ...   6.00       0.800   \n",
      "238328   8538.73934  73.000000  15.000000  68.750000  ...   6.00       0.800   \n",
      "238329   8538.73934  73.000000  15.000000  75.666667  ...   6.00       0.800   \n",
      "\n",
      "         SGOT   SGPT  Total_bili  INR  input_total  input_4hourly  \\\n",
      "0       151.2  145.4        1.48  1.1     6297.000           30.0   \n",
      "1        38.0   23.0        4.90  1.3     6347.000           50.0   \n",
      "2        38.0   23.0        4.90  1.5     6397.000           50.0   \n",
      "3        38.0   23.0        4.90  1.1     6447.000           50.0   \n",
      "4        38.0   23.0        4.90  1.1     6477.000           30.0   \n",
      "...       ...    ...         ...  ...          ...            ...   \n",
      "238325  139.0   46.0        0.20  1.0     2113.583            0.0   \n",
      "238326   19.0   19.0        0.20  1.1     2113.583            0.0   \n",
      "238327   60.0   31.0        0.80  1.7     2113.583            0.0   \n",
      "238328   60.0   31.0        0.80  1.7     2113.583            0.0   \n",
      "238329   60.0   31.0        0.80  1.7     2113.583            0.0   \n",
      "\n",
      "        output_total  output_4hourly  \n",
      "0            10987.0           580.0  \n",
      "1            11562.0           575.0  \n",
      "2            12412.0           850.0  \n",
      "3            13062.0           650.0  \n",
      "4            13532.0           470.0  \n",
      "...              ...             ...  \n",
      "238325        4660.0           600.0  \n",
      "238326        5360.0           700.0  \n",
      "238327        5360.0             0.0  \n",
      "238328        5780.0           420.0  \n",
      "238329        6300.0           520.0  \n",
      "\n",
      "[238330 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rearrange the dataframe in order of binary, normal, and log data from left to right\n",
    "MIMIC_raw = pd.concat([colbin_df, colnorm_df, collog_df], axis=1)\n",
    "print(MIMIC_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag  \\\n",
      "0         -0.5       0.5           -0.5          -0.5        0.5      -0.5   \n",
      "1         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "2         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "3         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "4         -0.5       0.5           -0.5          -0.5        0.5       0.5   \n",
      "...        ...       ...            ...           ...        ...       ...   \n",
      "238325    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238326    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238327    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238328    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "238329    -0.5      -0.5           -0.5          -0.5        0.5      -0.5   \n",
      "\n",
      "             age  Weight_kg       GCS        HR  ...       BUN  Creatinine  \\\n",
      "0      -0.973100  -0.318414 -0.687933 -0.569736  ... -0.545751   -0.214781   \n",
      "1      -0.973100  -0.169199 -0.687933 -0.491800  ... -1.189188   -0.679473   \n",
      "2      -0.973100  -0.169199 -0.640362 -0.567776  ... -1.189188   -0.679473   \n",
      "3      -0.973100  -0.169199 -0.402506 -0.749137  ... -1.189188   -0.679473   \n",
      "4      -0.973100  -0.169199  0.510859  0.630189  ... -1.189188   -0.679473   \n",
      "...          ...        ...       ...       ...  ...       ...         ...   \n",
      "238325 -2.467202  -0.361999  0.739200  0.191981  ... -1.909177   -0.494229   \n",
      "238326 -2.467202   0.228708  0.739200 -0.072708  ... -1.909177   -0.494229   \n",
      "238327 -2.467202  -0.403020  0.739200 -0.205053  ... -1.909177   -0.494229   \n",
      "238328 -2.467202  -0.403020  0.739200 -1.102056  ... -1.909177   -0.494229   \n",
      "238329 -2.467202  -0.403020  0.739200 -0.695219  ... -1.909177   -0.494229   \n",
      "\n",
      "            SGOT      SGPT  Total_bili       INR  input_total  input_4hourly  \\\n",
      "0       0.977035  1.112233    0.334065 -0.692745     0.410209       0.096264   \n",
      "1      -0.268657 -0.443987    1.387969 -0.214109     0.412665       0.240649   \n",
      "2      -0.268657 -0.443987    1.387969  0.200505     0.415103       0.240649   \n",
      "3      -0.268657 -0.443987    1.387969 -0.692745     0.417521       0.240649   \n",
      "4      -0.268657 -0.443987    1.387969 -0.692745     0.418963       0.096264   \n",
      "...          ...       ...         ...       ...          ...            ...   \n",
      "238325  0.901093  0.140315   -1.185843 -0.962914     0.071129      -1.521065   \n",
      "238326 -0.892408 -0.604775   -1.185843 -0.692745     0.071129      -1.521065   \n",
      "238327  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "238328  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "238329  0.143061 -0.192522   -0.180792  0.566220     0.071129      -1.521065   \n",
      "\n",
      "        output_total  output_4hourly  \n",
      "0           0.757770        0.733745  \n",
      "1           0.771262        0.731161  \n",
      "2           0.790024        0.847813  \n",
      "3           0.803524        0.767750  \n",
      "4           0.812873        0.670986  \n",
      "...              ...             ...  \n",
      "238325      0.530930        0.743862  \n",
      "238326      0.567943        0.789867  \n",
      "238327      0.567943       -1.852903  \n",
      "238328      0.587895        0.637420  \n",
      "238329      0.610678        0.701156  \n",
      "\n",
      "[238330 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# We want a Z-Score for every item. This a measure of variance to see how far a value is from the mean\n",
    "\n",
    "# Scipy provides a library to execute this kind of thing\n",
    "from scipy.stats import zscore\n",
    "# We need to normalize binaries to -0.5 and 0.5 for later use\n",
    "MIMIC_zscores = MIMIC_raw\n",
    "\n",
    "# No need for the zscore algorithm here, -0.5 and 0.5 suffice\n",
    "MIMIC_zscores[colbin] = MIMIC_zscores[colbin] - 0.5\n",
    "\n",
    "# Recall these columns are logarithmic, so they needed converted back for proper Z-Scoring (+ 0.1 to avoid log(0))\n",
    "MIMIC_zscores[collog] = np.log(MIMIC_zscores[collog] + 0.1).apply(zscore)\n",
    "\n",
    "# Normal column requires no modifications. Z-Scores are calculated as normal\n",
    "MIMIC_zscores[colnorm] = MIMIC_zscores[colnorm].apply(zscore)\n",
    "print(MIMIC_zscores)\n",
    "\n",
    "# We want Re Admission and fluid intake scaled Similarly to the other variables\n",
    "MIMIC_zscores['re_admission'] = np.log(MIMIC_zscores['re_admission'] + 0.6)\n",
    "# Apply a scalar to fluid intake\n",
    "MIMIC_zscores['input_total'] = 2 * MIMIC_zscores['input_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Percentage: 0.2036062060289801\n",
      "Training Percentage: 0.79639379397102\n"
     ]
    }
   ],
   "source": [
    "### The main loop to generate all possible models\n",
    "\n",
    "num_rows = id_count  # Total Number of Patients to divy data up\n",
    "testing_flag = 1     # The random number we use to identify a patient used for testing\n",
    "\n",
    "# TODO: Change this to 1 in MDP_COUNT\n",
    "#for model in range(1, 2): #mdp_count):\n",
    "train_ids = []       # A list containing all training ids from the icu_ids list\n",
    "test_ids =[]         # A list containing all testing ids from the icu_ids list\n",
    "\n",
    "# We want approximate 20% test, 80% train, so we random numbers 1-5\n",
    "# 1s Represent data points that will be used to test, 2-5 will be used to train\n",
    "group_ids = pd.DataFrame([int(np.floor(5 * np.random.random() + 1)) for i in range(1, id_count + 1)])\n",
    "icu_pair_set = pd.concat([pd.DataFrame(icu_ids), group_ids], axis=1, sort=False)\n",
    "icu_pair_set.columns = ['id', 'fil_val']\n",
    "train_ids =  icu_pair_set[icu_pair_set['fil_val'] != testing_flag]\n",
    "test_ids = icu_pair_set[icu_pair_set['fil_val'] == testing_flag]\n",
    "\n",
    "# We want to insure that the testing patients + training patients = total patients\n",
    "if (train_ids['id'].size + test_ids['id'].size) != id_count:\n",
    "    print(\"The testing and training set do not add up to the total set\")\n",
    "    exit()\n",
    "\n",
    "# Percentage for testing should be about 20%, Training about 80%\n",
    "print(\"Testing Percentage: \" + str((test_ids['id'].size / id_count)))\n",
    "print(\"Training Percentage: \" + str((train_ids['id'].size / id_count)))\n",
    "\n",
    "# After grabbing all the IDs, we want to flag all the rows that are train or test\n",
    "train_flag = patientdata['icustayid'].isin(train_ids['id'])\n",
    "test_flag = patientdata['icustayid'].isin(test_ids['id'])\n",
    "\n",
    "import pickle\n",
    "# Temporarily write train_flag for later use DELETE LATER\n",
    "with open('sample_train.txt', 'wb') as fp:\n",
    "    pickle.dump(train_flag, fp)\n",
    "\n",
    "#Validating that all data is being selected, and that the train and test sets are perfect opposites\n",
    "if patientdata['icustayid'].size != train_flag.size or not((train_flag.equals(~test_flag))):\n",
    "    print(\"Not all rows were grabbed properly, there is something wrong with the split\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the Z-Scores for the training set and the testing set\n",
    "train_zscores = MIMIC_zscores[train_flag]\n",
    "test_zscores = MIMIC_zscores[test_flag]\n",
    "\n",
    "# Validate all data is selected\n",
    "if(train_zscores.size + test_zscores.size != MIMIC_zscores.size):\n",
    "    print(\"The Z-Scores are all evenly distributed\")\n",
    "    exit()\n",
    "\n",
    "    \n",
    "# The blocs of relevance in order based on the train and test set\n",
    "# These will be used to build relevant data frames later down\n",
    "train_blocs = patientdata[train_flag]['bloc']\n",
    "test_blocs = patientdata[test_flag]['bloc']\n",
    "\n",
    "# Doing the same with the patient ids\n",
    "train_id_list = patientdata[train_flag]['icustayid']\n",
    "test_id_list = patientdata[test_flag]['icustayid']\n",
    "\n",
    "# Grabbing the boolean values for the patients who died within 90 days in the training set\n",
    "train_90d = patientdata[train_flag]['mortality_90d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Train Data used for the Sample: 0.2496938603158517\n"
     ]
    }
   ],
   "source": [
    "# Next, we want to sample the existing training set to only pick cluster_sample percent to use\n",
    "\n",
    "# We want to flag all the data points in the train_zscores set that will be used to create the MDP\n",
    "\n",
    "# Note: len(train_zscores.index) is the fastest way to get the number of rows in a dataframe in pandas\n",
    "\n",
    "# Additional Note: np.floor(np.random.random() + cluster_sample) is a computationally speedy way to get an approximate\n",
    "# percentage sample from a proportion value (cluster_sample). If cluster sample is 0.25, approximately 25% of the values\n",
    "# will be flagged as a 1, making it into the sample training set\n",
    "sample_train_flags = [bool(np.floor(np.random.random() + cluster_sample)) for i in range(len(train_zscores.index))]\n",
    "\n",
    "# It's good to know how much of the data was selected as sample\n",
    "print(\"Proportion of Train Data used for the Sample: \" + str(sample_train_flags.count(True)/len(sample_train_flags)))\n",
    "\n",
    "# The actual set to use\n",
    "sample_train_set = train_zscores[sample_train_flags]\n",
    "\n",
    "# Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "import pickle\n",
    "\n",
    "# Save the important data (clusters created as a result of the K-Means operations)\n",
    "# This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "# until the code is put into dev.\n",
    "with open('train_zscores.txt', 'wb') as fp:\n",
    "    pickle.dump(train_zscores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Not Run the following code block if temporarily troubleshooting\n",
    "# It takes a long time and there is a checkpoint system in place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[457 317 134 ... 427 427 349]\n",
      "[[ 4.09836066e-02  3.19672131e-01 -1.97557377e-01 ...  6.83318018e-01\n",
      "   9.82620620e-02 -1.33184787e-01]\n",
      " [ 1.51515152e-01 -3.78787879e-01 -4.93090909e-01 ...  3.89758578e-01\n",
      "   3.61326177e-01  5.01574931e-01]\n",
      " [-3.46944695e-17 -4.72222222e-01 -5.00000000e-01 ... -6.70717127e-01\n",
      "   1.85758170e-01 -1.85290276e+00]\n",
      " ...\n",
      " [-1.25000000e-01 -3.33333333e-01 -4.97916667e-01 ... -2.83927555e-01\n",
      "   6.14615492e-01  5.56076190e-01]\n",
      " [-2.63157895e-02 -4.47368421e-01 -5.00000000e-01 ... -1.52106461e+00\n",
      "  -1.83957232e+00 -1.40553964e+00]\n",
      " [-1.05263158e-01  3.15789474e-01  3.98526316e-01 ...  9.70609632e-01\n",
      "  -2.91374122e-01 -2.61021581e-01]]\n"
     ]
    }
   ],
   "source": [
    "# In order to prepare a proper set of states, we want to use k-means clustering to group various patients into \n",
    "# distinct states based on Z-Scores\n",
    "\n",
    "# K-Means or K-Means++ is a technique used to condense very diverse and sparse data into similar groups called 'clusters'\n",
    "# The K-means algorithm will create k clusters from N data points. In the case of this research,\n",
    "# the algorithm divides patients into groups that have similar data (age, blood pressure, etc..) and creates a faux 'point'\n",
    "# at the center of that particular clustering of data\n",
    "\n",
    "\n",
    "# Skikit offers a solution to perform K-Means++ clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# The KMeans takes three 'settings' arguments\n",
    "# 1. n_clusters: The number of clusters (later to be used as states), that we desire the algorithm to produce\n",
    "# this value has been preset to state_count which is 750\n",
    "# 2. max_iter: How many times each round of k-means clustering will make adjustments, set at 10,000 in my case\n",
    "# 3. n_init: The number of max_iter batches that will be conducted in a row. The best of these will be chosen\n",
    "# and saved in the variable clusters_models\n",
    "clusters_models = KMeans(n_clusters=state_count, max_iter=10000, n_init=clustering_iter).fit(sample_train_set)\n",
    "print(clusters_models.labels_)\n",
    "print(clusters_models.cluster_centers_)\n",
    "\n",
    "# Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "import pickle\n",
    "\n",
    "# Save the important data (clusters created as a result of the K-Means operations)\n",
    "# This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "# until the code is put into dev.\n",
    "with open('cluster_labels.txt', 'wb') as fp:\n",
    "    pickle.dump(clusters_models.labels_, fp)\n",
    "with open('cluster_centers.txt', 'wb') as fp:\n",
    "    pickle.dump(clusters_models.cluster_centers_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.09836066e-02  3.19672131e-01 -1.97557377e-01 ...  6.83318018e-01\n",
      "   9.82620620e-02 -1.33184787e-01]\n",
      " [ 1.51515152e-01 -3.78787879e-01 -4.93090909e-01 ...  3.89758578e-01\n",
      "   3.61326177e-01  5.01574931e-01]\n",
      " [-3.46944695e-17 -4.72222222e-01 -5.00000000e-01 ... -6.70717127e-01\n",
      "   1.85758170e-01 -1.85290276e+00]\n",
      " ...\n",
      " [-1.25000000e-01 -3.33333333e-01 -4.97916667e-01 ... -2.83927555e-01\n",
      "   6.14615492e-01  5.56076190e-01]\n",
      " [-2.63157895e-02 -4.47368421e-01 -5.00000000e-01 ... -1.52106461e+00\n",
      "  -1.83957232e+00 -1.40553964e+00]\n",
      " [-1.05263158e-01  3.15789474e-01  3.98526316e-01 ...  9.70609632e-01\n",
      "  -2.91374122e-01 -2.61021581e-01]] \n",
      " Dimensions:  750  x  50 \n",
      "         gender  mechvent  max_dose_vaso  re_admission  qSOFAFlag  SOFAFlag  \\\n",
      "23         0.5      -0.5           -0.5     -2.302585       -0.5      -0.5   \n",
      "24         0.5      -0.5           -0.5     -2.302585       -0.5      -0.5   \n",
      "25         0.5      -0.5           -0.5     -2.302585       -0.5      -0.5   \n",
      "26         0.5      -0.5           -0.5     -2.302585       -0.5      -0.5   \n",
      "27        -0.5       0.5           -0.5     -2.302585       -0.5      -0.5   \n",
      "...        ...       ...            ...           ...        ...       ...   \n",
      "238325    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238326    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238327    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238328    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "238329    -0.5      -0.5           -0.5     -2.302585        0.5      -0.5   \n",
      "\n",
      "             age  Weight_kg       GCS        HR  ...       BUN  Creatinine  \\\n",
      "23     -1.890896  -1.297285  0.739200  0.557645  ... -1.338058   -0.679473   \n",
      "24     -1.890896  -1.297285  0.739200  0.941934  ... -1.338058   -0.679473   \n",
      "25     -1.890896  -1.297285  0.739200  0.253742  ... -1.338058   -0.679473   \n",
      "26     -1.890896  -1.297285  0.739200  0.477257  ... -1.338058   -0.679473   \n",
      "27      1.211500  -0.415327 -2.510272 -1.200790  ...  0.052579   -0.889485   \n",
      "...          ...        ...       ...       ...  ...       ...         ...   \n",
      "238325 -2.467202  -0.361999  0.739200  0.191981  ... -1.909177   -0.494229   \n",
      "238326 -2.467202   0.228708  0.739200 -0.072708  ... -1.909177   -0.494229   \n",
      "238327 -2.467202  -0.403020  0.739200 -0.205053  ... -1.909177   -0.494229   \n",
      "238328 -2.467202  -0.403020  0.739200 -1.102056  ... -1.909177   -0.494229   \n",
      "238329 -2.467202  -0.403020  0.739200 -0.695219  ... -1.909177   -0.494229   \n",
      "\n",
      "            SGOT      SGPT  Total_bili       INR  input_total  input_4hourly  \\\n",
      "23     -0.543661 -1.231725    0.155616  0.893363    -6.044334      -1.521065   \n",
      "24     -0.543661 -1.231725    0.155616  0.893363    -6.044334      -1.521065   \n",
      "25     -0.543661 -1.231725    0.155616  0.893363    -6.044334      -1.521065   \n",
      "26     -0.543661 -1.231725    0.155616  0.893363    -6.044334      -1.521065   \n",
      "27     -0.645645 -0.803489   -0.922661 -0.692745    -0.159650       1.163412   \n",
      "...          ...       ...         ...       ...          ...            ...   \n",
      "238325  0.901093  0.140315   -1.185843 -0.962914     0.142258      -1.521065   \n",
      "238326 -0.892408 -0.604775   -1.185843 -0.692745     0.142258      -1.521065   \n",
      "238327  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "238328  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "238329  0.143061 -0.192522   -0.180792  0.566220     0.142258      -1.521065   \n",
      "\n",
      "        output_total  output_4hourly  \n",
      "23         -2.312066       -1.852903  \n",
      "24         -2.312066       -1.852903  \n",
      "25          0.009992        0.767750  \n",
      "26          0.172128        0.717895  \n",
      "27         -0.161361        0.349468  \n",
      "...              ...             ...  \n",
      "238325      0.530930        0.743862  \n",
      "238326      0.567943        0.789867  \n",
      "238327      0.567943       -1.852903  \n",
      "238328      0.587895        0.637420  \n",
      "238329      0.610678        0.701156  \n",
      "\n",
      "[189456 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "import pickle\n",
    "\n",
    "# Read these values back in from being saved to file\n",
    "cluster_values = []\n",
    "cluster_labels = [] \n",
    "train_zscores = []\n",
    "\n",
    "with open ('cluster_centers.txt', 'rb') as fp:\n",
    "    cluster_values = pickle.load(fp)\n",
    "with open ('cluster_labels.txt', 'rb') as fp:\n",
    "    cluster_labels = pickle.load(fp)\n",
    "with open ('train_zscores.txt', 'rb') as fp:\n",
    "    train_zscores = pickle.load(fp)\n",
    "    \n",
    "print(cluster_values, \"\\n\", \"Dimensions: \", len(cluster_values),\" x \", len(cluster_values[0]), \"\\n\", train_zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189456\n"
     ]
    }
   ],
   "source": [
    "# We now want to use the clusters to determine their nearest real data point neighbors\n",
    "# As a visual of this. Suppose we have 4 flags of different colors scattered over a park. The K-Means++ algorithm\n",
    "# is what planted the flags in the middle of groups of people that are similar. The KNN Search (K nearest neighbor search)\n",
    "# can be used in MatLab as a simple point finder instead of as a more complicated Supervised Learning algorithm. In Python \n",
    "# we can make use of the Vector Quanization (vq) package to assign each point to a centroid\n",
    "from scipy.cluster.vq import vq\n",
    "closest_clusters = vq(train_zscores, cluster_values)\n",
    "\n",
    "# Check to make sure each cluster has a value\n",
    "print(len(closest_clusters[0]))\n",
    "\n",
    "# As an aside, closest_clusters[1] contains the distance between each point's values (in this case 50 of them)\n",
    "# and their closest cluster's values.\n",
    "# Ex: If a point is [1, 1, 1] and it's closest cluster is the point [3, 3, 3]  closest_clusters[1] would contain the vector\n",
    "# [abs(3 - 1), abs(3 - 1), abs(3 - 1)] or [2, 2, 2]\n",
    "\n",
    "# Validate that all the points are in the range 0-749 (since there are only 750 clusters as specified previously)\n",
    "for i in closest_clusters[0]:\n",
    "    if(i > 749 or i < 0):\n",
    "        print(\"The clusters you are searching for are not configured properly and are out of bounds\")\n",
    "        print(\"Did you modify the cluster_count variable without changing this error configuration?\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Lowest IV Fluid Rank:  1.5\n",
      "New Highest IV Fluid Rank:  173142.0\n",
      "Old Lowest IV Fluid Rank:  0.0\n",
      "New Highest IV Fluid Rank:  1.0\n",
      "0         2\n",
      "1         3\n",
      "2         3\n",
      "3         3\n",
      "4         2\n",
      "         ..\n",
      "238325    1\n",
      "238326    1\n",
      "238327    1\n",
      "238328    1\n",
      "238329    1\n",
      "Length: 238330, dtype: int64\n",
      "0         1.0\n",
      "1         2.0\n",
      "2         2.0\n",
      "3         2.0\n",
      "4         1.0\n",
      "         ... \n",
      "238319    3.0\n",
      "238320    4.0\n",
      "238321    4.0\n",
      "238322    1.0\n",
      "238324    2.0\n",
      "Name: input_4hourly, Length: 173142, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "#  We want to begin constructing the set of possible actions between states\n",
    "###\n",
    "\n",
    "# The number of possible actions is represented as an action_count by action_count matrix\n",
    "# This is assuming that any action in the list can lead to any other action \n",
    "number_actions = action_count * action_count\n",
    "\n",
    "#  This may prove to be not as useful since this is diagnosis based: extracting information on\n",
    "#  Fluid input and max dose of vasopressors\n",
    "iv_fluid = patientdata['input_4hourly']\n",
    "\n",
    "#  Avoid any fluid that is 0 (That was not administered)\n",
    "iv_fluid = iv_fluid[iv_fluid > 0]\n",
    "# Determine minimum and maxium to scale data appropriately\n",
    "print(\"Old Lowest IV Fluid Rank: \", min(iv_fluid.rank()))\n",
    "print(\"New Highest IV Fluid Rank: \", max(iv_fluid.rank()))\n",
    "# Now we want to rank these actions in order of their value (lowest to highest)\n",
    "# We normalize our range from (1.5, 173142.0) to (0, 1)\n",
    "\n",
    "# Moving the minimum to zero\n",
    "iv_fluid_ranks = (iv_fluid.rank() - min(iv_fluid.rank()))\n",
    "# Shifting the max to approximately 1\n",
    "iv_fluid_ranks = iv_fluid_ranks / max(iv_fluid_ranks)\n",
    "\n",
    "# Validate that the range is indeed 0 to 1\n",
    "print(\"Old Lowest IV Fluid Rank: \", min(iv_fluid_ranks))\n",
    "print(\"New Highest IV Fluid Rank: \", max(iv_fluid_ranks))\n",
    "\n",
    "if round(max(iv_fluid_ranks), 3) != 1 or round(min(iv_fluid_ranks), 3) != 0:\n",
    "    print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "    print(\"Current max: \", round(max(iv_fluid_ranks), 3))\n",
    "    print(\"Curret min: \", round(min(iv_fluid_ranks), 3))\n",
    "    exit()\n",
    "\n",
    "# This is a mathematics trick to seperate all the values into three distinct groups based on their rank.\n",
    "# Since ranks are determined based on Vasopressor quantity, the four groups represent the amount of iv fluid\n",
    "# Administered to a patient (Group 1 - Low, Group 2 - Mid-Low, Group 3 - Mid-High, Group 4 - High)\n",
    "iv_fluid_groups = np.floor((iv_fluid_ranks + 0.2499999999) * 4)\n",
    "\n",
    "# Validate that groups are all associated with the numbers 1-4\n",
    "if not(iv_fluid_groups.isin([1,2,3,4]).any()):\n",
    "    print(\"Groups chosen fall outside the desired 1-4 window\")\n",
    "    \n",
    "# If an IV fluid amount is 0, we denote it to be action 1. \n",
    "# If an IV fluid falls into non-zero amounts, we use ranks built above (1 - 4) plus one. Making \n",
    "# the subset of these actions to be action 2 thru 5.\n",
    "# In short, the model can choose to give a 'patient' 5 different IV amounts \n",
    "num_of_rows = patientdata['input_4hourly'].size\n",
    "iv_fluid_actions = pd.Series([1 for i in range(0, num_of_rows)])\n",
    "\n",
    "# If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "for index in iv_fluid_groups.index:\n",
    "    iv_fluid_actions[index] = iv_fluid_groups[index] + 1\n",
    "\n",
    "print(iv_fluid_actions)\n",
    "print(iv_fluid_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# The generate_action_column function takes 4 arguments: \n",
    "#\n",
    "# column_values: A series of column values from a dataframe that we want to turn into action states\n",
    "# num_groups: How many groups or distinct actions we want to split the data into\n",
    "# column_name: The name of the column used for print debug statements\n",
    "# num_rows: The total number of rows in the full column before modifications (This is normally patientdata[column_name].size)\n",
    "# \n",
    "# This function returns column_actions, a series that represents the 'action', or group that each row of data falls under.\n",
    "#\n",
    "# An example is found down below, but in words, this function takes a full column of data, groups \n",
    "# the values for that data into num_groups distinct actions, and returns a series representing actions based on row\n",
    "# \n",
    "# Ex: Patients' blood pressure might be grouped into 5 categories (Action 1: < 20 mmHg, Action 2: > 20 mmHg && < 60 mmHg... etc)\n",
    "###\n",
    "\n",
    "def generate_action_column(column_values, num_groups, column_name, num_rows):\n",
    "    # Determine minimum and maxium to scale data appropriately\n",
    "    print(\"Old Lowest \", column_name, \" Rank: \", min(column_values.rank()))\n",
    "    print(\"Old Highest \" , column_name,  \" Rank: \", max(column_values.rank()))\n",
    "    # Now we want to rank these actions in order of their value (lowest to highest)\n",
    "    # Normalizing according to lowest and highest rank\n",
    "    \n",
    "    # Moving the minimum to zero\n",
    "    column_ranks = (column_values.rank() - min(column_values.rank()))\n",
    "    # Shifting the max to approximately 1\n",
    "    column_ranks = column_ranks / max(column_ranks)\n",
    "\n",
    "    # Validate that the range is indeed 0 to 1\n",
    "    print(\"New Lowest \", column_name, \" Rank: \", min(column_ranks))\n",
    "    print(\"New Highest \", column_name, \" Rank: \", max(column_ranks))\n",
    "\n",
    "    if round(max(column_ranks), 3) != 1 or round(min(column_ranks), 3) != 0:\n",
    "        print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "        print(\"Current max: \", round(max(column_ranks), 3))\n",
    "        print(\"Curret min: \", round(min(column_ranks), 3))\n",
    "        exit()\n",
    "    # This is a mathematics trick to seperate all the values into {num_groups} distinct groups based on their rank.\n",
    "    # Given different columns of interest this can take different forms. For IV fluids, this number is 4.\n",
    "    column_groups = np.floor(((column_ranks + 1.0/float(num_groups) - 0.000000001) * num_groups))\n",
    "\n",
    "    # Validate that groups are all associated with desired group split\n",
    "    if not(iv_fluid_groups.isin([i for i in range(1, num_groups + 1)]).any()):\n",
    "        print(\"Groups chosen fall outside the desired 1-4 window\")\n",
    "        exit()\n",
    "    \n",
    "    column_actions = pd.Series([1 for i in range(0, num_rows)])\n",
    "\n",
    "    # If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "    for index in column_groups.index:\n",
    "        column_actions[index] = column_groups[index] + 1\n",
    "\n",
    "    #print(column_actions)\n",
    "    #print(column_groups)\n",
    "    return column_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Lowest  IV Fluid  Rank:  1.5\n",
      "Old Highest  IV Fluid  Rank:  173142.0\n",
      "New Lowest  IV Fluid  Rank:  0.0\n",
      "New Highest  IV Fluid  Rank:  1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# This small sample insures the function performs the same as the test conducted above for IV Fluid\n",
    "iv_fluid = patientdata['input_4hourly']\n",
    "iv_fluid = iv_fluid[iv_fluid > 0]\n",
    "\n",
    "test_column = generate_action_column(iv_fluid, 4, \"IV Fluid\", patientdata['input_4hourly'].size)\n",
    "\n",
    "print(test_column.equals(iv_fluid_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Lowest  Max Dose Vasopressor  Rank:  1.0\n",
      "Old Highest  Max Dose Vasopressor  Rank:  35503.0\n",
      "New Lowest  Max Dose Vasopressor  Rank:  0.0\n",
      "New Highest  Max Dose Vasopressor  Rank:  1.0\n",
      "[1 5 3 4 2]\n"
     ]
    }
   ],
   "source": [
    "# Now we want the exact same thing but done with given Vasopressor amounts\n",
    "vasopressor_administered = patientdata['max_dose_vaso']\n",
    "vasopressor_administered = vasopressor_administered[vasopressor_administered > 0]\n",
    "\n",
    "vasopressor_actions = generate_action_column(vasopressor_administered, 4, \"Max Dose Vasopressor\", patientdata['max_dose_vaso'].size)\n",
    "print(vasopressor_actions.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# This function takes two arguments:\n",
    "# actions_column: A column of action groups generated by the above function (generate_action_column())\n",
    "# real_values: The actual values from the dataset corresponding to the same column as actions_column\n",
    "# and returns a list that contains the real median values for each 'group' actions.\n",
    "#\n",
    "# Ex: We apply the function to the action_column \"IV_Fluid\", which has split the data into 4 different groups of \n",
    "# IV_Fluid actions. This function will produce a list containing the median amount of IV_Fluid administered for each of those\n",
    "# groups (Group 1 -> Adminster 20 mL, Group 2 -> Administer 40 mL, Group 3 -> Administer 60 mL, Group 4 -> Administer 80 mL\n",
    "###\n",
    "\n",
    "def median_action_values(actions_column, real_values):\n",
    "    # Grab all the unique actions for a column and sort them\n",
    "    all_groups = np.sort(actions_column.unique())\n",
    "    # Concatanate the group number and real value for each row\n",
    "    action_set = pd.concat([actions_column, real_values], axis=1, sort=False)\n",
    "    # Name the columns for accurate querying\n",
    "    action_set.columns = ['group_id', 'data_val']\n",
    "    # Grab the median value for each group based on group number using python list comprehension\n",
    "    median_values = [np.median(action_set[action_set['group_id'] == i]['data_val']) for i in all_groups]\n",
    "    return median_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV Action Median Values: [0.0, 30.0, 80.66666667, 308.0, 955.5037749999999] \n",
      "Vasopressor Action Median Values:  [0.0, 0.04, 0.135, 0.27, 0.7625] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iv_median_actions = median_action_values(iv_fluid_actions, patientdata['input_4hourly'])\n",
    "vasopressor_median_actions = median_action_values(vasopressor_actions, patientdata['max_dose_vaso'])\n",
    "print(\"IV Action Median Values:\", str(iv_median_actions), \"\\nVasopressor Action Median Values: \", vasopressor_median_actions, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# This function takes one argument\n",
    "# list_action_columns: This is a Pandas dataframe that contains all the action_columns we desir to be grouped by index\n",
    "# This can be retrieving using the previously defined 'median action' function \n",
    "# \n",
    "# and returns two items:\n",
    "# list_action columns: The 'keys' or integers that represent every permutation of actions\n",
    "# chosen_action: The key that was chosen based on the action values in each column\n",
    "### \n",
    "def generate_action_matrix(list_action_columns):\n",
    "    # Grabs the list of columns the user has provided for use\n",
    "    desired_columns = [column for column in list_action_columns]\n",
    "    # Drops all group combinations that are duplicates\n",
    "    list_action_columns_indexes = list_action_columns.drop_duplicates(desired_columns)\n",
    "    # Sorts all combinations in order\n",
    "    list_action_columns_indexes = list_action_columns_indexes.sort_values(desired_columns)\n",
    "    # Create a dictionary based on the values from the dataframe \n",
    "    list_action_columns_indexes = list_action_columns_indexes.values.tolist() \n",
    "    # Determine which index in the list each row corresponds to \n",
    "    # Ex: For an 2-D action permutation list of [1,1] thru [5,5], there are 5 x 5 possibilities\n",
    "    # {1..5}, {1..5}, so there are 25 possible permutations, the indexes will run 1 - 25\n",
    "    chosen_action = [list_action_columns_indexes.index(val_pair) for val_pair in list_action_columns.values.tolist()]\n",
    "    # Return the keys first, and then the true values for the dataset\n",
    "    return list_action_columns_indexes, chosen_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Index of Action Chosen: 1 through 24')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "# FINISH CONSTRUCTION OF ALL ACTIONS AND THEIR VALUES\n",
    "###\n",
    "\n",
    "# Combine the columns that we desire to observe (iv_fluid_actions, vasopressor_actions)\n",
    "iv_vaso_groups = pd.concat([iv_fluid_actions, vasopressor_actions], axis=1, sort=False)\n",
    "# Name the columns for proper usage in the function\n",
    "iv_vaso_groups.columns = ['iv_group', 'vasopressor_group']\n",
    "\n",
    "action_keys, action_list = generate_action_matrix(iv_vaso_groups)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(action_list, density=False, bins=24)  # `density=False` would make counts\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Index of Action Chosen: 1 through 24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 30.0, 80.66666667, 308.0, 955.5037749999999] [0.0, 0.04, 0.135, 0.27, 0.7625]\n"
     ]
    }
   ],
   "source": [
    "# Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "import pickle\n",
    "train_chosen_actions = []\n",
    "with open ('sample_train.txt', 'rb') as fp:\n",
    "    train_chosen_actions = pickle.load(fp)\n",
    "\n",
    "# Grab a Series representing the action taken by the train data only\n",
    "train_chosen_actions = pd.Series(action_list)[train_chosen_actions]\n",
    "\n",
    "# Assign all action choices to their corresponding median values as shown previously\n",
    "print(iv_median_actions, vasopressor_median_actions)\n",
    "\n",
    "# Itertools provides an easy way to perform Cartesian product on multiple sets\n",
    "from itertools import product as cartesian_prod\n",
    "\n",
    "# This gives us the representative median values for a patient's vitals present in various action groups\n",
    "# action_keys[i] corresponds to train_action_values[i]\n",
    "# So, if the patient falls into group [1, 1] or no iv fluid given, no vasopressor administered,\n",
    "# The corresponding median values for this group will be represented by train_action_values (0.0, 0.0).\n",
    "# A patient in group [1, 2] (no iv fluid, a little vasopressor) will have a median real value of (0.0, 0.04)\n",
    "train_action_values = list(cartesian_prod(iv_median_actions, vasopressor_median_actions))\n",
    "\n",
    "if len(train_action_values) != len(iv_median_actions) * len(vasopressor_median_actions):\n",
    "    print(\"Something went wrong in determining the Cartesian product\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Blocs Length:  189456 Closest Clusters Length:  189456 Action List Length:  189456 Train 90d Length 189456 Range Matrix Length:  189456\n",
      "        training_bloc  closest_cluster_index  chosen_action_index  \\\n",
      "0                   1                    133                    0   \n",
      "1                   2                    133                    0   \n",
      "2                   3                    386                    0   \n",
      "3                   4                    386                    0   \n",
      "4                   1                    563                   20   \n",
      "...               ...                    ...                  ...   \n",
      "189451              9                    452                    0   \n",
      "189452             10                    176                    0   \n",
      "189453             11                    219                    0   \n",
      "189454             12                    547                    0   \n",
      "189455             13                    547                    0   \n",
      "\n",
      "        90d_morality_status  lower_range  upper_range  \n",
      "0                         0          100         -100  \n",
      "1                         0          100         -100  \n",
      "2                         0          100         -100  \n",
      "3                         0          100         -100  \n",
      "4                         1         -100          100  \n",
      "...                     ...          ...          ...  \n",
      "189451                    0          100         -100  \n",
      "189452                    0          100         -100  \n",
      "189453                    0          100         -100  \n",
      "189454                    0          100         -100  \n",
      "189455                    0          100         -100  \n",
      "\n",
      "[189456 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# BEGIN CONSTRUCTION OF PRE-STATE MATRIX\n",
    "# This will be used to build the full state/action matrix\n",
    "### \n",
    "\n",
    "# Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "# If they have died, the range is [-100, 100]\n",
    "# If they are alive, the range is [100, -100]\n",
    "range_vals = [100, -100]\n",
    "# Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "# This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "train_90d_polarity = (2 * (1 - train_90d) - 1)\n",
    "range_matrix = [np.multiply(polarity, range_vals) for polarity in train_90d_polarity]\n",
    "# Grab the lower range limit and upper range limit seperately in order to build the \n",
    "all_lower_ranges = [i[0] for i in range_matrix]\n",
    "all_upper_ranges = [i[1] for i in range_matrix]\n",
    "# The qlearning_dataset prior to modification contains 6 columns and ~190885 rows (around 75% of the data)\n",
    "# The columns are as follows:\n",
    "#\n",
    "# training_bloc: time_series stamps for a patient's state over time, very in range from {1..?}\n",
    "#\n",
    "# closest_cluster_index: The index of the nearest cluster to the z-scores of the patient's data, \n",
    "# corresponding actual data for each cluster's index (i) can be found in cluster_values[i]\n",
    "#\n",
    "# chosen_action_index: The chosen action or representation of a patient's IV_Fluid and Vasopressor status [0 - 24]\n",
    "# \n",
    "# 90d_morality_status: 0 means the patient is alive 90 days after discharge from ICU\n",
    "#                      1 means the patient is dead  90 days after discharge from ICU\n",
    "#\n",
    "# lower_range + upper_range: An index to be used later on, gathered from the range index\n",
    "print(\"Training Blocs Length: \", len(train_blocs), \"Closest Clusters Length: \", len(closest_clusters[0]), \"Action List Length: \", len(train_chosen_actions), \"Train 90d Length\", len(train_90d), \"Range Matrix Length: \", len(range_matrix))\n",
    "qlearning_dataset = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                               pd.Series(closest_clusters[0]), \n",
    "                               pd.Series(train_chosen_actions.tolist()), \n",
    "                               pd.Series(train_90d.tolist()), \n",
    "                               pd.Series(all_lower_ranges), \n",
    "                               pd.Series(all_upper_ranges)], \n",
    "                              axis=1, sort=False)\n",
    "qlearning_dataset.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', '90d_morality_status', 'lower_range', 'upper_range']\n",
    "print(qlearning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base qlearning_dataset does not account for endpoints in either life or death\n",
    "# These states have not been established yet, which is what this step corrects\n",
    "qlearning_dataset_len = len(qlearning_dataset.index)\n",
    "# We need space to add a death/life state for every patient, about a 20% increase in size from the original MDP\n",
    "# We will cut the excess off by the end of the loop\n",
    "qlearning_dataset_len_mod = int(np.floor(qlearning_dataset_len * 1.2))\n",
    "qlearning_dataset_mod = np.array([[0 for i in range(0, 4)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "\n",
    "# Start construction of modified data\n",
    "row = 0\n",
    "# In Markov theory, an absorbing state is one which can be entered, but cannot be left. (Similar to the Hotel California)\n",
    "# In the case of this experiment, those states are either life (state_count) or death (state_count + 1) per patient as\n",
    "# defined by me\n",
    "absorbing_states = [state_count, state_count + 1]\n",
    "\n",
    "# Start the loop to begin capping the markov chain off at life and death states\n",
    "for i in range(0, qlearning_dataset_len - 1):\n",
    "    # Use the already gathered data for each row\n",
    "    qlearning_dataset_mod[row, :] = qlearning_dataset.iloc[i][0:4]\n",
    "    # If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\n",
    "    if qlearning_dataset.iloc[i + 1]['training_bloc'] <= qlearning_dataset.iloc[i]['training_bloc']:\n",
    "        # Grab the row\n",
    "        whole_row = qlearning_dataset.iloc[i]\n",
    "        # Set most of the row to the original data's values, except set the action to be either state 750 or 751\n",
    "        # Life or death respectively\n",
    "        row = row + 1\n",
    "        # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "        qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_morality_status']], 25,  whole_row['lower_range']]\n",
    "    row = row + 1\n",
    "# Add in the last row\n",
    "whole_row = qlearning_dataset.iloc[len(qlearning_dataset.index) - 1]\n",
    "qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_morality_status']], 25,  whole_row['lower_range']]\n",
    "row = row + 1\n",
    "# Get rid of the unneeded rows\n",
    "qlearning_dataset_mod = pd.DataFrame(qlearning_dataset_mod[0:row, :])\n",
    "qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17093\n",
      "0\n",
      "17093\n",
      "13080\n",
      "4013\n"
     ]
    }
   ],
   "source": [
    "# Total patients being observed in the test\n",
    "print(len(qlearning_dataset_mod[qlearning_dataset_mod['training_bloc'] == 1]['training_bloc']))\n",
    "# Show that we now have end states established \n",
    "print(len(qlearning_dataset[qlearning_dataset['chosen_action_index'] == 25]['chosen_action_index']))\n",
    "print(len(qlearning_dataset_mod[qlearning_dataset_mod['chosen_action_index'] == 25]['chosen_action_index']))\n",
    "print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == 750]['chosen_action_index']))\n",
    "print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == 751]['chosen_action_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Now that we officially have some a valid bloc for actions, and a valid set of states, it's time \n",
    "#  to begin building the transitions matrix.\n",
    "### \n",
    "\n",
    "### If the matrix is bidirectional (S1 -> S2, S2 -> S1 are both valid, we can build two matrices)\n",
    "\n",
    "### \n",
    "# The MDP Toolbox we are going to be using requires Transition and Reward Matrices to be in the form\n",
    "# M(action, state1, state2)\n",
    "###\n",
    "\n",
    "###\n",
    "# The create_transition_matrix method takes 4 arguments:\n",
    "# num_actions: The total number of possible actions (calculated by action_count ^ 2 or in py, action_count ** 2)\n",
    "# num_states:  Number of states the model uses\n",
    "# qlearning_dataset: The dataset that will be used for the qlearning process\n",
    "# transition_threshold: How many actions do we want to deem as scarce and not worth keeping (default = 5)\n",
    "# reverse: If false, the matrix that is created is represented as transition[A][S1][S2], if true: transition[A][S2][S1]\n",
    "###\n",
    "def create_transition_matrix(num_actions, num_states, ql_data_input, transition_threshold = 5, reverse = False):\n",
    "    # The transition matrix is a 3D construct, involving a transition between two states\n",
    "    # and an action. The dimensions for the matrix are (state_count * 2) * (state_count + 2) * action_count\n",
    "    transition_matrix = [[[0 for i in range(0, num_states + 2)] for i in range(0, num_states + 2)] for i in range(0, num_actions)]\n",
    "    # NP Arrays allow for more compact and efficient slicing\n",
    "    transition_matrix = np.array(transition_matrix).astype(float)\n",
    "    # We also need a matrix to denote the policy that corresponds with taken a particular action from a state\n",
    "    transition_policy_count = [[0 for i in range(0, num_states + 2 )] for i in range(0, num_actions)]\n",
    "    transition_policy_count = np.array(transition_policy_count).astype(float)\n",
    "    # Iterate over the actual data in order to form the actual states and their corresponding actions\n",
    "    # As soon as we hit the next patient (the next row has a training bloc value of 1), we stop processing actions for that patient\n",
    "    for i in range(0, len(qlearning_dataset_mod) - 1):\n",
    "        # Since 1 is our 'endpoint' for each patient, there are no actions we can take from this point on\n",
    "        if ql_data_input.iloc[i + 1]['training_bloc'] > ql_data_input.iloc[i]['training_bloc']:\n",
    "            S1 = ql_data_input.iloc[i]['closest_cluster_index']\n",
    "            S2 = ql_data_input.iloc[i + 1]['closest_cluster_index'] \n",
    "            action_id = ql_data_input.iloc[i]['chosen_action_index']\n",
    "            if not(reverse):\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S1][S2] = transition_matrix[action_id][S1][S2] + 1\n",
    "            else:\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S2][S1] = transition_matrix[action_id][S2][S1] + 1\n",
    "                \n",
    "            # Count the number of times action A is used to transition from S1\n",
    "            transition_policy_count[action_id][S1] = transition_policy_count[action_id][S1] + 1        \n",
    "\n",
    "    # In order to avoid drastically altering our model, we fix a constant\n",
    "    # value (set by default to 5), in order to declare sparse actions \n",
    "    # as essentially not happening (make their count 0)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if transition_policy_count[i][j] <= transition_threshold:\n",
    "                transition_policy_count[i][j] = 0 \n",
    "    # Now, we want to prevent transitions from state -> state using\n",
    "    # a certain action if that action is sparse or nonexistant\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if not(reverse):\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,j,:] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,j,:] = transition_matrix[i,j,:]/np.float64(transition_policy_count[i][j])\n",
    "            else:\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,:,j] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,:,j] = transition_matrix[i,:,j]/np.float64(transition_policy_count[i][j])\n",
    "    \n",
    "    # Ensure no divisions create NaNs or infinities\n",
    "    transition_matrix = np.nan_to_num(transition_matrix)\n",
    "    \n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 752 752\n"
     ]
    }
   ],
   "source": [
    "# Constructing Transition Matrix(A, State1, State2)\n",
    "total_actions = action_count ** 2        \n",
    "# Execute the function call\n",
    "transition_mat = create_transition_matrix(total_actions, state_count, qlearning_dataset_mod, transition_threshold, False)\n",
    "# Should be 25, 752, 752\n",
    "print(len(transition_mat), len(transition_mat[0]), len(transition_mat[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate whether or not the Sum of all actions from a given state is either 0.0 or 1.0\n",
    "# Where 1 indicates there are further actions to take, and 0 means we have hit the end of potential actions to take\n",
    "for i in range(0, 25):\n",
    "    for j in range(0, 752):\n",
    "        total_prob = sum(transition_mat[i][j])\n",
    "        if (abs(total_prob - 1.0) > 0.001):\n",
    "            print(\"All probabilities should be either 0.0 or almost 1.0 (0.999....), check your arguments to transition_mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life Endpoints: 2779 \n",
      "Death Endpoints 1885\n"
     ]
    }
   ],
   "source": [
    "# State 750 is life, State 751 is death\n",
    "# This provides a count of how many states in the finalized MDP end in life and death\n",
    "life_count = 0\n",
    "death_count = 0\n",
    "\n",
    "for i in range(0, total_actions):\n",
    "    for j in range(0, state_count + 2):\n",
    "        if transition_mat[i, j, 750] != 0:\n",
    "            life_count = life_count + 1\n",
    "        if transition_mat[i, j, 751] != 0:\n",
    "            death_count = death_count + 1\n",
    "\n",
    "print(\"Life Endpoints:\", life_count, \"\\nDeath Endpoints\", death_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the MDP built and the probabilities evaluated, it is time to construct the reward matrix \n",
    "reward_mat = np.array([[[0 for i in range(0, state_count + 2)] for i in range(0, state_count + 2)] for i in range(0, total_actions)])\n",
    "# We want to fix initial penalities and rewards like before at -100 and +100\n",
    "# Note, we allot the 2 extra columns for the reward and penalty\n",
    "for i in range(0, total_actions):\n",
    "    for j in range(0, state_count + 2):\n",
    "        # If the patient hits the final state of death, we penalize the model\n",
    "        reward_mat[i][j][state_count] = -100\n",
    "        # If the patient hits the final state of life, we reward the model\n",
    "        reward_mat[i][j][state_count + 1] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4664\n"
     ]
    }
   ],
   "source": [
    "# Perform Matrix multiplication to multiply reward value by probability of that reward\n",
    "# This gives us the 'real reward' where sparse events are not weighted as heavily \n",
    "# as more frequent events.\n",
    "# I.E\n",
    "# An action that occurs 2% of the time resulting in death should suffer a much lower penalty than a death route that occurs 50% of the time\n",
    "# Likewise, an action occurs 80% of the time yielding life should be looked at more closely than a 2% chance\n",
    "# an action that incurse a 2%\n",
    "reward_mat_final = np.multiply(transition_mat, reward_mat)\n",
    "\n",
    "reward_count = 0\n",
    "for i in range(0, total_actions):\n",
    "    for j in range(0, state_count + 2):\n",
    "        for k in range(0, state_count + 2):\n",
    "            if reward_mat_final[i][j][k] != 0:\n",
    "                reward_count = reward_count + 1\n",
    "# Reward Count should be somewhere in the 2000 - 3500 count\n",
    "print(reward_count)\n",
    "            \n",
    "\n",
    "# Validate that all rewards are in the range -100 to 100 \n",
    "for i in range(0, total_actions):\n",
    "    for j in range(0, state_count + 2):\n",
    "        for k in range(0, state_count + 2):\n",
    "            if reward_mat_final[i][j][k] > 100 or reward_mat_final[i][j][k] < -100:\n",
    "                print(\"The entry at\", i + \",\" + j, \"has a reward value of \", reward_mat_other[i][j], \"which is not in the range (-100, 100)\")\n",
    "                exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  The Full MDP is now finished. The Transition Matrix exists, the reward matrix that corresponds to it also exists.\n",
    "#  Now is the time to actually perform Q-Learning\n",
    "###\n",
    "\n",
    "# The initial MDP matrix\n",
    "# We need the values of weights that determines how much the model\n",
    "# prefers transitioning from one state (medical conditional), to another\n",
    "# The Matrix must be in the form [[A][S1][S2]] Where S1 is initial state, S2 is the second state, and\n",
    "# A is the action taken to get from S1 to S2. \n",
    "transitions = transition_mat\n",
    "\n",
    "# We need to determine the reward value for predicting an outcome leading to survival (+)\n",
    "# and a penalty for an outcome that will yield death (-)\n",
    "# The Matrix must be in the form [[R][S1][S2]] Where S1 is initial state, S2 is the second state, and\n",
    "# R is the reward for taking the action from S1 to S2. \n",
    "reward = reward_mat_final\n",
    "\n",
    "# We need to determine the discount value to influence the model to continue changing\n",
    "# when outcomes are not desired, This value should be kept in the range 0 < discount < 1\n",
    "discount = 1\n",
    "\n",
    "# The Q-Learning algorithm will run a fixed number of times\n",
    "numOfIterations = 10000\n",
    "\n",
    "# We need to determine whether or not we want to validate that the transitions and rewards matrix\n",
    "# to make sure they are valid, this option will only be turned off for speed\n",
    "scheck = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP Toolbox (Built originally in MatLab) offers a nice way to perform Q-Learning on an Already Built MDP \n",
    "import mdptoolbox\n",
    "ql_runner = mdptoolbox.mdp.QLearning(transitions, reward, discount, n_iter= numOfIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
