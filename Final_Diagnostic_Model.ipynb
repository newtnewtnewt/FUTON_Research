{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  FUTON Model MDP + Q-Learning Creation Script\n",
    "#  A Research Project conducted by Noah Dunn \n",
    "###\n",
    "\n",
    "# Import the standard tools for working with Pandas dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import ctypes\n",
    "import csv\n",
    "# Pickle provides easy Object Serialization for quick read + writes of data\n",
    "import pickle\n",
    "# Vector Quantization for Determining Cluster Centers\n",
    "from scipy.cluster.vq import vq\n",
    "# Skikit offers a solution to perform K-Means++ clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# Scipy provides a library to execute Z-Score Normalization\n",
    "from scipy.stats import zscore\n",
    "# We want to do type hinting for API clarification\n",
    "from typing import *\n",
    "# Itertools provides an easy way to perform Cartesian product on multiple sets\n",
    "from itertools import product as cartesian_prod\n",
    "import multiprocessing\n",
    "# We need to perform 10 fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# For undersampling during the balancing phase\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# For using PCA (Principal Component Analysis) to check variance\n",
    "from sklearn.decomposition import PCA\n",
    "### Some repetitive type hinting\n",
    "int_matrix2D = np.array\n",
    "float_matrix2D = np.array\n",
    "int_matrix3D = np.array\n",
    "float_matrix3D = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Data File that will be used to conduct the experiments\n",
    "patientdata:pd.DataFrame = pd.read_csv(\"G:/MIMIC-ALL/MIMIC-PATIENTS/patient_data_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#  An MDP, or Markov Decision Process is used to model relationships between various states and actions.\n",
    "#  A state can be thought of in medical solution as a patient's diagnosis based on current vitals and state of being. \n",
    "#  An action can be thought of as a change in current diagnosis based on one of those vitals.\n",
    "#  The inspirations for the bulk of this code came from Komorowksi's AI Clinician which can be found \n",
    "#  here: https://github.com/matthieukomorowski/AI_Clinician/blob/master/AIClinician_core_160219.m\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The extract_init_column_data function takes 6 arguments: \n",
    "# \n",
    "# patient_data:   The full DataFrame of all the patient data, raw and unfiltered\n",
    "# id_column:      The name of the column in the dataframe containing the patient IDs\n",
    "# binary_columns: A list containing the names of all the columns that are binary data (only 0s and 1s)\n",
    "# normal_columns: A list containing the names of all the columns that are regular data (require no function transformations)\n",
    "# log_columns:    A list containing the names of all the columns that are logarithmic data (Log has already been applied)\n",
    "# debug_flag:     A boolean flag that indicates whether or not print statements will be executed\n",
    "# \n",
    "# The function has 6 return values:\n",
    "#\n",
    "# colbin:       The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:      The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:       The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw:    The DataFrame containing the data of all desired columns and their values\n",
    "# id_count:     The total number of IDs to be used  \n",
    "# icu_ids:      The ids of all patients to be used\n",
    "# patient_idxs: \n",
    "\"\"\"\n",
    "\n",
    "def extract_init_column_data(patient_data:pd.DataFrame, id_column:str='icustayid', binary_columns:List[str]=None, normal_columns:List[str]=None,\n",
    "                            log_columns:List[str]=None, debug_flag:bool=False) -> (List[str], List[str], List[str], \n",
    "                                                                                   pd.DataFrame, pd.DataFrame, int, List[List[int]]):\n",
    "\n",
    "    # Grab list of unique patient ICU stay IDs\n",
    "    icu_ids:int = patient_data[id_column].unique()\n",
    "    # Number of patients to be used for states\n",
    "    id_count:int = icu_ids.size\n",
    "    if debug_flag:\n",
    "        print(id_count)\n",
    "\n",
    "    # All our columns are broken up into 3 distinct categories:\n",
    "    # 1. Binary values (0 or 1)\n",
    "    # 2. Standard Ranges (Plain old Integers + Decimals)\n",
    "    # 3. Logarthmic Values (columnvalue = log(columnvalue))\n",
    "    colbin:List[str] = []\n",
    "    colnorm:List[str] = [] \n",
    "    collog:List[str] = []\n",
    "    \n",
    "    # Enables custom column selection\n",
    "    if binary_columns == None:\n",
    "        colbin = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag']\n",
    "    else:\n",
    "        colbin = binary_columns\n",
    "    \n",
    "    if normal_columns == None:\n",
    "        #colnorm = ['paO2', 'PaO2_FiO2', 'Platelets_count', 'GCS', 'MeanBP', 'SysBP', 'RR']\n",
    "        colnorm = ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', 'qSOFA'];\n",
    "    else:\n",
    "        colnorm = normal_columns\n",
    "    if log_columns == None:\n",
    "        # collog = ['Total_bili', 'Creatinine', 'output_total','output_4hourly']\n",
    "        collog = ['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly'];\n",
    "    else:\n",
    "        collog = log_columns\n",
    "    # Create seperate dataframes for each of the columns\n",
    "    colbin_df:pd.DataFrame = patient_data[colbin]\n",
    "    colnorm_df:pd.DataFrame = patient_data[colnorm]\n",
    "    collog_df:pd.DataFrame = patient_data[collog]\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Let's make sure we have what we need\n",
    "        print(colbin_df, \"\\n\", colnorm_df, \"\\n\", collog_df)\n",
    "    # Rearrange the dataframe in order of binary, normal, and log data from left to right\n",
    "    MIMIC_raw:pd.DataFrame = pd.concat([colbin_df, colnorm_df, collog_df], axis=1)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_raw) \n",
    "    return colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_zscores function takes 4 arguments: \n",
    "# \n",
    "# colbin:    The list of all columns representing binary values\n",
    "# colnorm:   The list of all columns representing normal values\n",
    "# collog:    The list of all columns representing log values\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "#\n",
    "# and returns \n",
    "# colbin:    The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:   The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:    The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "\"\"\"\n",
    "\n",
    "def construct_zscores(colbin:List[str], colnorm:List[str], collog:List[str], MIMIC_raw:pd.DataFrame, debug_flag:bool) -> pd.DataFrame:\n",
    "\n",
    "    # We want a Z-Score for every item. This a measure of variance to see how far a value is from the mean\n",
    "\n",
    "    # We need to normalize binaries to -0.5 and 0.5 for later use\n",
    "    MIMIC_zscores:pd.DataFrame = MIMIC_raw\n",
    "\n",
    "    # No need for the zscore algorithm here, -0.5 and 0.5 suffice\n",
    "    MIMIC_zscores[colbin] = MIMIC_zscores[colbin] - 0.5\n",
    "\n",
    "    # Recall these columns are logarithmic, so they needed converted back for proper Z-Scoring (+ 0.1 to avoid log(0))\n",
    "    # Note that log(0.1) is essentially 0, Mathematically proved\n",
    "    \n",
    "    # zscore is the function pulled from the stats library in the initial import calls\n",
    "    MIMIC_zscores[collog] = np.log(MIMIC_zscores[collog] + 0.1).apply(zscore)\n",
    "\n",
    "    # Normal column requires no modifications. Z-Scores are calculated as normal\n",
    "    MIMIC_zscores[colnorm] = MIMIC_zscores[colnorm].apply(zscore)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_zscores)\n",
    "    if 're_admission' in colbin:\n",
    "        # We want the Re_Admission and fluid intake scaled Similarly to the other variables\n",
    "        MIMIC_zscores['re_admission'] = np.log(MIMIC_zscores['re_admission'] + 0.6)\n",
    "    if 'input_total' in collog:\n",
    "        # Apply a scalar to fluid intake\n",
    "        MIMIC_zscores['input_total'] = 2 * MIMIC_zscores['input_total']\n",
    "    return MIMIC_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to have a model severely skewed to over-predict death/life, it is desireable\n",
    "to include an equal number of patients who have lived or died. \n",
    "Already with data that has been stratified based on averages, the balance gives the cleanest\n",
    "possible situation to achieve balanced output\n",
    "\n",
    "Input: \n",
    "train_set_predict: A DataFrame containing the IDs and summary data (mean, q1, q3, max, min)\n",
    "for a given patient selected for the training set\n",
    "train_set_response: A DataFrame containing information on whether or not a patient lived or died\n",
    "is_debug: A flag that enables print statements in the functions\n",
    "\n",
    "Output:\n",
    "list_ids: The full list of IDs to be used for the training set\n",
    "\n",
    "\"\"\"\n",
    "def balance(train_set_predict:List[int], train_set_response:List[int], is_debug:bool) -> List[int]:\n",
    "    # This will undersample the majority class using a specific seed to keep consistency across runs\n",
    "    # In this case, that value is 47\n",
    "    rus = RandomUnderSampler(random_state=47, sampling_strategy='majority')\n",
    "    # train_set_predict = train_set_predict.reset_index()\n",
    "    # Duplicate column names like are present here do not enable the sampling alg to work properly\n",
    "    # Save the initial column headers to change back after the fact\n",
    "    # Also, the reset_index shoves all the indexes to a 'level_0' column that will be used to identify them\n",
    "    train_set_predict = train_set_predict.reset_index()\n",
    "    start_column_headers = train_set_predict.columns\n",
    "    train_set_modified = train_set_predict[:]\n",
    "    # The fake column headers are just place holders so the randomized undersampling is allowed to take place\n",
    "    train_set_modified.columns = [\"fake\" + str(i) for i in range(0, len(train_set_modified.columns))]\n",
    "    # Perform the Undersampling of the majority class (Patients that Live)\n",
    "    train_set_predict_bal, train_set_response_bal = rus.fit_resample(train_set_modified, train_set_response)\n",
    "    # Reset column headers back to where they were \n",
    "    train_set_predict_bal.columns = start_column_headers\n",
    "    # Validate that the number of columns is correct\n",
    "    if is_debug:\n",
    "        print(train_set_response_bal['death_state'].value_counts())\n",
    "    list_ids = np.sort(train_set_predict_bal['level_0'].values.tolist())\n",
    "    # Return the IDs that we intend to use for the training\n",
    "    return list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The golden standard for obtained ideal results for any machine learning exercise in the modern day is the\n",
    "K-Fold Cross-Validation, specifically the stratified version of this technique. The K-fold divides the data into 10\n",
    "seperate chunks chosen based on the feature set, and the stratified portion indicates that this data is relatively \n",
    "balanced in terms of the included data per fold. This function also calls the balance function for the training\n",
    "output, in order to insure an equal number of patients who lived and patients who died are chosen\n",
    "\n",
    "Input:\n",
    "stratified_data: Time-Series DataFrame that has been summarized by patient in order to be used in the K-Fold\n",
    "predictor_columns:  The columns to be used for stratification acting as predictors (I.E. the Summary Statistics for all Features)\n",
    "response_column: The column that concerns performance, in this case the life/death status\n",
    "is_debug: Enables print statements in the function\n",
    "\n",
    "Output: \n",
    "all_train_sets: The List of Lists of all IDs to be used for the training data each run\n",
    "all_test_sets: The List of lists of all IDs to be used for the testing data each run\n",
    "\"\"\"\n",
    "\n",
    "def crossval_split(stratified_data:pd.DataFrame, predictor_columns:List[str], \n",
    "                   response_column:List[str], is_debug:bool, id_correction_dict:Dict[int, int],\n",
    "                   balance_flag:bool) -> (int_matrix2D, int_matrix2D):\n",
    "    # The cross fold has an inner and outer component for proper division\n",
    "    # This is provided by the sklearn library\n",
    "    inner_cv = StratifiedKFold(10)\n",
    "    outer_cv = StratifiedKFold(10)\n",
    "    # Divy up the data into the new dataframes based on the selected columns\n",
    "    predict_data = stratified_data[predictor_columns]\n",
    "    response_data = stratified_data[response_column]\n",
    "    # Save these to be appended to after all the cross folds are done\n",
    "    all_train_sets = []\n",
    "    all_test_sets = []\n",
    "    # Iterate through using the cross-folds to build the correct IDs\n",
    "    for training_samples, test_samples in outer_cv.split(predict_data, response_data):\n",
    "        for inner_train, inner_test in inner_cv.split(predict_data.iloc[training_samples], response_data.iloc[training_samples]):\n",
    "            # Grab the training data indexes and corresponding reponses\n",
    "            train_predict = predict_data.iloc[training_samples].iloc[inner_train]\n",
    "            train_response = response_data.iloc[training_samples].iloc[inner_train]\n",
    "            # Grab the testing data indexes and corresponding responses\n",
    "            test_predict = predict_data.iloc[training_samples].iloc[inner_test]\n",
    "            train_ids = []\n",
    "            if balance_flag:\n",
    "                # Balance the training dataset, grab the desired ids\n",
    "                train_ids = balance(train_predict, train_response, is_debug=False)\n",
    "            else:\n",
    "                train_ids = list(train_predict.index)\n",
    "            # Grab the ids to use for testing\n",
    "            test_ids = list(test_predict.index)\n",
    "            # Fix the IDs to the correct indexes\n",
    "            train_ids =list(map(id_correction_dict.get, train_ids))\n",
    "            test_ids = list(map(id_correction_dict.get, test_ids))\n",
    "            # Load them into a grand list\n",
    "            all_train_sets.append(train_ids)\n",
    "            all_test_sets.append(test_ids)\n",
    "    return all_train_sets, all_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normally in cross-fold validation, it is perfectly usable to insert a filtered dataset directly\n",
    "into the cross-validation split function. Due to the nature of this data being time-series, with a variety of recorded \n",
    "chart events for each patient, a trick to still benefit from the cross-validation step is to 'flatten' the data\n",
    "by providing summary statistics for each feature (mean, min, max, q1, q3), and form a stratification off that\n",
    "\n",
    "Input:\n",
    "patientdata: A DataFrame containing all the entire unflattened dataset\n",
    "icu_ids: A list of all the ids of the patients\n",
    "\n",
    "Output:\n",
    "all_patient_summaries_df: A DataFrame with flattened values for each of the patients\n",
    "\"\"\"\n",
    "def flatten_timeseries_data(patientdata:pd.DataFrame, icu_ids:List[int], all_factors:List[str]) -> pd.DataFrame:\n",
    "    # Iterate over all the factors for all the patients\n",
    "    all_patient_summaries:List[pd.DataFrame] = []\n",
    "    for id_val in icu_ids:\n",
    "        # Grab each patient's full time-series run through the data\n",
    "        single_patient_data = patientdata[patientdata['icustayid'] == id_val]\n",
    "        # We can stratified time series data by 'flattening' it, constructing some summary statistic for each column\n",
    "        desired_patient_columns = pd.DataFrame(single_patient_data, columns=all_factors)\n",
    "        # Turn into dataframe and make each column seperate (Not 50 rows, 1 column)\n",
    "        patient_mean_values = pd.DataFrame(desired_patient_columns.mean()).transpose()\n",
    "        patient_min_values = pd.DataFrame(desired_patient_columns.min()).transpose()\n",
    "        patient_max_values = pd.DataFrame(desired_patient_columns.max()).transpose()\n",
    "        # Need to reset index on these due to funky interaction with concat\n",
    "        patient_q1_values = pd.DataFrame(desired_patient_columns.quantile(0.25)).transpose().reset_index()\n",
    "        patient_q3_values = pd.DataFrame(desired_patient_columns.quantile(0.75)).transpose().reset_index()\n",
    "        # Build the dataframe from the pieces\n",
    "        full_summary = pd.concat([patient_mean_values, patient_min_values, \n",
    "                                  patient_max_values, patient_q1_values, \n",
    "                                  patient_q3_values], axis=1, sort=False)\n",
    "        # Add it to the master set\n",
    "        all_patient_summaries.append(full_summary)\n",
    "    # Fix the List into a dataframe\n",
    "    # Also, set the indexes back to start at 1\n",
    "    all_patient_summaries_df = pd.concat(all_patient_summaries, axis=0, sort=False).reset_index().drop(columns=['level_0'])\n",
    "    return all_patient_summaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As discussed in the cross_validate and balance functions, this step takes in the standard, preprocessed\n",
    "dataframe of MIMIC data to be used for this experiment, and outputs Lists of balanced ID sets to be used.\n",
    "The cross-validation and balancing steps produce a much better, and less biased model\n",
    "\n",
    "Input:\n",
    "icu_ids: DataFrame of IDs of all patients\n",
    "debug_flag: Whether or not print statements are included\n",
    "save_to_file: Whether or not intermediate steps are written to a file\n",
    "regenerate_flag: When no files have been saved previously, set this to True to rerun all data\n",
    "patientdata: The standard input MIMIC DataFrame\n",
    "\n",
    "Output:\n",
    "train_ids_set: A 2D list of all the lists of IDs to be used for training\n",
    "test_ids_set:  A 2D list of all the lists of IDs to be used for testing\n",
    "train_flag_set: A boolean representation of all the lists of IDs to be used for training\n",
    "test_flag_set: A boolean representation of all the list of IDs to be used for testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cross_validate_and_balance(icu_ids:pd.DataFrame, debug_flag:bool, save_to_file:bool, \n",
    "                                     regenerate_flag:bool, patientdata:pd.DataFrame, regenerate_cross:bool,\n",
    "                                     all_factors:List[str]) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    if regenerate_flag:\n",
    "        if not(regenerate_cross):\n",
    "            # Flatten the full dataframe into summary statistics per patient\n",
    "            # This is used to stratify and balance appropriately\n",
    "            all_patient_summaries_df = flatten_timeseries_data(patientdata, icu_ids, all_factors)\n",
    "            # This step messes up the indexes, we need to key the indexes to the correct ICU_IDS\n",
    "            incorrect_indices = [i for i in all_patient_summaries_df.index]\n",
    "            # Use dictionary comprehension to create a way of mapping the incorrect indices with \n",
    "            # their correct counterparts\n",
    "            id_correction_dict = {incorrect_indices[i]: icu_ids[i] for i in range(0, len(incorrect_indices))}\n",
    "            if debug_flag:\n",
    "                print(all_patient_summaries_df)\n",
    "            if save_to_file:\n",
    "                    with open('pre_cross_data.txt', 'wb') as fp:\n",
    "                        pickle.dump(all_patient_summaries_df, fp)\n",
    "            # Grab the patient's 90d death statuses\n",
    "            death_status = patientdata.drop_duplicates('icustayid')['mortality_90d']\n",
    "            # To ensure everything is lined up adjacently, apply two more reset_index's\n",
    "            # Also, the creation of the dataframe creates column indexes we don't desire. Get rid of these\n",
    "            id_table = pd.DataFrame(icu_ids)\n",
    "            id_table.columns = ['column_name']\n",
    "            death_status = pd.DataFrame(death_status.values.tolist()).reset_index()\n",
    "            death_status = death_status.drop(death_status.columns[0], axis=1)\n",
    "            death_status.columns = ['death_state']\n",
    "            icu_pair_set:pd.DataFrame = pd.concat([id_table, death_status, all_patient_summaries_df], axis=1, sort=False)\n",
    "            if save_to_file:\n",
    "                with open('interm_cross_data.txt', 'wb') as fp:\n",
    "                        pickle.dump(icu_pair_set, fp)\n",
    "                with open('id_correction_dictionary', 'wb') as fp:\n",
    "                        pickle.dump(id_correction_dict, fp)\n",
    "        # Remove duplicates from the columns list overall\n",
    "        icu_pair_set = None\n",
    "        id_correction_dict = None\n",
    "        with open('interm_cross_data.txt', 'rb') as fp:\n",
    "            icu_pair_set = pickle.load(fp)\n",
    "        with open('id_correction_dictionary', 'rb') as fp:\n",
    "            id_correction_dict = pickle.load(fp)\n",
    "        predictor_columns = list(set(icu_pair_set.columns))\n",
    "        # Death state is the response\n",
    "        predictor_columns.remove('death_state')\n",
    "        response_column = ['death_state']\n",
    "        # Split the data using 10-fold Cross Validation and Balance the Training Dataset\n",
    "        train_ids_set, test_ids_set = crossval_split(icu_pair_set, predictor_columns, response_column, \n",
    "                                                     is_debug=False, id_correction_dict=id_correction_dict, balance_flag=False)\n",
    "        train_flag_set = []\n",
    "        test_flag_set = []\n",
    "        for train_ids in train_ids_set:\n",
    "            train_flag_set.append(patientdata['icustayid'].isin(train_ids))\n",
    "        for test_ids in test_ids_set:\n",
    "            test_flag_set.append(patientdata['icustayid'].isin(test_ids))\n",
    "        # Save the full data into files to be reused\n",
    "        # There is no reason to redo the operations if nothing about the dataset has changed\n",
    "        if save_to_file:\n",
    "            with open('train_flags.txt', 'wb') as fp:\n",
    "                pickle.dump(train_flag_set, fp)\n",
    "            with open('test_flags.txt', 'wb') as fp:\n",
    "                pickle.dump(test_flag_set, fp)\n",
    "            with open('train_ids.txt', 'wb') as fp:\n",
    "                pickle.dump(train_ids_set, fp)\n",
    "            with open('test_ids.txt', 'wb') as fp:\n",
    "                pickle.dump(test_ids_set, fp)\n",
    "            \n",
    "        return train_ids_set, test_ids_set, train_flag_set, test_flag_set\n",
    "    else:\n",
    "        # If the data has already been processed, load it from file\n",
    "        train_ids_set = []\n",
    "        test_ids_set = []\n",
    "        train_flag_set = []\n",
    "        test_flag_set = []\n",
    "        with open('train_flags.txt', 'rb') as fp:\n",
    "            train_flag_set = pickle.load(fp)\n",
    "        with open('test_flags.txt', 'rb') as fp:\n",
    "            test_flag_set = pickle.load(fp)\n",
    "        with open('train_ids.txt', 'rb') as fp:\n",
    "            train_ids_set = pickle.load(fp)\n",
    "        with open('test_ids.txt', 'rb') as fp:\n",
    "            test_ids_set = pickle.load(fp)\n",
    "        return train_ids_set, test_ids_set, train_flag_set, test_flag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# calculate_all_hyperparameter_combos \n",
    "# \n",
    "# Input: N/A\n",
    "# \n",
    "# Output: A 2D Float Matrix containing all the desired hyperparameters to test\n",
    "# \n",
    "\"\"\"\n",
    "def calculate_all_hyperparameter_combos() -> float_matrix2D:\n",
    "    # Internal function to iterate by double\n",
    "    def frange(start, stop, step):\n",
    "        i = start\n",
    "        while i < stop:\n",
    "            yield i\n",
    "            i += step\n",
    "    # Gammas are incremented by 0.01\n",
    "    # Alphas are incremented by 0.1\n",
    "    gamma_values = [round(gamma, 2) for gamma in frange(0.01, 1.0, 0.01)]\n",
    "    alpha_values = [round(alpha, 2) for alpha in frange(0.1, 1.0, 0.1)]\n",
    "    # Return all possible combinations of hyperparameters using the cartesian product\n",
    "    return list(cartesian_prod(gamma_values, alpha_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 8 arguments: \n",
    "# \n",
    "# train_flag:       The dataframe representing whether or not a row is used in training or not\n",
    "# test_flag:        The dataframe representing whether or not a row is used in testing or not\n",
    "# MIMIC_zscores:    The dataframe representing the zscores of the dataset\n",
    "# debug_flag:       A flag that determines if print statements are executed\n",
    "# save_flag:        A flag that determines if the training_zscores are saved\n",
    "# patient_data:     The raw MIMIC dataframe\n",
    "# bloc_name:        The name of the column that dictates a 'bloc' of time within an ICU visit\n",
    "# id_name:          The name of the column that dictates an ICU stay's ID\n",
    "# death_name:       The name of the column that dictates a patient's life status \n",
    "# \n",
    "# and returns 7 values:\n",
    "# \n",
    "# train_zscores:   The portion of MIMIC_zscores that is in the training set\n",
    "# test_zscores:    The portion of MIMIC_zscores that is in the testing set\n",
    "# train_blocs:     The rows of the MIMIC dataset that is in the training set\n",
    "# test_blocs:      The rows of the MIMIC dataset that is in the testing set\n",
    "# train_id_list:   The list of all IDs in the training set\n",
    "# train_90d:       A flag indicating if a given patient died in the training set after 90d\n",
    "# test_90d:        A flag indicating if a given patient died in the testing set after 90d\n",
    "\"\"\"\n",
    "\n",
    "def zscores_for_train_and_test(train_flag:pd.DataFrame, test_flag:pd.DataFrame, MIMIC_zscores:pd.DataFrame, \n",
    "                               debug_flag:bool, save_flag:bool, patient_data:pd.DataFrame, bloc_name:str, \n",
    "                               id_name:str, death_name:str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, \n",
    "                                                                pd.DataFrame, pd.DataFrame, pd.DataFrame, \n",
    "                                                                pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    # Seperate the Z-Scores for the training set and the testing set\n",
    "    train_zscores:pd.DataFrame = MIMIC_zscores[train_flag]\n",
    "    test_zscores:pd.DataFrame = MIMIC_zscores[test_flag]\n",
    "\n",
    "    # The blocs of relevance in order based on the train and test set\n",
    "    # These will be used to build relevant data frames later down\n",
    "    train_blocs:List[int] = patientdata[train_flag][bloc_name]\n",
    "    test_blocs:List[int] = patientdata[test_flag][bloc_name]\n",
    "\n",
    "    # Doing the same with the patient ids\n",
    "    train_id_list:pd.DataFrame = patientdata[train_flag][id_name].reset_index()['icustayid']\n",
    "    test_id_list:pd.DataFrame = patientdata[test_flag][id_name].reset_index()['icustayid']\n",
    "    # We modify a column later to use for the test_id_list, not necesssary here.\n",
    "\n",
    "    # Grabbing the boolean values for the patients who died within 90 days in the training set\n",
    "    train_90d:pd.DataFrame = patientdata[train_flag][death_name]\n",
    "    test_90d:pd.DataFrame = patientdata[test_flag][death_name]\n",
    "    \n",
    "\n",
    "    if save_flag:\n",
    "        # Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('train_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(train_zscores, fp)\n",
    "        with open('test_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(test_zscores, fp)\n",
    "    return train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, test_id_list, train_90d, test_90d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# determine_optimal_clusters is a method that uses the modern curvature method to optimize clusters in the KMeans\n",
    "# clustering algorithm. The data for this is provided by euclidean variance calculated using Redhawk supercluster\n",
    "#\n",
    "# The input is:\n",
    "# variance_data: A list containing the total variance observed at each k-value (number of clusters)\n",
    "# show_graph: whether or not we would like to display the graph of the curvature\n",
    "# \n",
    "# The output is:\n",
    "# optimal_cluster_count: The int representing the optimal number of clusters\n",
    "\"\"\"\n",
    "def determine_optimal_clusters(variance_data:List[float], show_graph:bool): \n",
    "    # A large portion of this was provided/inspired by code from Dr. Giabbanelli's machine learning course\n",
    "    lower_bound:int = 1\n",
    "    upper_bound:int = len(variance_data) + 1\n",
    "    val_range = range(lower_bound, upper_bound)\n",
    "    # Approximating the values to a polynomial fit\n",
    "    coefs:List[float] = np.polyfit(val_range, variance_data, 3)\n",
    "    # Generate a list of values at each point\n",
    "    coefs_vals:List[float] = np.polyval(coefs[::1], val_range)\n",
    "    if show_graph:\n",
    "        # Generate the plot\n",
    "        plt.plot(val_range, variance_data)\n",
    "        plt.show()\n",
    "    # Curve values fluctuate based on geometric scaling, as such, it is required\n",
    "    # to test different k-values across different alphas\n",
    "    alphas:List[float] = [i/20.0 for i in range(0, 400)]\n",
    "    max_curve:float = -1\n",
    "    max_k:float = -1\n",
    "    # Test on a variety of Alphas and find the maximal result\n",
    "    for alpha in alphas:\n",
    "        # Scale the variation values, equation coefficients, and curve values based on various alphas\n",
    "        scaled_vars:List[float] = [alpha * variance_val for variance_val in variance_data]\n",
    "        scaled_coefs:List[float] = np.polyfit(val_range, scaled_vars, 3)\n",
    "        # Calculate the curvature of the line at each step\n",
    "        curve_vals = np.polyder(scaled_coefs)\n",
    "        curve_vals_mod = np.polyder(curve_vals)\n",
    "        scaled_curves:List[float] = []\n",
    "        # The authors of the curvature method use these two tranformations to calculate values\n",
    "        for k in val_range:\n",
    "            function_val_one:float = abs(np.polyval(curve_vals_mod, k))\n",
    "            function_val_two:float = 1 + np.polyval(curve_vals, k)**2\n",
    "            scaled_curves.append(function_val_one / (function_val_two**1.5))\n",
    "        # Iterate over all the scaled curves to determine the max\n",
    "        index_and_value = max(enumerate(scaled_curves), key=(lambda x: x[1]))\n",
    "        max_index:float = index_and_value[0]\n",
    "        max_value:float = index_and_value[1]\n",
    "        if(max_value > max_curve):\n",
    "            max_curve = max_value \n",
    "            max_k = max_index\n",
    "    # The k value needs to be scaled back to it's actual value, not its list index\n",
    "    true_k = max_k + 1\n",
    "    return true_k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters(train_zscores:pd.DataFrame, max_state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                max_num_iter:int=32):\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in range(1, max_state_count):\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(train_zscores)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "    with open('variance_full_results.txt', 'wb') as fp:\n",
    "            pickle.dump(variance_results, fp)\n",
    "    return variance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_parallel(num:int):\n",
    "    data_set=sample_train_set\n",
    "    max_state_count=state_count\n",
    "    num_loops_per_iter=10000\n",
    "    max_num_iter=32\n",
    "    total_needed_runs:List[int] = [i for i in range(1, 751)]\n",
    "    thread_needed_runs:List[int] = np.array_split(total_needed_runs, 24)[num]\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in thread_needed_runs:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(sample_train_set)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "        single_run = f'{state_count},{total_variance}'\n",
    "        with open('all_cluster_runs.csv', 'a') as f:\n",
    "            print(single_run, file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_driver(data_set:pd.DataFrame, run_optimal_clusters:bool, run_multithread:bool, \n",
    "                                      show_graph:bool, thread_count:int, max_state_count:int) -> int:\n",
    "    # Cluster selection\n",
    "    optimal_cluster_count = 0 \n",
    "    # If we wish to find the optimal number of clusters\n",
    "    if run_optimal_clusters:\n",
    "        # This code is here for posterity, and cannot be run natively in juptyer. Use the stock Python CLI command or \n",
    "        # PyPi to actually run this. Note, it can take forever with large Nodes\n",
    "        if run_multithread:     \n",
    "            pool = multiprocessing.Pool()\n",
    "            pool.map(calculate_optimal_clusters_parallel, range(23))\n",
    "        else:\n",
    "            calculate_optimal_clusters(data_set=data_set, max_state_count=max_state_count, num_loops_per_iter=10000, \n",
    "                                max_num_iter=32)\n",
    "    # Load the data from the cluster variances into here\n",
    "    cluster_results = []\n",
    "    with open('all_cluster_runs.csv', 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            cluster_results.append(float(row[1]))\n",
    "    optimal_cluster_count = determine_optimal_clusters(cluster_results, show_graph=show_graph)\n",
    "    return optimal_cluster_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The kmeans_cluster_calculations function takes 5 arguments: \n",
    "# data_set:               The training dataset used to generate the various clusters\n",
    "# state_count:            Total number of clusters desired in the end result\n",
    "# num_loops_per_iter:     The number of loops ran per 1 iteration to approximate clusters\n",
    "# max_num_iter:           Number of iterations to run the algorithm\n",
    "# debug_flag:             True indicates print statements are included, False indicates no prints\n",
    "# regenerate_flag:        True indictates the kmeans will be run again. False means the cached/previous file value will be used\n",
    "# \n",
    "# and returns 4 values:\n",
    "# \n",
    "# cluster_values:         A value generated based on the Z-Scores of all the patient's data relative to each other\n",
    "# cluster_labels:         A number generated for the correspond state -> value relationship. Range is 0 to state_count\n",
    "# train_zscores:          The Z-Scores of the training set as created earlier\n",
    "# closest_clusters:       A list representing which state each patient datapoint is closest to\n",
    "# \n",
    "\"\"\"\n",
    "def kmeans_cluster_calculations(train_zscores:pd.DataFrame, test_zscores:pd.DataFrame,\n",
    "                                max_num_iter:int=32, state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                debug_flag:bool=True, regenerate_flag=True, load_zscores=False) -> (List[List[np.float64]], List[int], pd.DataFrame, \n",
    "                                                                                                     np.ndarray, np.ndarray):\n",
    "    # In order to prepare a proper set of states, we want to use k-means clustering to group various patients into \n",
    "    # distinct states based on Z-Scores\n",
    "\n",
    "    # K-Means or K-Means++ is a technique used to condense very diverse and sparse data into similar groups called 'clusters'\n",
    "    # The K-means algorithm will create k clusters from N data points. In the case of this research,\n",
    "    # the algorithm divides patients into groups that have similar data (age, blood pressure, etc..) and creates a faux 'point'\n",
    "    # at the center of that particular clustering of data\n",
    "\n",
    "    # The KMeans takes three 'settings' arguments\n",
    "    # 1. n_clusters: The number of clusters (later to be used as states), that we desire the algorithm to produce\n",
    "    # this value has been preset to state_count which is 750\n",
    "    # 2. max_iter: How many times each round of k-means clustering will make adjustments, set at 10,000 in my case\n",
    "    # 3. n_init: The number of max_iter batches that will be conducted in a row. The best of these will be chosen\n",
    "    # and saved in the variable clusters_models\n",
    "    cluster_models = []\n",
    "    cluster_labels:List[int] = [] \n",
    "    cluster_values:List[List[np.float64]] = []\n",
    "    if regenerate_flag:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(train_zscores)\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        cluster_labels = clusters_models.labels_\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        with open('cluster_labels.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.labels_, fp)\n",
    "        with open('cluster_centers.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.cluster_centers_, fp)\n",
    "        if debug_flag:\n",
    "            print(clusters_models.labels_)\n",
    "            print(clusters_models.cluster_centers_)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    with open ('cluster_centers.txt', 'rb') as fp:\n",
    "        cluster_values = pickle.load(fp)\n",
    "    with open ('cluster_labels.txt', 'rb') as fp:\n",
    "        cluster_labels = pickle.load(fp)\n",
    "    if load_zscores:\n",
    "        with open ('train_zscores.txt', 'rb') as fp:\n",
    "            train_zscores = pickle.load(fp)\n",
    "        with open ('test_zscores.txt', 'rb') as fp:\n",
    "            test_zscores = pickle.load(fp)\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(cluster_values, \"\\n\", \"Dimensions: \", len(cluster_values),\" x \", len(cluster_values[0]), \"\\n\", train_zscores)\n",
    "    \n",
    "    # We now want to use the clusters to determine their nearest real data point neighbors\n",
    "    # As a visual of this. Suppose we have 4 flags of different colors scattered over a park. The K-Means++ algorithm\n",
    "    # is what planted the flags in the middle of groups of people that are similar. The KNN Search (K nearest neighbor search)\n",
    "    # can be used in MatLab as a simple point finder instead of as a more complicated Supervised Learning algorithm. In Python \n",
    "    # we can make use of the Vector Quanization (vq) package to assign each point to a centroid\n",
    "    closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "    closest_clusters_test:np.ndarray = vq(test_zscores, cluster_values)\n",
    "    \n",
    "\n",
    "    if debug_flag:\n",
    "        print(len(closest_clusters[0]))\n",
    "\n",
    "    # As an aside, closest_clusters[1] contains the distance between each point's values (in this case 50 of them)\n",
    "    # and their closest cluster's values.\n",
    "    # Ex: If a point is [1, 1, 1] and it's closest cluster is the point [3, 3, 3]  closest_clusters[1] would contain the vector\n",
    "    # [abs(3 - 1), abs(3 - 1), abs(3 - 1)] or [2, 2, 2]\n",
    "\n",
    "    # Validate that all the points are in the range 0-749 (since there are only 750 clusters as specified previously)\n",
    "    for i in closest_clusters[0]:\n",
    "        if(i > (state_count - 1) or i < 0):\n",
    "            print(\"The clusters you are searching for are not configured properly and are out of bounds\")\n",
    "            print(\"Did you modify the cluster_count variable without changing this error configuration?\")\n",
    "    \n",
    "    return cluster_values, cluster_labels, train_zscores, closest_clusters, closest_clusters_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The generate_action_column function takes 4 arguments: \n",
    "#\n",
    "# column_values: A series of column values from a dataframe that we want to turn into action states\n",
    "# num_groups: How many groups or distinct actions we want to split the data into\n",
    "# column_name: The name of the column used for print debug statements\n",
    "# num_rows: The total number of rows in the full column before modifications (This is normally patientdata[column_name].size)\n",
    "# \n",
    "# This function returns column_actions, a series that represents the 'action', or group that each row of data falls under.\n",
    "#\n",
    "# An example is found down below, but in words, this function takes a full column of data, groups \n",
    "# the values for that data into num_groups distinct actions, and returns a series representing actions based on row\n",
    "# \n",
    "# Ex: Patients' blood pressure might be grouped into 5 categories (Action 1: < 20 mmHg, Action 2: > 20 mmHg && < 60 mmHg... etc)\n",
    "\"\"\"\n",
    "\n",
    "def generate_action_column(column_values:pd.DataFrame, num_groups:int, column_name:str, num_rows:int, debug_flag:bool=True) -> pd.Series:\n",
    "    # Determine minimum and maxium to scale data appropriately\n",
    "    if debug_flag:\n",
    "        print(\"Old Lowest \", column_name, \" Rank: \", min(column_values.rank()))\n",
    "        print(\"Old Highest \" , column_name,  \" Rank: \", max(column_values.rank()))\n",
    "    # Now we want to rank these actions in order of their value (lowest to highest)\n",
    "    # Normalizing according to lowest and highest rank\n",
    "    \n",
    "    # Moving the minimum to zero\n",
    "    column_ranks:pd.Series = (column_values.rank() - min(column_values.rank()))\n",
    "    # Shifting the max to approximately 1.0\n",
    "    column_ranks:pd.Series = column_ranks / max(column_ranks)\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Validate that the range is indeed 0 to 1\n",
    "        print(\"New Lowest \", column_name, \" Rank: \", min(column_ranks))\n",
    "        print(\"New Highest \", column_name, \" Rank: \", max(column_ranks))\n",
    "\n",
    "    # The Max of all column values needs to be nearly 1, and the min of all column\n",
    "    # values needs to be nearly 0 \n",
    "    if round(max(column_ranks), 3) != 1 or round(min(column_ranks), 3) != 0:\n",
    "        print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "        print(\"Current max: \", round(max(column_ranks), 3))\n",
    "        print(\"Curret min: \", round(min(column_ranks), 3))\n",
    "    # Normalize the rank values to even intevals of ranks\n",
    "    old_values:List[float] = np.sort(column_ranks.unique()).tolist()\n",
    "    even_intervals:List[float] = [i/column_ranks.unique().size for i in range(0, column_ranks.unique().size)]\n",
    "    # Iterate over the Series to apply the normalized values\n",
    "    for i in range(0, column_ranks.size):\n",
    "        old_value_index:float = old_values.index(column_ranks[i])\n",
    "        column_ranks[i] = even_intervals[old_value_index] \n",
    "    \n",
    "    # This is a mathematics trick to seperate all the values into {num_groups} distinct groups based on their rank.\n",
    "    # Given different columns of interest this can take different forms. For IV fluids, this number is 5.\n",
    "    column_groups:pd.Series = np.floor(((column_ranks + 1.0/float(num_groups)) * (num_groups - 1))) + 1\n",
    "    \n",
    "    # Validate that groups are all associated with desired group split\n",
    "    if not(column_groups.isin([i for i in range(1, num_groups + 1)]).any()):\n",
    "        print(\"Groups chosen fall outside the desired 1-\" + num_groups + \" window\")\n",
    "\n",
    "    column_actions:pd.Series = pd.Series([1 for i in range(0, num_rows)])\n",
    "\n",
    "    # If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "    for index in column_groups.index:\n",
    "        column_actions[index] = column_groups[index]\n",
    "        \n",
    "    return column_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This function takes two arguments:\n",
    "# actions_column: A column of action groups generated by the above function (generate_action_column())\n",
    "# real_values: The actual values from the dataset corresponding to the same column as actions_column\n",
    "# and returns a list that contains the real median values for each 'group' actions.\n",
    "#\n",
    "# Ex: We apply the function to the action_column \"IV_Fluid\", which has split the data into 4 different groups of \n",
    "# IV_Fluid actions. This function will produce a list containing the median amount of IV_Fluid administered for each of those\n",
    "# groups (Group 1 -> Adminster 20 mL, Group 2 -> Administer 40 mL, Group 3 -> Administer 60 mL, Group 4 -> Administer 80 mL\n",
    "\"\"\"\n",
    "\n",
    "def median_action_values(actions_column: pd.DataFrame, real_values:pd.DataFrame) -> List[np.float64]:\n",
    "    # Grab all the unique actions for a column and sort them\n",
    "    all_groups:List[np.float64] = np.sort(actions_column.unique())\n",
    "    # Concatanate the group number and real value for each row\n",
    "    action_set:pd.DataFrame = pd.concat([actions_column, real_values], axis=1, sort=False)\n",
    "    # Name the columns for accurate querying\n",
    "    action_set.columns = ['group_id', 'data_val']\n",
    "    # Grab the median value for each group based on group number using python list comprehension\n",
    "    median_values:List[np.float64] = [np.median(action_set[action_set['group_id'] == i]['data_val']) for i in all_groups]\n",
    "    return median_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# This function takes one argument\n",
    "# list_action_columns: This is a Pandas dataframe that contains all the action_columns we desir to be grouped by index\n",
    "# This can be retrieving using the previously defined 'median action' function \n",
    "# \n",
    "# and returns two items:\n",
    "# list_action columns: The 'keys' or integers that represent every permutation of actions\n",
    "# chosen_action: The key that was chosen based on the action values in each column\n",
    "\"\"\"\n",
    "def generate_action_matrix(list_action_columns:pd.DataFrame) -> (List[int], List[int]):\n",
    "    # Grabs the list of columns the user has provided for use\n",
    "    desired_columns:List[str] = [column for column in list_action_columns]\n",
    "    # Drops all group combinations that are duplicates\n",
    "    list_action_columns_indexes:pd.DataFrame = list_action_columns.drop_duplicates(desired_columns)\n",
    "    # Sorts all combinations in order\n",
    "    list_action_columns_indexes = list_action_columns_indexes.sort_values(desired_columns)\n",
    "    # Create a dictionary based on the values from the dataframe \n",
    "    list_action_columns_indexes:List[int] = list_action_columns_indexes.values.tolist() \n",
    "    # Determine which index in the list each row corresponds to \n",
    "    # Ex: For an 2-D action permutation list of [1,1] thru [5,5], there are 5 x 5 possibilities\n",
    "    # {1..5}, {1..5}, so there are 25 possible permutations, the indexes will run 1 - 25\n",
    "    chosen_action:List[int] = [list_action_columns_indexes.index(val_pair) for val_pair in list_action_columns.values.tolist()]\n",
    "    # Return the keys first, and then the true values for the dataset\n",
    "    return list_action_columns_indexes, chosen_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The build_dataset_actions function takes 7 arguments: \n",
    "# patientdata: A Dataframe representing all the patients and their data\n",
    "# first_column: Our first column that we desire to build our action matrix with (first_column x second_column) (SOFA by default)\n",
    "# second_column: Our second column that we desire to build or action matrix with (first_column x second_column) (qSOFA by default)\n",
    "# num_groups_first_column: The number of groups we want to divide our first column into (5 by default)\n",
    "# num_groups_second_column: The number of groups we want to divide our second column into (4 by default)\n",
    "# debug_flag: A boolean that determines whether we want debug prints presents\n",
    "# graph_flag: Whether we want to print the graph or not\n",
    "# train_flag: The List representing which rows have been marked for training\n",
    "# \n",
    "# and returns 3 values:\n",
    "# \n",
    "# train_chosen_actions:        The actions for each row of the training dataset\n",
    "# train_action_values:         The matrix representing (first_column x second_column)\n",
    "# action_list:                 A list of actions taken at every given datapoint\n",
    "\"\"\"\n",
    "def build_dataset_actions(patientdata:pd.DataFrame, first_column:str=\"SOFA\", second_column:str=\"qSOFA\", num_groups_first_column:int=5,\n",
    "                         num_groups_second_column:int=4, debug_flag:bool=True, graph_flag:bool=True,\n",
    "                         train_flag:pd.Series=[], test_flag:pd.Series=[]) -> (pd.Series, List[List[int]], List[int]):\n",
    "    # Generate the actions for a the first desired column of data\n",
    "    first_col:pd.DataFrame = patientdata[first_column]\n",
    "    first_col_actions:pd.Series = generate_action_column(column_values = first_col, num_groups = num_groups_first_column, \n",
    "                                                        column_name = first_column, \n",
    "                                                         num_rows = patientdata[first_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(first_col_actions.unique())    \n",
    "    # Now do the same for the second desired column of data\n",
    "    second_col:pd.DataFrame = patientdata[second_column]\n",
    "    second_col_actions:pd.Series = generate_action_column(column_values = second_col, num_groups = num_groups_second_column,  \n",
    "                                                         column_name = second_column, \n",
    "                                                         num_rows = patientdata[second_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(second_col_actions.unique())\n",
    "    \n",
    "    # Obtain the median real values that each division represents\n",
    "    first_median_actions:List[np.float64] = median_action_values(actions_column = first_col_actions, real_values = patientdata[first_column])\n",
    "    second_median_actions:List[np.float64] = median_action_values(actions_column = second_col_actions, real_values = patientdata[second_column])\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(first_column,\" Action Median Values:\", str(first_col_actions), \"\\n\" + second_column + \":\", second_median_actions, \"\\n\")\n",
    "    ###\n",
    "    # FINISH CONSTRUCTION OF ALL ACTIONS AND THEIR VALUES\n",
    "    ###\n",
    "    # Combine the columns that we desire to observe (iv_fluid_actions, vasopressor_actions)\n",
    "    combined_groups:pd.DataFrame = pd.concat([first_col_actions, second_col_actions], axis=1, sort=False)\n",
    "    # Name the columns for proper usage in the function\n",
    "    combined_groups.columns = [first_column, second_column]\n",
    "    \n",
    "    # The Key value pair for every datapoint and the corresponding action taken at that point\n",
    "    action_keys, action_list = generate_action_matrix(list_action_columns = combined_groups)\n",
    "    # Plot the distribution of actions\n",
    "    if graph_flag:\n",
    "        plt.hist(action_list, density=False, bins=20)  # `density=False` would make counts\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Index of Action Chosen: 1 through 24\")\n",
    "    # Grab a Series representing the action taken by the train data only\n",
    "    train_chosen_actions:pd.Series = pd.Series(action_list)[train_flag]\n",
    "    # Grab a Series representing the action taken by the test data only\n",
    "    test_chosen_actions:pd.Series = pd.Series(action_list)[test_flag]\n",
    "\n",
    "    # Assign all action choices to their corresponding median values as shown previously\n",
    "    if debug_flag:\n",
    "        print(first_median_actions, second_median_actions)\n",
    "\n",
    "\n",
    "    # This gives us the representative median values for a patient's vitals present in various action groups\n",
    "    # action_keys[i] corresponds to train_action_values[i]\n",
    "    # So, if the patient falls into group [1, 1] or no iv fluid given, no vasopressor administered,\n",
    "    # The corresponding median values for this group will be represented by train_action_values (0.0, 0.0).\n",
    "    # A patient in group [1, 2] (no iv fluid, a little vasopressor) will have a median real value of (0.0, 0.04)\n",
    "    action_values_matrix:List[List[int]] = list(cartesian_prod(first_median_actions, second_median_actions))\n",
    "\n",
    "    if len(action_values_matrix) != len(first_median_actions) * len(second_median_actions):\n",
    "        print(\"Something went wrong in determining the Cartesian product\")\n",
    "    \n",
    "    action_count:int = len(action_values_matrix)\n",
    "    return train_chosen_actions, test_chosen_actions, action_values_matrix, action_list, action_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_prestate_matrix_train function takes 5 arguments: \n",
    "# train_90d:              A column representing whether or not a patient was dead or alive at the end of 90days\n",
    "# train_blocs:            All the rows of data for a patient for each individual hospital stay\n",
    "# closest_clusters:       The closest data cluster that a given data point falls near\n",
    "# train_chosen_actions:   The action that is represented by two of the patient's characteristics, calculated previously\n",
    "# is_debug:               Whether or not statements are printed over time.\n",
    "# \n",
    "# and returns 1 value:\n",
    "# \n",
    "# qlearning_dataset_mod: The full training dataset configured to be converted into states and actions\n",
    "# all_lower_ranges:      The lower bound for the reward values\n",
    "# all_upper_ranges:      The upper bound for the reward values\n",
    "\"\"\"\n",
    "def construct_prestate_matrix_train(train_90d:pd.DataFrame, train_blocs:pd.DataFrame, closest_clusters:List[List[int]],\n",
    "                                    train_chosen_actions:pd.DataFrame, train_id_list:pd.DataFrame, \n",
    "                                    is_debug:bool = True, action_count:int = 20, \n",
    "                                    state_count:int = 750) -> (pd.DataFrame, List[int], List[int]):\n",
    "    ###\n",
    "    # BEGIN CONSTRUCTION OF PRE-STATE MATRIX\n",
    "    # This will be used to build the full state/action matrix\n",
    "    ### \n",
    "\n",
    "    # Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "    # If they have died, the range is [-100, 100]\n",
    "    # If they are alive, the range is [100, -100]\n",
    "    range_vals:List[int] = [100, -100]\n",
    "    # Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "    # This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "    train_90d_polarity:List[int] = (2 * (1 - train_90d) - 1)\n",
    "    range_matrix:List[int] = [np.multiply(polarity, range_vals) for polarity in train_90d_polarity]\n",
    "    # Grab the lower range limit and upper range limit seperately in order to build the\n",
    "    # full range of reward values\n",
    "    all_lower_ranges:List[int] = [i[0] for i in range_matrix]\n",
    "    all_upper_ranges:List[int] = [i[1] for i in range_matrix]\n",
    "        \n",
    "    # The qlearning_dataset prior to modification contains 6 columns and ~190885 rows (around 75% of the data)\n",
    "    # The columns are as follows:\n",
    "    #\n",
    "    # training_bloc: time_series stamps for a patient's state over time, very in range from {1..?}\n",
    "    #\n",
    "    # closest_cluster_index: The index of the nearest cluster to the z-scores of the patient's data, \n",
    "    # corresponding actual data for each cluster's index (i) can be found in cluster_values[i]\n",
    "    #\n",
    "    # chosen_action_index: The chosen action or representation of a patient's IV_Fluid and Vasopressor status [0 - 24]\n",
    "    # \n",
    "    # 90d_mortality_status: 0 means the patient is alive 90 days after discharge from ICU\n",
    "    #                      1 means the patient is dead  90 days after discharge from ICU\n",
    "    #\n",
    "    # lower_range + upper_range: An index to be used later on, gathered from the range index\n",
    "    if is_debug:\n",
    "        print(\"Training Blocs Length: \", str(len(train_blocs)), \"\\nClosest Clusters Length: \", str(len(closest_clusters[0])), \n",
    "              \"\\nAction List Length: \", str(len(train_chosen_actions)), \"\\nTrain 90d Length\", str(len(train_90d)), \n",
    "              \"\\nRange Matrix Length: \", len(range_matrix), \"\\nTrain IDs Length\", str(len(train_id_list)))\n",
    "    qlearning_dataset:pd.DataFrame = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters[0]), \n",
    "                                   pd.Series(train_chosen_actions.tolist()), \n",
    "                                   pd.Series(all_lower_ranges),\n",
    "                                   pd.Series(train_90d.tolist()),\n",
    "                                   train_id_list], \n",
    "                                   axis=1, sort=False)\n",
    "    qlearning_dataset.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', '90d_mortality_status', 'patient_id']\n",
    "    if is_debug: \n",
    "        print(qlearning_dataset)\n",
    "    # Modify the set for the final time in order to construct the final life + death states for each patient\n",
    "    qlearning_dataset_mod = modify_qlearning_dataset(ql_dataset = qlearning_dataset, state_count = state_count)\n",
    "    if is_debug:\n",
    "        # Total patients being observed in the test\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['training_bloc'] == 1]['training_bloc']))\n",
    "        # Show that we now have end states established \n",
    "        print(len(qlearning_dataset[qlearning_dataset['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == state_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] ==  state_count + 1]['chosen_action_index']))\n",
    "        print(qlearning_dataset_mod, \"\\n\")\n",
    "    return qlearning_dataset_mod, all_lower_ranges, all_upper_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# modify_qlearning_dataset is a function that takes a dataframe intended for qlearning and modifies\n",
    "# it in preparation for the ML. In essence, it runs through creating the life and and death states in preparation\n",
    "# for constructing the MDP\n",
    "#\n",
    "# Parameters: \n",
    "# ql_dataset:  The Dataset we want to modify in preparation\n",
    "# is_training: Whether we are building the training set or not.\n",
    "# is_debug:    Whether prints are included or not\n",
    "#       \n",
    "# \n",
    "# Returns: qlearning_dataset_mod - The modified dataset\n",
    "\"\"\"\n",
    "\n",
    "def modify_qlearning_dataset(ql_dataset:pd.DataFrame, state_count:int) -> pd.DataFrame:\n",
    "    # The base qlearning_dataset does not account for endpoints in either life or death\n",
    "    # These states have not been established yet, which is what this step corrects\n",
    "    qlearning_dataset:pd.DataFrame = ql_dataset.copy()\n",
    "    qlearning_dataset_len:int = len(qlearning_dataset.index)\n",
    "    # We need space to add a death/life state for every patient, about a 20% increase in size from the original MDP\n",
    "    # We will cut the excess off by the end of the loop\n",
    "    qlearning_dataset_len_mod:float = int(np.floor(qlearning_dataset_len * 1.2))\n",
    "    qlearning_dataset_mod:np.ndarray = []\n",
    "    qlearning_dataset_mod:np.ndarray = np.array([[0 for i in range(0, 6)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "    # Start construction of modified data\n",
    "    row:int = 0\n",
    "    # In Markov theory, an absorbing state is one which can be entered, but cannot be left. (Similar to the Hotel California)\n",
    "    # In the case of this experiment, those states are either life (state_count) or death (state_count + 1) per patient as\n",
    "    # defined by me (750, 751)\n",
    "    absorbing_states:List[int] = [state_count, state_count + 1]\n",
    "    # Start the loop to begin capping the markov chain off at life and death states\n",
    "    for i in range(0, qlearning_dataset_len - 1):\n",
    "        # Use the already gathered data for each row\n",
    "        qlearning_dataset_mod[row, :] = qlearning_dataset.iloc[i][0:6]\n",
    "        # If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\n",
    "        if qlearning_dataset.iloc[i + 1]['training_bloc'] <= qlearning_dataset.iloc[i]['training_bloc']:\n",
    "            # Grab the row\n",
    "            whole_row:pd.DataFrame = qlearning_dataset.iloc[i]\n",
    "            # Set most of the row to the original data's values, except set the action to be either state 750 or 751\n",
    "            # Life or death respectively\n",
    "            row = row + 1\n",
    "            # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "            qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status'], whole_row['patient_id']]\n",
    "        row = row + 1\n",
    "    # Add in the last row\n",
    "    whole_row:pd.DataFrame = qlearning_dataset.iloc[len(qlearning_dataset.index) - 1]\n",
    "    qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status'], whole_row['patient_id']]\n",
    "    \n",
    "    row = row + 1\n",
    "    # Get rid of the unneeded rows\n",
    "    qlearning_dataset_mod:pd.DataFrame = pd.DataFrame(qlearning_dataset_mod[0:row, :])\n",
    "    qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', 'death_state', 'patient_id']\n",
    "    # Set all rows not in the terminal states to 0 reward to start\n",
    "    qlearning_dataset_mod.loc[qlearning_dataset_mod['chosen_action_index'] != -1,'reward_value'] = 0\n",
    "    return qlearning_dataset_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Now that we officially have some a valid bloc for actions, and a valid set of states, it's time \n",
    "#  to begin building the transitions matrix.\n",
    "###\n",
    "\n",
    "### If the matrix is bidirectional (S1 -> S2, S2 -> S1 are both valid, we can build two matrices)\n",
    "\n",
    "### \n",
    "# The MDP Toolbox we are going to be using requires Transition and Reward Matrices to be in the form\n",
    "# M(action, state1, state2)\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "# The create_transition_matrix method takes 4 arguments:\n",
    "# num_actions: The total number of possible actions (calculated by action_count ^ 2 or in py, action_count ** 2)\n",
    "# num_states:  Number of states the model uses\n",
    "# qlearning_dataset: The dataset that will be used for the qlearning process\n",
    "# transition_threshold: How many actions do we want to deem as scarce and not worth keeping (default = 5)\n",
    "# reverse: If false, the matrix that is created is represented as transition[A][S1][S2], if true: transition[A][S2][S1]\n",
    "# \n",
    "# and returns 2 values:\n",
    "# transition_matrix: The counts of which actions were chosen in which states\n",
    "# physician_policy:  The transition_matrix that has been turned into probabilties by dividing counts in each state by \n",
    "# total counts\n",
    "# \n",
    "\"\"\"\n",
    "def create_transition_matrix(num_actions:int, num_states:int, ql_data_input:pd.DataFrame, \n",
    "                             transition_threshold:int = 5, reverse:bool = False) -> (int_matrix2D, float_matrix2D):\n",
    "    # The transition matrix is a 3D construct, involving a transition between two states\n",
    "    # and an action. The dimensions for the matrix are (state_count * 2) * (state_count + 2) * action_count\n",
    "    transition_matrix:float_matrix2D = [[[0 for i in range(0, num_states + 2)] for i in range(0, num_states + 2)] for i in range(0, num_actions)]\n",
    "    # NP Arrays allow for more compact and efficient slicing\n",
    "    transition_matrix = np.array(transition_matrix).astype(float)\n",
    "    # We also need a matrix to denote the policy that corresponds with taken a particular action from a state\n",
    "    transition_policy_count:int_matrix2D = [[0 for i in range(0, num_states + 2 )] for i in range(0, num_actions)]\n",
    "    transition_policy_count = np.array(transition_policy_count).astype(float)\n",
    "    # Iterate over the actual data in order to form the actual states and their corresponding actions\n",
    "    # As soon as we hit the next patient (the next row has a training bloc value of 1), we stop processing actions for that patient\n",
    "    for i in range(0, len(ql_data_input) - 1):\n",
    "        # Since 1 is our 'endpoint' for each patient, there are no actions we can take from this point on\n",
    "        if ql_data_input.iloc[i + 1]['training_bloc'] > ql_data_input.iloc[i]['training_bloc']:\n",
    "            S1:int = ql_data_input.iloc[i]['closest_cluster_index']\n",
    "            S2:int = ql_data_input.iloc[i + 1]['closest_cluster_index'] \n",
    "            action_id:int = ql_data_input.iloc[i]['chosen_action_index']\n",
    "            if not(reverse):\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S1][S2] = transition_matrix[action_id][S1][S2] + 1\n",
    "            else:\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S2][S1] = transition_matrix[action_id][S2][S1] + 1\n",
    "                \n",
    "            # Count the number of times action A is used to transition from S1\n",
    "            transition_policy_count[action_id][S1] = transition_policy_count[action_id][S1] + 1        \n",
    "\n",
    "    # In order to avoid drastically altering our model, we fix a constant\n",
    "    # value (set by default to 5), in order to declare sparse actions \n",
    "    # as essentially not happening (make their count 0)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if transition_policy_count[i][j] <= transition_threshold:\n",
    "                transition_policy_count[i][j] = 0 \n",
    "    # Now, we want to prevent transitions from state -> state using\n",
    "    # a certain action if that action is sparse or nonexistant\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if not(reverse):\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,j,:] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,j,:] = transition_matrix[i,j,:]/np.float64(transition_policy_count[i][j])\n",
    "            else:\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,:,j] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,:,j] = transition_matrix[i,:,j]/np.float64(transition_policy_count[i][j])\n",
    "    \n",
    "    # Ensure no divisions create NaNs or infinities\n",
    "    transition_matrix = np.nan_to_num(transition_matrix)\n",
    "    # Determine the phyisican's policy based on total count\n",
    "    # This comes in handy later when comparing model ability\n",
    "    total_transitions:float = sum(transition_policy_count)\n",
    "    physician_policy:float_matrix2D = np.divide(transition_policy_count, total_transitions)\n",
    "    return transition_matrix, physician_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eval_predict_correct checks the current reward value against whether or not\n",
    "the patient's terminal state was predicted correctly and evaluates whether or\n",
    "not the model should be rewarded or penalized\n",
    "\n",
    "Input: \n",
    "reward_value - The total reward value evaluated for a given patient\n",
    "correct_answer - 0 if the patient lived, 1 if the patient died\n",
    "\n",
    "Output:\n",
    "reponse_reward - The total amount of reward that justifies a particular response\n",
    "\"\"\"\n",
    "def eval_predict_correct(reward_value:int, correct_answer:int):\n",
    "    # Values to help clean up readability\n",
    "    PATIENT_LIVED = 0\n",
    "    PATIENT_DIED = 1\n",
    "    PREDICTED_DEAD = reward_value < 0\n",
    "    PREDICTED_LIVING = reward_value >= 0\n",
    "    return_reward = 0\n",
    "    # If the model guessed correctly and the patient died, weight the path -100 \n",
    "    if (PREDICTED_DEAD and correct_answer == PATIENT_DIED):\n",
    "        return_reward = -100\n",
    "    # If the model guessed correctly and the patient lived, weight the path +100\n",
    "    elif (PREDICTED_LIVING and correct_answer == PATIENT_LIVED):\n",
    "        return_reward = 100\n",
    "    # If the model guessed incorrectly and the patient lived, weight the path +100\n",
    "    elif (PREDICTED_DEAD and correct_answer == PATIENT_LIVED):\n",
    "        return_reward = 100\n",
    "    # If the model guessed incorrectly and the patient died, weight the path -100\n",
    "    elif (PREDICTED_LIVING and correct_answer == PATIENT_DIED):\n",
    "        return_reward = -100\n",
    "    # This should not be reachable with a properly prepared dataset\n",
    "    else:\n",
    "        print(\"Your dataset is not prepared properly\")\n",
    "    return return_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_Q_learning_eval is a method that takes 6 arguments and returns 2 items\n",
    "# This method evaluates the performance of the MDP determined previously\n",
    "# \n",
    "# Parameters:\n",
    "# ql_train_set_Q: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# alpha: A hyperparameter that weights our reward function at each step\n",
    "# numtraces: Number of Q-Learning iterations we would like to perform\n",
    "# num_actions: Total number of actions in the set (For Sepsis: 25)\n",
    "# num_clusters: Total number of states in the set (For Sepsis: 752)\n",
    "# is_training:  If False, this is the first phase (construction of the model). If True, the model is being Trained\n",
    "# \n",
    "# Returns:\n",
    "# Q_Equation = The set of Q-Values obtained by the algorithm's performance\n",
    "# sum_Q_values = The Q-Equation's performance at a given step in the algorithm\n",
    "\"\"\"\n",
    "def offpolicy_Q_learning_eval(ql_train_set_Q: pd.DataFrame, gamma: float, alpha: float, \n",
    "                              numtraces: int, num_actions: int, \n",
    "                              num_clusters: int, stopping_difference: float, \n",
    "                              is_random=False,\n",
    "                              modulus_val:int=5000) -> (float_matrix2D, List[float]):\n",
    "    # We need to save the Q-value for each run \n",
    "    sum_Q_values:List[float] = np.zeros((numtraces))\n",
    "    # Where the Q-Values are saved at each given run\n",
    "    Q_Equation:int_matrix2D = np.zeros((num_actions, num_clusters))\n",
    "    # We need to save the Average Q value after so many iterations\n",
    "    previous_avg_Q:int = 0\n",
    "    # Grab the full list of IDs\n",
    "    patient_indexes:List[str] = ql_train_set_Q['patient_id'].drop_duplicates().tolist()\n",
    "    # This is a seperate index used for the Sum of the Q-Values\n",
    "    jj:int = 0\n",
    "    # If we don't want to use a random set of the data points, only do each data point once\n",
    "    if not(is_random):\n",
    "        numtraces = len(patient_indexes)\n",
    "    # We iterate for the total number of times we want to do this process (random) or all the ids\n",
    "    for i in range(0, numtraces):\n",
    "        # Either go in order, or randomly choose a patient\n",
    "        patient_id:int = 0\n",
    "        if is_random:\n",
    "            patient_id = random.choice(patient_indexes)\n",
    "        else:\n",
    "            patient_id = patient_indexes[i]\n",
    "        # As Q-learning progreses, we need a data structure to track progress\n",
    "        full_trace:List[Tuple[float, int, int]] = []\n",
    "        # Stash a total reward value for the whole run\n",
    "        total_reward:int = 0\n",
    "        # Evaluate over a single patient\n",
    "        single_patient = ql_train_set_Q[ql_train_set_Q['patient_id'] == patient_id]\n",
    "        # Iterate over a single patient's data to contribute to the overall Q_Equation\n",
    "        for i in range(0, len(single_patient.index) - 1):\n",
    "            # Grab state (Initial State at this point)\n",
    "            state_index:int = single_patient.iloc[i + 1]['closest_cluster_index']\n",
    "            # Grab action taken from this point\n",
    "            action_index:int = single_patient.iloc[i + 1]['chosen_action_index']\n",
    "            # Grab reward provided by taken an action from this state to the next\n",
    "            reward_value:float = single_patient.iloc[i + 1]['reward_value']\n",
    "            # Add to the total\n",
    "            total_reward = total_reward + reward_value\n",
    "            # A 'step' in the trace, a single data point snapshot\n",
    "            trace_step:Tuple[float, int, int] = (reward_value, state_index, action_index)\n",
    "            # Add the step to the full trace\n",
    "            full_trace.append(trace_step)\n",
    "        # Grab the full length of the trace\n",
    "        trace_length:int = len(full_trace)\n",
    "        # Grab whether the patient lived or died\n",
    "        death_state:int = single_patient.iloc[0]['death_state']\n",
    "        return_reward:float = eval_predict_correct(total_reward, death_state)\n",
    "        # Walk the trace stack backwards to construct the Q-Equation\n",
    "        # The terminal state and the penulitmate state both should not have a reward\n",
    "        for j in range(trace_length - 2, - 1, -1):\n",
    "            # Grab the state, action, and reward at each step\n",
    "            step_state:int = full_trace[j][1]\n",
    "            step_action:int = full_trace[j][2]\n",
    "            # Use alpha blending to blend a portion of the old reward with the new reward\n",
    "            Q_Equation[step_action, step_state] = (1 - alpha) * Q_Equation[step_action, step_state] + alpha * return_reward\n",
    "            # Cap the range for node values (-100, 100)\n",
    "            if Q_Equation[step_action, step_state] > 100:\n",
    "                Q_Equation[step_action, step_state] = 100\n",
    "            if Q_Equation[step_action, step_state] < -100:\n",
    "                Q_Equation[step_action, step_state] = -100\n",
    "            # Recall we have a gamma value to determine the impact of previous decisions on future ones\n",
    "            # Note: this is a Hyperparameter (a parameter on the model itself)\n",
    "            return_reward = return_reward * gamma  + full_trace[j][0]\n",
    "            # Save the overall value based on the current states and actions avaiable at the \n",
    "            # current iteration\n",
    "        sum_Q_values[jj] = np.sum(Q_Equation)\n",
    "        jj = jj + 1\n",
    "        # If we haven't hit our max iterations, we still want to see if we should keep pushing forward\n",
    "        # If there is no noticable progress, we want to stop\n",
    "        # This is only applicable if we are not using all the training data set patients\n",
    "        if is_random:\n",
    "            # Perform a check every modulus_val runs\n",
    "            if i % modulus_val == 0:\n",
    "                # Grab the current slice of unchecked {modulus_val} values\n",
    "                slice_mean:float = np.mean(sum_Q_values[j - modulus_val:j])\n",
    "                # Calculate the difference between current and last average \n",
    "                max_difference:float =(slice_mean - previous_avg_Q)/previous_avg_Q\n",
    "                # Check if the average of this {modulus_val} values is less than 0.001 away from the previous\n",
    "                if abs(max_difference) < stopping_difference:\n",
    "                    break\n",
    "                previous_avg_Q = slice_mean\n",
    "    # Trim off the portion of the list we did not use\n",
    "    sum_Q_values = sum_Q_values[0:jj]\n",
    "    return Q_Equation, sum_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "construct_trained_model takes an existing dataframe and information regarding the Q_Equation\n",
    "already constructing and modifies it with correct Reward Values\n",
    "\n",
    "Input:\n",
    "ql_final_dataset: The dataset that is inputted into the Q-Learning algorithm\n",
    "state_count: The number of states built by the K-Means algorithm\n",
    "total_actions: The number of actions constructed in the action matrix\n",
    "physician_policy: Probability distributions for likelihood of patient transitions from state to state\n",
    "Q_Equation: The reward values generated for transitions from state to state using a given action generated\n",
    "by the Q-Learning algorithm\n",
    "\n",
    "Output: \n",
    "qlearning_train_final: The finalized dataframe with reward values inserted\n",
    "\"\"\"\n",
    "def construct_trained_model(ql_final_dataset:pd.DataFrame, state_count:int, \n",
    "                            total_actions:int, weighted_probabilities:bool, \n",
    "                            physician_policy:List[List[float]], Q_Equation:List[List[float]]) -> pd.DataFrame:\n",
    "    # Set variables for readability\n",
    "    NO_ACTION = -1\n",
    "    # Make a final duplicate of the data\n",
    "    qlearning_train_final:pd.DataFrame = ql_final_dataset.copy()\n",
    "    # If weights are considered, use the weighted rewards from the Q_Equation, otherwise \n",
    "    # use the Q_Equation as is \n",
    "    if weighted_probabilities:\n",
    "        # Weight Q-Value rewards according to their frequency of occuring\n",
    "        # The Q-Equation for a given state + action pair is equivalent to the reward value\n",
    "        # The Phyiscian Policy is the probability of that action ocurring given a state\n",
    "        # These weights prevent rare events from having massively scewed rewards\n",
    "        # For example, a path that occurs through a given state exactly once would have a much larger\n",
    "        # reward than a frequently traveled path, which could scew the data.\n",
    "        value_matrix = np.zeros((state_count, total_actions))\n",
    "        for i in range(0, state_count):\n",
    "            for j in range(0, total_actions):\n",
    "                value_matrix[i][j] = physician_policy[j][i] * Q_Equation[j][i]\n",
    "        for i in range(0, len(qlearning_train_final.index)):\n",
    "            row = qlearning_train_final.iloc[i]\n",
    "            if((row['closest_cluster_index'] != state_count) and (row['closest_cluster_index'] != state_count + 1)):\n",
    "                row['reward_value'] = value_matrix[row['closest_cluster_index']][row['chosen_action_index']]\n",
    "    else:\n",
    "        for i in range(0, len(qlearning_train_final.index) - 1):\n",
    "            # The next state and action pair has a reward value that represents the reward\n",
    "            # for the next transition\n",
    "            curr_row = qlearning_train_final.iloc[i]\n",
    "            next_row = qlearning_train_final.iloc[i + 1]\n",
    "            # If we are not going into the terminal state or at the terminal state, assign reward\n",
    "            if((curr_row['chosen_action_index'] != NO_ACTION) and (next_row['chosen_action_index'] != NO_ACTION)):\n",
    "                curr_row['reward_value'] = Q_Equation[next_row['chosen_action_index']][next_row['closest_cluster_index']]\n",
    "    \n",
    "    return qlearning_train_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# evaluate_model_accuracy calculates summations for all patients and determines if the overall reward value\n",
    "# predicts a patient to live or die \n",
    "# \n",
    "# Input: \n",
    "# qlearning_train_final: The complete dataframe with reward values intact\n",
    "# test_name: The desired title to be printed to the console\n",
    "# is_debug: Whether or not to include print statements\n",
    "# gamma - A hyperparameter for determining how much we value previous data\n",
    "# alpha - A hyperparameter that weights our reward function at each step\n",
    "# run_num - The number of the testing iteration the model is on \n",
    "# \n",
    "# Output:\n",
    "# N/A\n",
    "# \n",
    "\"\"\"\n",
    "def evaluate_model_accuracy(qlearning_train_final:pd.DataFrame, test_name:str, is_debug:bool, \n",
    "                            alpha:float, gamma:float, run_num:int):\n",
    "    all_patients_ids = qlearning_train_final['patient_id'].drop_duplicates().tolist()\n",
    "    # Iterate until the full set is done\n",
    "    total_patients:int = len(all_patients_ids)\n",
    "    unique_total:pd.DataFrame = qlearning_train_final[['patient_id', 'death_state']].drop_duplicates()\n",
    "    total_alive:int = unique_total['death_state'].value_counts()[0]\n",
    "    total_dead:int = unique_total['death_state'].value_counts()[1]\n",
    "    correct_guesses:int = 0\n",
    "    dead_instead_live:int = 0\n",
    "    live_instead_dead:int = 0\n",
    "    zero_valued_reward:int = 0\n",
    "    for id_val in all_patients_ids:\n",
    "        all_rewards:pd.Series = qlearning_train_final[qlearning_train_final['patient_id'] == id_val]['reward_value']\n",
    "        # We are not supposed to know the terminal reward for evaluation, this is the life/death state\n",
    "        all_rewards:pd.Series = all_rewards[:-1]\n",
    "        # Sum the total reward value for a given column, determine the sign value\n",
    "        reward_sum:float = np.sign(np.sum(all_rewards))\n",
    "        # Treat 0s as Living\n",
    "        if reward_sum == 0:\n",
    "            zero_valued_reward = zero_valued_reward + 1\n",
    "            reward_sum = 1\n",
    "        # Grab the death state of the patient\n",
    "        death_state = qlearning_train_final[qlearning_train_final['patient_id'] == id_val]['death_state'].tolist()[0]\n",
    "        # Change Values to match reward system 0 -> +1 -> Patient Lived, 1 -> -1 -> Patient Died\n",
    "        PATIENT_LIVED = 0\n",
    "        PATIENT_DIED = 1\n",
    "        if death_state == PATIENT_LIVED:\n",
    "            death_state = 1\n",
    "        else:\n",
    "            death_state = -1\n",
    "        # If the prediction and actual values matched, the model predicted correctly\n",
    "        if reward_sum == death_state:\n",
    "            correct_guesses = correct_guesses + 1\n",
    "        # If the patient was presumed to be dead and lived, count it\n",
    "        elif reward_sum == -1:\n",
    "            live_instead_dead = live_instead_dead + 1\n",
    "        # If the patient was presumed to be alive and died, count it\n",
    "        else:\n",
    "            dead_instead_live = dead_instead_live + 1\n",
    "    # Grab all the calculated values\n",
    "    overall_accuracy:float = correct_guesses/total_patients\n",
    "    dead_accuracy:float = (total_dead - dead_instead_live) / total_dead\n",
    "    live_accuracy:float = (total_alive - live_instead_dead) / total_alive\n",
    "    if is_debug:\n",
    "        print(\"Test Name: \" + test_name)\n",
    "        print(\"Overall Accuracy: \" + str(overall_accuracy))\n",
    "        print(\"Accuracy for Dead: \" + str(dead_accuracy))\n",
    "        print(\"Accuracy for Living: \" + str(live_accuracy))\n",
    "        print(\"Living People Guessed Dead: \" + str(live_instead_dead))\n",
    "        print(\"Dead People Guessed Living: \" + str(dead_instead_live))\n",
    "        print(\"Total Guesses: \" + str(total_patients))\n",
    "        print(\"Correct Guesses: \" + str(correct_guesses))\n",
    "        print(\"Alive People: \" + str(total_alive))\n",
    "        print(\"Dead People: \" + str(total_dead))\n",
    "        print(\"Empty Paths: \" + str(zero_valued_reward))\n",
    "        print(\"\\n\")\n",
    "    # Construct a formatted CSV string with all the values that have been set\n",
    "    total_string = (f'{run_num},{alpha},{gamma},{total_patients},{correct_guesses},'\n",
    "                    f'{overall_accuracy},{dead_accuracy}'\n",
    "                    f'{live_accuracy},{live_instead_dead},{dead_instead_live},'\n",
    "                    f'{total_alive},{total_dead},{zero_valued_reward},'\n",
    "                    f'{test_name}')\n",
    "    # Check the test name and write to the corresponding file\n",
    "    if test_name == 'Test_Weighted':\n",
    "        with open('test_weighted_runs.csv', 'a') as f:\n",
    "            f.write(total_string + \"\\n\")\n",
    "    elif test_name == 'Test_No_Weighting':\n",
    "        with open('test_no_weighting_runs.csv', 'a') as f:\n",
    "            f.write(total_string + \"\\n\")\n",
    "    elif test_name == 'Train_Weighted':\n",
    "        with open('train_weighted_runs.csv', 'a') as f:\n",
    "            f.write(total_string + \"\\n\")\n",
    "    else:\n",
    "        with open('train_no_weighting_runs.csv', 'a') as f:\n",
    "            f.write(total_string + \"\\n\")\n",
    "    # Return the final formatted string if desired\n",
    "    return total_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PCA (Principal Component Analysis) is a technique to perform dimensionality reduction in large featured datasets (I.E) MIMIC\n",
    "# It turns a series of features into multi-component lists, each possessing a certain variance amount \n",
    "# based on the inputted dataset\n",
    "# \n",
    "# Input:\n",
    "# MIMIC_zscores - The MIMIC dataset zscores in the form of a dataframe\n",
    "# variance_allowed - The decimal representation of how much of the original variance desired to be intact\n",
    "# a value of 0 will have a completely unique dataset, 1 will be the original dataset in components\n",
    "# is_debug: Enables print statements\n",
    "# num_components: For the initial run, this should be set to -1, the following runs should be set to the output\n",
    "# of this function. This value is the number of components that are presented in each list PCA generates\n",
    "#\n",
    "# Output: \n",
    "# pca_dataset: The PCA generated dataset\n",
    "# num_components: The optimal number of components selected by PCA with the specified variance_allowed\n",
    "# \n",
    "\"\"\"\n",
    "def pca_variance_analysis(MIMIC_zscores:pd.DataFrame, variance_allowed:float, is_debug:bool, num_components:int) -> float_matrix2D:\n",
    "    if num_components == -1:\n",
    "        model = PCA()\n",
    "        model.fit(MIMIC_zscores.values)\n",
    "        total_variances = np.cumsum(model.explained_variance_ratio_)\n",
    "        num_components = 0\n",
    "        for variance in total_variances:\n",
    "            if variance < variance_allowed:\n",
    "                num_components = num_components + 1\n",
    "        if is_debug: \n",
    "            print(\"Number of Components Chosen: \", num_components)\n",
    "    real_model = PCA(n_components=num_components)\n",
    "    pca_dataset = real_model.fit_transform(MIMIC_zscores.values)\n",
    "    return pca_dataset, num_components\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# aggregate_dataset_construct is a function that groups all the functions to build the initial dataset for training\n",
    "#\n",
    "# Input: \n",
    "# debug_flag - Whether or not print statements are included\n",
    "# save_to_file - Whether or not intermediate steps are saved to file\n",
    "# regenerate_flag - Whether to run clustering, or read from file\n",
    "# regenerate_cross - Whether or not to run cross_validation split or read from file\n",
    "#\n",
    "# Output:\n",
    "# train_flag_set - All the rows marked to be used for training data\n",
    "# test_flag_set - All the rows mared to be used for testing\n",
    "# MIMIC_zscores - The MIMIC data reduced to Z-Scores\n",
    "\"\"\"\n",
    "def aggregate_dataset_construct(debug_flag:bool = False, save_to_file:bool = True, \n",
    "                                regenerate_flag:bool = False, regenerate_cross:bool=False):\n",
    "    # Extract the initial columns and build the first dataset from the CSV file\n",
    "    colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids = extract_init_column_data(patient_data=patientdata, debug_flag=False)\n",
    "    # Construct the Z-Scores version of the dataframe\n",
    "    MIMIC_zscores:pd.DataFrame = construct_zscores(colbin, colnorm, collog, MIMIC_raw, debug_flag=False) \n",
    "    # Allow for dynamic construction of chosen factors\n",
    "    all_factors = [colbin, colnorm, collog]\n",
    "    # Trick to Flatten nested list\n",
    "    all_factors = [item for sublist in all_factors for item in sublist]\n",
    "    # Stratify the train and test sets\n",
    "    train_ids_set, test_ids_set, train_flag_set, test_flag_set = cross_validate_and_balance(icu_ids, debug_flag, \n",
    "                                                                             save_to_file, regenerate_flag, \n",
    "                                                                             patientdata, regenerate_cross,\n",
    "                                                                             all_factors)\n",
    "    # The flags are all we need for the subsequent steps\n",
    "    return train_flag_set, test_flag_set, MIMIC_zscores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# aggregate_training_clustering is a function that groups all the functions to cluster and build the training dataset\n",
    "#\n",
    "# Input: \n",
    "# train_flag - All the rows marked to be used for training\n",
    "# test_flag - All the rows marked to be used for testing\n",
    "# MIMIC_zscores - The Z-Scores of the standard patientdata set\n",
    "# is_debug - Whether or not to use print statements\n",
    "# save_flag - Whether or not intermediate steps are saved to files\n",
    "# variance_allowed - Tolerance used for PCA if it is enabled (Principal Component Analysis)\n",
    "# pca_flag - Whether or not to enable PCA\n",
    "# graph_flag - Whether or not the graph of actions should be displayed\n",
    "# load_zscores - Whether or not zscores should be loaded from file\n",
    "# regenerate_flag - Whether or not to rerun clustering \n",
    "# \n",
    "# Output:\n",
    "# qlearning_dataset_train_final - The final, modified, clustered dataset for training \n",
    "# qlearning_dataset_test_final - The final, modified, clustered dataset for testing\n",
    "# transition_mat - The counts of transitioning from one state to another using a given action\n",
    "# physician_policy - The probabilities of transitioning from one state to another using a given action\n",
    "# total_actions - Total number of actions to be used for training\n",
    "# state_count - Total number of states to be used for training\n",
    "# \n",
    "\"\"\"\n",
    "def aggregate_training_clustering(train_flag:pd.DataFrame, test_flag:pd.DataFrame, MIMIC_zscores:pd.DataFrame,\n",
    "                                  is_debug:bool=False, save_flag:bool=True, variance_allowed=0.8, \n",
    "                                  pca_flag:bool = False, graph_flag:bool = False, \n",
    "                                  load_zscores:bool=False, regenerate_flag=False):\n",
    "    # Construct the training and testing variants of the Z-Scores\n",
    "    train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, test_id_list, train_90d, test_90d = \\\n",
    "    zscores_for_train_and_test(train_flag, test_flag, MIMIC_zscores, debug_flag=is_debug, save_flag=save_flag, \n",
    "                               patient_data=patientdata, bloc_name='bloc', id_name='icustayid', death_name='mortality_90d')\n",
    "    # If it desired to perform Principal Component Analysis (PCA) instead of using raw Z-Scores\n",
    "    if pca_flag:\n",
    "        train_zscores, num_train_comp = pca_variance_analysis(MIMIC_zscores=train_zscores, \n",
    "                                                       variance_allowed=variance_allowed, is_debug=is_debug, num_components=-1)\n",
    "        test_zscores, _ = pca_variance_analysis(MIMIC_zscores=test_zscores, variance_allowed=variance_allowed, \n",
    "                                                 is_debug=is_debug, num_components=num_train_comp)\n",
    "    # Optimize or load in the optimal number of clusters (for this dataset, that cluster count happens to be 42)\n",
    "    optimal_cluster_count = calculate_optimal_clusters_driver(data_set=train_zscores, run_optimal_clusters=False, \n",
    "                                                              run_multithread=False, show_graph=graph_flag, \n",
    "                                                              thread_count=-1, max_state_count=15)\n",
    "    # Hard override for testing\n",
    "    optimal_cluster_count = 10\n",
    "    # State count is the optimal cluster count \n",
    "    state_count = optimal_cluster_count\n",
    "    # Perform K-Means clustering with the optimal number of clusters\n",
    "    cluster_values, cluster_labels, train_zscores, closest_clusters, closest_clusters_test = \\\n",
    "    kmeans_cluster_calculations(train_zscores=train_zscores, test_zscores=test_zscores,\n",
    "                                max_num_iter=32, state_count=optimal_cluster_count, num_loops_per_iter=10000,\n",
    "                                debug_flag=is_debug, regenerate_flag=regenerate_flag, load_zscores=load_zscores)\n",
    "    # Training and Testing Construction based on cluster outputs\n",
    "    train_chosen_actions, test_chosen_actions, action_values_matrix, action_list, action_count = build_dataset_actions(patientdata=patientdata, \n",
    "                      first_column=\"SOFA\", second_column=\"qSOFA\", \n",
    "                      num_groups_first_column=5, num_groups_second_column=4, \n",
    "                      debug_flag=is_debug, graph_flag=graph_flag, train_flag=train_flag, test_flag=test_flag)\n",
    "    # Construct the State->Action Matrix for the training set\n",
    "    qlearning_dataset_train_final, _, _ = construct_prestate_matrix_train(train_90d = train_90d, \n",
    "                                                                                            train_blocs = train_blocs, \n",
    "                                                                                            closest_clusters = closest_clusters,\n",
    "                                                                                            train_chosen_actions = train_chosen_actions, \n",
    "                                                                                            train_id_list = train_id_list,\n",
    "                                                                                            is_debug=is_debug,\n",
    "                                                                                            action_count=action_count, \n",
    "                                                                                            state_count=state_count)\n",
    "    qlearning_dataset_test_final, _, _ = construct_prestate_matrix_train(train_90d = test_90d, \n",
    "                                                                     train_blocs = test_blocs, \n",
    "                                                                     closest_clusters = closest_clusters_test,\n",
    "                                                                     train_chosen_actions = test_chosen_actions,\n",
    "                                                                     train_id_list = test_id_list,\n",
    "                                                                     is_debug=False,\n",
    "                                                                     action_count=action_count, \n",
    "                                                                     state_count=state_count)\n",
    "    # Constructing Transition Matrix(A, State1, State2)\n",
    "    total_actions:int = len(action_values_matrix)     \n",
    "    # Execute the function call\n",
    "    transition_mat, physician_policy = create_transition_matrix(num_actions = total_actions, \n",
    "                                                                num_states = state_count,ql_data_input = \n",
    "                                                                qlearning_dataset_train_final, \n",
    "                                                                transition_threshold = 5, \n",
    "                                                                reverse = False)\n",
    "    return qlearning_dataset_train_final, qlearning_dataset_test_final, transition_mat,\\\n",
    "           physician_policy, total_actions, state_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# aggregate_Q_learning_model_construction is a function that groups all the functions to perform Q-Learning\n",
    "#\n",
    "# Input: \n",
    "# qlearning_dataset_train_final - The final, modified, clustered dataset for training \n",
    "# qlearning_dataset_test_final - The final, modified, clustered dataset for testing\n",
    "# gamma - A hyperparameter for determining how much we value previous data\n",
    "# alpha - A hyperparameter that weights our reward function at each step\n",
    "# state_count - Total number of states to be used for training\n",
    "# total_actions - Total number of actions to be used for training\n",
    "# physician_policy - The probabilities of transitioning from one state to another using a given action\n",
    "#\n",
    "# Output:\n",
    "# qlearning_dataset_train_final_weighted - Q-Learning final model output for the weighted training variant\n",
    "# qlearning_dataset_train_final_no_weighting - Q-Learning final model output for the unweighted training variant\n",
    "# qlearning_dataset_test_final_weighted - Q-Learning final model output for the weighted test set \n",
    "# qlearning_dataset_test_final_no_weighting - Q-Learning final model output for the unweighted test set\n",
    "# \n",
    "\"\"\"\n",
    "def aggregate_Q_learning_model_construction(qlearning_dataset_train_final:pd.DataFrame, \n",
    "                                            qlearning_dataset_test_final:pd.DataFrame,\n",
    "                                            gamma:float, alpha:float, state_count:int, \n",
    "                                            total_actions:int, physician_policy:pd.DataFrame):\n",
    "    # Two rounds of Q-Learning, one to set initial values, and then one to tweak those\n",
    "    Q_Equation, sum_Q_values = offpolicy_Q_learning_eval(\n",
    "            ql_train_set_Q=qlearning_dataset_train_final,\n",
    "            gamma=gamma, \n",
    "            alpha=alpha,\n",
    "            numtraces=30000,\n",
    "            num_actions=total_actions,\n",
    "            num_clusters=state_count,\n",
    "            stopping_difference=0.001,\n",
    "            is_random=False,\n",
    "            modulus_val=5000\n",
    "    )\n",
    "    \"\"\"\n",
    "    # First Round Model\n",
    "    qlearning_train_final = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy,\n",
    "                                                                Q_Equation=Q_Equation)\n",
    "    \n",
    "    Q_Equation, sum_Q_values = offpolicy_Q_learning_eval(\n",
    "            ql_train_set_Q=qlearning_train_final,\n",
    "            gamma=gamma, \n",
    "            alpha=alpha,\n",
    "            numtraces=30000,\n",
    "            num_actions=total_actions,\n",
    "            num_clusters=state_count,\n",
    "            stopping_difference=0.001,\n",
    "            is_random=False,\n",
    "            modulus_val=5000\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Second Round Model\n",
    "    qlearning_dataset_train_final_weighted = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy,\n",
    "                                                                             Q_Equation=Q_Equation)\n",
    "    qlearning_dataset_train_final_no_weighting = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy,\n",
    "                                                                              Q_Equation=Q_Equation)\n",
    "    qlearning_dataset_test_final_weighted = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy,\n",
    "                                                                             Q_Equation=Q_Equation)\n",
    "    qlearning_dataset_test_final_no_weighting = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy,\n",
    "                                                                              Q_Equation=Q_Equation)\n",
    "    return qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\\\n",
    "           qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# aggregate_Q_learning_evaluation is a function that evaluates all the model output and save it to an appended file\n",
    "#\n",
    "# Input: \n",
    "# qlearning_dataset_train_final_weighted - Q-Learning final model output for the weighted training variant\n",
    "# qlearning_dataset_train_final_no_weighting - Q-Learning final model output for the unweighted training variant\n",
    "# qlearning_dataset_test_final_weighted - Q-Learning final model output for the weighted test set \n",
    "# qlearning_dataset_test_final_no_weighting - Q-Learning final model output for the unweighted test set\n",
    "#\n",
    "# Output:\n",
    "# N/A\n",
    "# \n",
    "\"\"\"\n",
    "def aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted:pd.DataFrame, qlearning_dataset_train_final_no_weighting:pd.DataFrame,\n",
    "                                    qlearning_dataset_test_final_weighted:pd.DataFrame, qlearning_dataset_test_final_no_weighting:pd.DataFrame,\n",
    "                                    gamma:float, alpha:float, run_num:int):\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_weighted, test_name=\"Test_Weighted\", is_debug=True, alpha=alpha, gamma=gamma, run_num=run_num)\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_no_weighting, test_name=\"Test_No_Weighting\", is_debug=True, alpha=alpha, gamma=gamma, run_num=run_num)\n",
    "    evaluate_model_accuracy(qlearning_dataset_train_final_weighted, test_name=\"Train_Weighted\", is_debug=True, alpha=alpha, gamma=gamma, run_num=run_num)\n",
    "    evaluate_model_accuracy(qlearning_dataset_train_final_no_weighting, test_name=\"Train_No_Weighting\", is_debug=True, alpha=alpha, gamma=gamma, run_num=run_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_fast._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 2, in where\n",
      "KeyboardInterrupt\n",
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.7536231884057971\n",
      "Accuracy for Dead: 0.03070175438596491\n",
      "Accuracy for Living: 0.9769647696476965\n",
      "Living People Guessed Dead: 34\n",
      "Dead People Guessed Living: 442\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1456\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 161\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7753623188405797\n",
      "Accuracy for Dead: 0.1162280701754386\n",
      "Accuracy for Living: 0.9789972899728997\n",
      "Living People Guessed Dead: 31\n",
      "Dead People Guessed Living: 403\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1498\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 42\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.7554072710538426\n",
      "Accuracy for Dead: 0.03928745729624207\n",
      "Accuracy for Living: 0.976290832455216\n",
      "Living People Guessed Dead: 315\n",
      "Dead People Guessed Living: 3937\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13132\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 1483\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7662793373216751\n",
      "Accuracy for Dead: 0.09077598828696926\n",
      "Accuracy for Living: 0.9746349540870088\n",
      "Living People Guessed Dead: 337\n",
      "Dead People Guessed Living: 3726\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13321\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 357\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.7127329192546584\n",
      "Accuracy for Dead: 0.07675438596491228\n",
      "Accuracy for Living: 0.9092140921409214\n",
      "Living People Guessed Dead: 134\n",
      "Dead People Guessed Living: 421\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1377\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 178\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7546583850931677\n",
      "Accuracy for Dead: 0.14035087719298245\n",
      "Accuracy for Living: 0.9444444444444444\n",
      "Living People Guessed Dead: 82\n",
      "Dead People Guessed Living: 392\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1458\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 50\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.7128969167050161\n",
      "Accuracy for Dead: 0.0956564177647633\n",
      "Accuracy for Living: 0.9032816498569923\n",
      "Living People Guessed Dead: 1285\n",
      "Dead People Guessed Living: 3706\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12393\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 1695\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7504601932811781\n",
      "Accuracy for Dead: 0.12323084431429966\n",
      "Accuracy for Living: 0.943925937076622\n",
      "Living People Guessed Dead: 745\n",
      "Dead People Guessed Living: 3593\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13046\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 414\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.6868530020703933\n",
      "Accuracy for Dead: 0.11842105263157894\n",
      "Accuracy for Living: 0.8624661246612466\n",
      "Living People Guessed Dead: 203\n",
      "Dead People Guessed Living: 402\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1327\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 271\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7339544513457557\n",
      "Accuracy for Dead: 0.13815789473684212\n",
      "Accuracy for Living: 0.9180216802168022\n",
      "Living People Guessed Dead: 121\n",
      "Dead People Guessed Living: 393\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1418\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 82\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.6870685687988956\n",
      "Accuracy for Dead: 0.1264031234748658\n",
      "Accuracy for Living: 0.8600030106879422\n",
      "Living People Guessed Dead: 1860\n",
      "Dead People Guessed Living: 3580\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 11944\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 2559\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7332029452369996\n",
      "Accuracy for Dead: 0.13299170326988774\n",
      "Accuracy for Living: 0.9183350895679663\n",
      "Living People Guessed Dead: 1085\n",
      "Dead People Guessed Living: 3553\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12746\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 739\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.677536231884058\n",
      "Accuracy for Dead: 0.1337719298245614\n",
      "Accuracy for Living: 0.8455284552845529\n",
      "Living People Guessed Dead: 228\n",
      "Dead People Guessed Living: 395\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1309\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 454\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.717391304347826\n",
      "Accuracy for Dead: 0.1513157894736842\n",
      "Accuracy for Living: 0.8922764227642277\n",
      "Living People Guessed Dead: 159\n",
      "Dead People Guessed Living: 387\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1386\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 98\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.6790151863782788\n",
      "Accuracy for Dead: 0.14470473401659345\n",
      "Accuracy for Living: 0.8438205629986452\n",
      "Living People Guessed Dead: 2075\n",
      "Dead People Guessed Living: 3505\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 11804\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 4141\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7223884031293143\n",
      "Accuracy for Dead: 0.14690092728160078\n",
      "Accuracy for Living: 0.8998946259220232\n",
      "Living People Guessed Dead: 1330\n",
      "Dead People Guessed Living: 3496\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12558\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 845\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.6713250517598344\n",
      "Accuracy for Dead: 0.1513157894736842\n",
      "Accuracy for Living: 0.8319783197831978\n",
      "Living People Guessed Dead: 248\n",
      "Dead People Guessed Living: 387\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1297\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 452\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7091097308488613\n",
      "Accuracy for Dead: 0.15789473684210525\n",
      "Accuracy for Living: 0.8794037940379403\n",
      "Living People Guessed Dead: 178\n",
      "Dead People Guessed Living: 384\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1370\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 165\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.6749884951679705\n",
      "Accuracy for Dead: 0.15714982918496828\n",
      "Accuracy for Living: 0.834713231973506\n",
      "Living People Guessed Dead: 2196\n",
      "Dead People Guessed Living: 3454\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 11734\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 4144\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7128393925448688\n",
      "Accuracy for Dead: 0.14543679843826257\n",
      "Accuracy for Living: 0.8878518741532441\n",
      "Living People Guessed Dead: 1490\n",
      "Dead People Guessed Living: 3502\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12392\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 1480\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.6677018633540373\n",
      "Accuracy for Dead: 0.1600877192982456\n",
      "Accuracy for Living: 0.8245257452574526\n",
      "Living People Guessed Dead: 259\n",
      "Dead People Guessed Living: 383\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1290\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 600\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.6987577639751553\n",
      "Accuracy for Dead: 0.16228070175438597\n",
      "Accuracy for Living: 0.8644986449864499\n",
      "Living People Guessed Dead: 200\n",
      "Dead People Guessed Living: 382\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1350\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 231\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.6723423838011965\n",
      "Accuracy for Dead: 0.16495851634943876\n",
      "Accuracy for Living: 0.8288423904862261\n",
      "Living People Guessed Dead: 2274\n",
      "Dead People Guessed Living: 3422\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 11688\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 5330\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7032903819604234\n",
      "Accuracy for Dead: 0.15202537823328452\n",
      "Accuracy for Living: 0.8733253048321541\n",
      "Living People Guessed Dead: 1683\n",
      "Dead People Guessed Living: 3475\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12226\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 1966\n",
      "\n",
      "\n",
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.6464803312629399\n",
      "Accuracy for Dead: 0.17763157894736842\n",
      "Accuracy for Living: 0.7913279132791328\n",
      "Living People Guessed Dead: 308\n",
      "Dead People Guessed Living: 375\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1249\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 698\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.6909937888198758\n",
      "Accuracy for Dead: 0.17105263157894737\n",
      "Accuracy for Living: 0.8516260162601627\n",
      "Living People Guessed Dead: 219\n",
      "Dead People Guessed Living: 378\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1335\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 244\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.6622756557754257\n",
      "Accuracy for Dead: 0.1898487066861884\n",
      "Accuracy for Living: 0.8079933764865271\n",
      "Living People Guessed Dead: 2551\n",
      "Dead People Guessed Living: 3320\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 11513\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 6272\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.6985158766682007\n",
      "Accuracy for Dead: 0.16447047340165935\n",
      "Accuracy for Living: 0.8632395002258016\n",
      "Living People Guessed Dead: 1817\n",
      "Dead People Guessed Living: 3424\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 12143\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 2087\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-223-995cecdbbc36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                                     \u001b[0mqlearning_dataset_test_final\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                                     \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                                                     total_actions=total_actions, physician_policy=physician_policy)\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# Evaluate the functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\n",
      "\u001b[1;32m<ipython-input-211-9e2205c0e935>\u001b[0m in \u001b[0;36maggregate_Q_learning_model_construction\u001b[1;34m(qlearning_dataset_train_final, qlearning_dataset_test_final, gamma, alpha, state_count, total_actions, physician_policy)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mstopping_difference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mis_random\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mmodulus_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# First Round Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-222-bad67ec840b4>\u001b[0m in \u001b[0;36moffpolicy_Q_learning_eval\u001b[1;34m(ql_train_set_Q, gamma, alpha, numtraces, num_actions, num_clusters, stopping_difference, is_random, modulus_val)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Evaluate over a single patient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msingle_patient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mql_train_set_Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mql_train_set_Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'patient_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpatient_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Iterate over a single patient's data to contribute to the overall Q_Equation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_patient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is the run loop to execute and save the results to the file for the Q-Learning Weighted and Unweighted Model\n",
    "if __name__ == \"__main__\":\n",
    "    train_flag_set, test_flag_set, MIMIC_zscores = aggregate_dataset_construct()\n",
    "    all_hyperparameters = calculate_all_hyperparameter_combos()\n",
    "    for i in range(0, len(train_flag_set)):\n",
    "        train_flag = train_flag_set[i]\n",
    "        test_flag = test_flag_set[i]\n",
    "        # Cluster to construct modified dataset\n",
    "        qlearning_dataset_train_final, qlearning_dataset_test_final, transition_mat, \\\n",
    "        physician_policy, total_actions, state_count = \\\n",
    "                aggregate_training_clustering(train_flag, test_flag, MIMIC_zscores)  \n",
    "        for j in range(0, len(all_hyperparameters)):\n",
    "            gamma = all_hyperparameters[j][0]\n",
    "            alpha = all_hyperparameters[j][1]\n",
    "            # Perform Q-Learning to build finalized dataset with rewards\n",
    "            qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting, \\\n",
    "            qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting = \\\n",
    "            aggregate_Q_learning_model_construction(qlearning_dataset_train_final, \n",
    "                                                    qlearning_dataset_test_final,\n",
    "                                                    gamma=gamma, alpha=alpha, state_count=state_count, \n",
    "                                                    total_actions=total_actions, physician_policy=physician_policy)\n",
    "            # Evaluate the functions\n",
    "            aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\n",
    "                                            qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting,\n",
    "                                            gamma=gamma, alpha=alpha, run_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs all tests over a single set of hyperparameters\n",
    "if __name__ == \"__main__\":\n",
    "    train_flag_set, test_flag_set, MIMIC_zscores = aggregate_dataset_construct()\n",
    "    for i in range(0, len(train_flag_set)):\n",
    "        train_flag = train_flag_set[i]\n",
    "        test_flag = test_flag_set[i]\n",
    "        # Cluster to construct modified dataset\n",
    "        qlearning_dataset_train_final, qlearning_dataset_test_final, transition_mat, \\\n",
    "        physician_policy, total_actions, state_count = \\\n",
    "                aggregate_training_clustering(train_flag, test_flag, MIMIC_zscores)\n",
    "        # Determined optimal hypers\n",
    "        gamma = 0.84\n",
    "        alpha = 0.1\n",
    "        # Perform Q-Learning to build finalized dataset with rewards\n",
    "        qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting, \\\n",
    "        qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting = \\\n",
    "        aggregate_Q_learning_model_construction(qlearning_dataset_train_final, \n",
    "                                                qlearning_dataset_test_final,\n",
    "                                                gamma=gamma, alpha=alpha, state_count=state_count, \n",
    "                                                total_actions=total_actions, physician_policy=physician_policy)\n",
    "        # Evaluate the functions\n",
    "        aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\n",
    "                                        qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting,\n",
    "                                        gamma=gamma, alpha=alpha, run_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.7639751552795031\n",
      "Accuracy for Dead: 0.0\n",
      "Accuracy for Living: 1.0\n",
      "Living People Guessed Dead: 0\n",
      "Dead People Guessed Living: 456\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1476\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 12\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7738095238095238\n",
      "Accuracy for Dead: 0.046052631578947366\n",
      "Accuracy for Living: 0.9986449864498645\n",
      "Living People Guessed Dead: 2\n",
      "Dead People Guessed Living: 435\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1495\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 33\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.764265991716521\n",
      "Accuracy for Dead: 0.0\n",
      "Accuracy for Living: 1.0\n",
      "Living People Guessed Dead: 0\n",
      "Dead People Guessed Living: 4098\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13286\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 104\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.7713989875747814\n",
      "Accuracy for Dead: 0.037579306979014154\n",
      "Accuracy for Living: 0.9977419840433539\n",
      "Living People Guessed Dead: 30\n",
      "Dead People Guessed Living: 3944\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13410\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 291\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs all tests over a single set of hyperparameters\n",
    "if __name__ == \"__main__\":\n",
    "    train_flag_set, test_flag_set, MIMIC_zscores = aggregate_dataset_construct()\n",
    "    train_flag = train_flag_set[0]\n",
    "    test_flag = test_flag_set[0]\n",
    "    # Cluster to construct modified dataset\n",
    "    qlearning_dataset_train_final, qlearning_dataset_test_final, transition_mat, \\\n",
    "    physician_policy, total_actions, state_count = \\\n",
    "            aggregate_training_clustering(train_flag, test_flag, MIMIC_zscores)\n",
    "    # Determined optimal hypers\n",
    "    gamma = 0.84\n",
    "    alpha = 0.1\n",
    "    # Perform Q-Learning to build finalized dataset with rewards\n",
    "    qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting, \\\n",
    "    qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting = \\\n",
    "    aggregate_Q_learning_model_construction(qlearning_dataset_train_final, \n",
    "                                            qlearning_dataset_test_final,\n",
    "                                            gamma=gamma, alpha=alpha, state_count=state_count, \n",
    "                                            total_actions=total_actions, physician_policy=physician_policy)\n",
    "    # Evaluate the functions\n",
    "    aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\n",
    "                                        qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting,\n",
    "                                        gamma=gamma, alpha=alpha, run_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Test_Weighted\n",
      "Overall Accuracy: 0.7644927536231884\n",
      "Accuracy for Dead: 0.0043859649122807015\n",
      "Accuracy for Living: 0.9993224932249323\n",
      "Living People Guessed Dead: 1\n",
      "Dead People Guessed Living: 454\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1477\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 19\n",
      "\n",
      "\n",
      "Test Name: Test_No_Weighting\n",
      "Overall Accuracy: 0.7815734989648033\n",
      "Accuracy for Dead: 0.08333333333333333\n",
      "Accuracy for Living: 0.997289972899729\n",
      "Living People Guessed Dead: 4\n",
      "Dead People Guessed Living: 418\n",
      "Total Guesses: 1932\n",
      "Correct Guesses: 1510\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Empty Paths: 35\n",
      "\n",
      "\n",
      "Test Name: Train_Weighted\n",
      "Overall Accuracy: 0.7647261849976991\n",
      "Accuracy for Dead: 0.00317227916056613\n",
      "Accuracy for Living: 0.9996236640072257\n",
      "Living People Guessed Dead: 5\n",
      "Dead People Guessed Living: 4085\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13294\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 117\n",
      "\n",
      "\n",
      "Test Name: Train_No_Weighting\n",
      "Overall Accuracy: 0.774390243902439\n",
      "Accuracy for Dead: 0.059785261102977064\n",
      "Accuracy for Living: 0.994806563299714\n",
      "Living People Guessed Dead: 69\n",
      "Dead People Guessed Living: 3853\n",
      "Total Guesses: 17384\n",
      "Correct Guesses: 13462\n",
      "Alive People: 13286\n",
      "Dead People: 4098\n",
      "Empty Paths: 293\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Perform Q-Learning to build finalized dataset with rewards\n",
    "qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting, \\\n",
    "qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting = \\\n",
    "aggregate_Q_learning_model_construction(qlearning_dataset_train_final, \n",
    "                                        qlearning_dataset_test_final,\n",
    "                                        gamma=gamma, alpha=alpha, state_count=state_count, \n",
    "                                        total_actions=total_actions, physician_policy=physician_policy)\n",
    "# Evaluate the functions\n",
    "aggregate_Q_learning_evaluation(qlearning_dataset_train_final_weighted, qlearning_dataset_train_final_no_weighting,\n",
    "                                        qlearning_dataset_test_final_weighted, qlearning_dataset_test_final_no_weighting,\n",
    "                                        gamma=gamma, alpha=alpha, run_num=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
