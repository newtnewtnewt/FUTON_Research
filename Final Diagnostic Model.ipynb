{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  FUTON Model MDP + Q-Learning Creation Script\n",
    "#  A Research Project conducted by Noah Dunn \n",
    "###\n",
    "\n",
    "# Import the standard tools for working with Pandas dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import ctypes\n",
    "import csv\n",
    "# Pickle provides easy Object Serialization for quick read + writes of data\n",
    "import pickle\n",
    "# Vector Quantization for Determining Cluster Centers\n",
    "from scipy.cluster.vq import vq\n",
    "# Skikit offers a solution to perform K-Means++ clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# Scipy provides a library to execute Z-Score Normalization\n",
    "from scipy.stats import zscore\n",
    "# We want to do type hinting for API clarification\n",
    "from typing import *\n",
    "# Import the MDP toolbox that contains a method for conducting Q-Learning\n",
    "# Tool can be found here: https://github.com/sawcordwell/pymdptoolbox\n",
    "# Documentation for the tool can be found here \n",
    "import mdptoolbox\n",
    "# Itertools provides an easy way to perform Cartesian product on multiple sets\n",
    "from itertools import product as cartesian_prod\n",
    "import multiprocessing\n",
    "# We need to perform 10 fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "### Some repetitive type hinting\n",
    "int_matrix2D = np.array\n",
    "float_matrix2D = np.array\n",
    "int_matrix3D = np.array\n",
    "float_matrix3D = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Data File that will be used to conduct the experiments\n",
    "patientdata:pd.DataFrame = pd.read_csv(\"G:/MIMIC-ALL/MIMIC-PATIENTS/patient_data_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#  An MDP, or Markov Decision Process is used to model relationships between various states and actions.\n",
    "#  A state can be thought of in medical solution as a patient's diagnosis based on current vitals and state of being. \n",
    "#  An action can be thought of as a change in current diagnosis based on one of those vitals.\n",
    "#  The inspirations for the bulk of this code came from Komorowksi's AI Clinician which can be found \n",
    "#  here: https://github.com/matthieukomorowski/AI_Clinician/blob/master/AIClinician_core_160219.m\n",
    "###\n",
    "\n",
    "###\n",
    "# Begin by establishing some global variables for use in the MDP creation\n",
    "###\n",
    "mdp_count:int = 500            # The number of repititions we want/count of MDPs we need to create \n",
    "clustering_iter:int = 32       # The number of times clustering will be conducted\n",
    "cluster_sample:float = 0.25    # Proportion of the data used for clustering\n",
    "gamma:float = 0.99             # How close we desire clusters to be in similarity (Percentage)\n",
    "transition_threshold:int = 5   # The cutoff value for the transition matrix\n",
    "final_policies:int = 1         # The number of policies we would like to end up with\n",
    "state_count:int = 750          # The number of distinct states\n",
    "action_count:int = 5           # Number of actions per state (reccommended 2 to 10)\n",
    "crossval_iter:int = 5          # Number of crossvalidation runs (Default is 80% Train, 20% Test)\n",
    "# This will be replaced by the loop index at some point (Iterations of all the models)\n",
    "loop_index:int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Data structures to hold our interim data\n",
    "###\n",
    "\n",
    "###\n",
    "# The 30 and 15 constants here are used solely for the purpose of allotting enough space\n",
    "# to save information later on down in the pipeline\n",
    "## \n",
    "model_data:np.ndarray = np.empty((mdp_count*2, 30,))\n",
    "model_data[:] = np.nan\n",
    "\n",
    "bestmodels_data:np.ndarray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The extract_init_column_data function takes 6 arguments: \n",
    "# \n",
    "# patient_data:   The full DataFrame of all the patient data, raw and unfiltered\n",
    "# id_column:      The name of the column in the dataframe containing the patient IDs\n",
    "# binary_columns: A list containing the names of all the columns that are binary data (only 0s and 1s)\n",
    "# normal_columns: A list containing the names of all the columns that are regular data (require no function transformations)\n",
    "# log_columns:    A list containing the names of all the columns that are logarithmic data (Log has already been applied)\n",
    "# debug_flag:     A boolean flag that indicates whether or not print statements will be executed\n",
    "# \n",
    "# The function has 6 return values:\n",
    "#\n",
    "# colbin:       The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:      The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:       The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw:    The DataFrame containing the data of all desired columns and their values\n",
    "# id_count:     The total number of IDs to be used  \n",
    "# icu_ids:      The ids of all patients to be used\n",
    "# patient_idxs: \n",
    "\"\"\"\n",
    "\n",
    "def extract_init_column_data(patient_data:pd.DataFrame, id_column:str='icustayid', binary_columns:List[str]=None, normal_columns:List[str]=None,\n",
    "                            log_columns:List[str]=None, debug_flag:bool=False) -> (List[str], List[str], List[str], \n",
    "                                                                                   pd.DataFrame, pd.DataFrame, int, List[List[int]]):\n",
    "\n",
    "    # Grab list of unique patient ICU stay IDs\n",
    "    icu_ids:int = patient_data[id_column].unique()\n",
    "    # Number of patients to be used for states\n",
    "    id_count:int = icu_ids.size\n",
    "    if debug_flag:\n",
    "        print(id_count)\n",
    "\n",
    "    # Create a data structure to save patient data for report\n",
    "    patient_idxs:List[List[int]]= np.zeros((id_count, mdp_count,), dtype=np.int64)\n",
    "\n",
    "    # All our columns are broken up into 3 distinct categories:\n",
    "    # 1. Binary values (0 or 1)\n",
    "    # 2. Standard Ranges (Plain old Integers + Decimals)\n",
    "    # 3. Logarthmic Values (columnvalue = log(columnvalue))\n",
    "    colbin:List[str] = []\n",
    "    colnorm:List[str] = [] \n",
    "    collog:List[str] = []\n",
    "    \n",
    "    # Enables custom column selection\n",
    "    if binary_columns == None:\n",
    "        colbin = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag']\n",
    "    else:\n",
    "        colbin = binary_columns\n",
    "    \n",
    "    if normal_columns == None:\n",
    "        colnorm = ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', 'qSOFA'];\n",
    "    else:\n",
    "        colnorm = normal_columns\n",
    "    if log_columns == None:\n",
    "        collog = ['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly'];\n",
    "    else:\n",
    "        collog = log_columns\n",
    "    # Create seperate dataframes for each of the columns\n",
    "    colbin_df:pd.DataFrame = patient_data[colbin]\n",
    "    colnorm_df:pd.DataFrame = patient_data[colnorm]\n",
    "    collog_df:pd.DataFrame = patient_data[collog]\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Let's make sure we have what we need\n",
    "        print(colbin_df, \"\\n\", colnorm_df, \"\\n\", collog_df)\n",
    "    # Rearrange the dataframe in order of binary, normal, and log data from left to right\n",
    "    MIMIC_raw:pd.DataFrame = pd.concat([colbin_df, colnorm_df, collog_df], axis=1)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_raw) \n",
    "    return colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_zscores function takes 4 arguments: \n",
    "# \n",
    "# colbin:    The list of all columns representing binary values\n",
    "# colnorm:   The list of all columns representing normal values\n",
    "# collog:    The list of all columns representing log values\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "#\n",
    "# and returns \n",
    "# colbin:    The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:   The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:    The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "\"\"\"\n",
    "\n",
    "def construct_zscores(colbin:List[str], colnorm:List[str], collog:List[str], MIMIC_raw:pd.DataFrame, debug_flag:bool) -> pd.DataFrame:\n",
    "\n",
    "    # We want a Z-Score for every item. This a measure of variance to see how far a value is from the mean\n",
    "\n",
    "    # We need to normalize binaries to -0.5 and 0.5 for later use\n",
    "    MIMIC_zscores:pd.DataFrame = MIMIC_raw\n",
    "\n",
    "    # No need for the zscore algorithm here, -0.5 and 0.5 suffice\n",
    "    MIMIC_zscores[colbin] = MIMIC_zscores[colbin] - 0.5\n",
    "\n",
    "    # Recall these columns are logarithmic, so they needed converted back for proper Z-Scoring (+ 0.1 to avoid log(0))\n",
    "    # Note that log(0.1) is essentially 0, Mathematically proved\n",
    "    \n",
    "    # zscore is the function pulled from the stats library in the initial import calls\n",
    "    MIMIC_zscores[collog] = np.log(MIMIC_zscores[collog] + 0.1).apply(zscore)\n",
    "\n",
    "    # Normal column requires no modifications. Z-Scores are calculated as normal\n",
    "    MIMIC_zscores[colnorm] = MIMIC_zscores[colnorm].apply(zscore)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_zscores)\n",
    "\n",
    "    # We want the Re_Admission and fluid intake scaled Similarly to the other variables\n",
    "    MIMIC_zscores['re_admission'] = np.log(MIMIC_zscores['re_admission'] + 0.6)\n",
    "    # Apply a scalar to fluid intake\n",
    "    MIMIC_zscores['input_total'] = 2 * MIMIC_zscores['input_total']\n",
    "    \n",
    "    return MIMIC_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The split_training_and_test function takes 4 arguments: \n",
    "# \n",
    "# id_count:     Total number of patient IDs to be used\n",
    "# icu_ids:      The IDs for all ICU visits\n",
    "# debug_flag:   A flag that enables/disables print statements \n",
    "# save_to_file: A flag that saves our deisred\n",
    "# \n",
    "# and returns 3 values:\n",
    "# \n",
    "# train_ids:  The ICU visit IDs to be used in the training set\n",
    "# test_ids:   The ICU visit IDs to be used in the testing set\n",
    "# train_flag: A list over all of MIMIC_raw that marks training rows as True\n",
    "\"\"\"\n",
    "\n",
    "def split_training_and_test(id_count:int, icu_ids:pd.DataFrame, debug_flag:bool, save_to_file:bool, \n",
    "                               patientdata:pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    ### The main loop to generate all possible models\n",
    "\n",
    "    num_rows:int = id_count  # Total Number of Patients to divy data up\n",
    "    testing_flag:int = 1     # The random number we use to identify a patient used for testing\n",
    "\n",
    "    # This will allow the loop over a large about of models\n",
    "\n",
    "    train_ids:List[int] = []       # A list containing all training ids from the icu_ids list\n",
    "    test_ids:List[int] = []         # A list containing all testing ids from the icu_ids list\n",
    "\n",
    "    # We want approximate 20% test, 80% train, so we random numbers 1-5\n",
    "    # 1s Represent data points that will be used to test, 2-5 will be used to train\n",
    "    group_ids:pd.DataFrame = pd.DataFrame([int(np.floor(crossval_iter * np.random.random() + 1)) for i in range(1, id_count + 1)])\n",
    "    icu_pair_set:pd.DataFrame = pd.concat([pd.DataFrame(icu_ids), group_ids], axis=1, sort=False)\n",
    "    icu_pair_set.columns = ['id', 'fil_val']\n",
    "    train_ids =  icu_pair_set[icu_pair_set['fil_val'] != testing_flag]\n",
    "    test_ids = icu_pair_set[icu_pair_set['fil_val'] == testing_flag]\n",
    "\n",
    "    # We want to ensure that the testing patients + training patients = total patients\n",
    "    if (train_ids['id'].size + test_ids['id'].size) != id_count:\n",
    "        print(\"The testing and training set do not add up to the total set\")\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Percentage for testing should be about 20%, Training about 80%\n",
    "        print(\"Testing Percentage: \" + str((test_ids['id'].size / id_count)))\n",
    "        print(\"Training Percentage: \" + str((train_ids['id'].size / id_count)))\n",
    "\n",
    "    # After grabbing all the IDs, we want to flag all the rows that are train or test\n",
    "    train_flag:pd.DataFrame = patientdata['icustayid'].isin(train_ids['id'])\n",
    "    # Save the training ids for later if we need to use them\n",
    "    if save_to_file:\n",
    "        # Temporarily write train_flag for later use DELETE LATER\n",
    "        with open('sample_train.txt', 'wb') as fp:\n",
    "            pickle.dump(train_flag, fp)\n",
    "    return train_ids, test_ids, train_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_and_balance(id_count:int, icu_ids:pd.DataFrame, debug_flag:bool, save_to_file:bool, \n",
    "                                     patientdata:pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    # Grab the patient's 90d death statuses\n",
    "    death_status = patientdata.drop_duplicates('icustayid')['mortality_90d']\n",
    "    icu_pair_set:pd.DataFrame = pd.concat([pd.DataFrame(icu_ids), pd.DataFrame(death_status.values.tolist())], axis=1, sort=False)\n",
    "    icu_pair_set.columns = ['id', 'death_status']\n",
    "    inner_cv = StratifiedKFold(10)\n",
    "    outer_cv = StratifiedKFold(10)\n",
    "    # outer_cv values\n",
    "    print(icu_pair_set)\n",
    "    print(icu_pair_set['death_status'].value_counts())\n",
    "    \n",
    "    return train_ids, test_ids, train_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 8 arguments: \n",
    "# \n",
    "# train_flag:       The dataframe representing whether or not a row is used in training or not\n",
    "# MIMIC_zscores:    The dataframe representing the zscores of the dataset\n",
    "# debug_flag:       A flag that determines if print statements are executed\n",
    "# save_flag:        A flag that determines if the training_zscores are saved\n",
    "# patient_data:     The raw MIMIC dataframe\n",
    "# bloc_name:        The name of the column that dictates a 'bloc' of time within an ICU visit\n",
    "# id_name:          The name of the column that dictates an ICU stay's ID\n",
    "# death_name:       The name of the column that dictates a patient's life status \n",
    "# \n",
    "# and returns 7 values:\n",
    "# \n",
    "# train_zscores:   The portion of MIMIC_zscores that is in the training set\n",
    "# test_zscores:    The portion of MIMIC_zscores that is in the testing set\n",
    "# train_blocs:     The rows of the MIMIC dataset that is in the training set\n",
    "# test_blocs:      The rows of the MIMIC dataset that is in the testing set\n",
    "# train_id_list:   The list of all IDs in the training set\n",
    "# train_90d:       A flag indicating if a given patient died in the training set after 90d\n",
    "# test_90d:        A flag indicating if a given patient died in the testing set after 90d\n",
    "\"\"\"\n",
    "\n",
    "def zscores_for_train_and_test(train_flag:pd.DataFrame, MIMIC_zscores:pd.DataFrame, debug_flag:bool, save_flag:bool,\n",
    "                               patient_data:pd.DataFrame, bloc_name:str, id_name:str, death_name:str) -> (pd.DataFrame,\n",
    "                               pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    # Seperate the Z-Scores for the training set and the testing set\n",
    "    train_zscores:pd.DataFrame = MIMIC_zscores[train_flag]\n",
    "    test_zscores:pd.DataFrame = MIMIC_zscores[~train_flag]\n",
    "\n",
    "    # Validate all data is selected\n",
    "    if(train_zscores.size + test_zscores.size != MIMIC_zscores.size):\n",
    "        print(\"The Z-Scores are all evenly distributed\")\n",
    "\n",
    "\n",
    "    # The blocs of relevance in order based on the train and test set\n",
    "    # These will be used to build relevant data frames later down\n",
    "    train_blocs:List[int] = patientdata[train_flag][bloc_name]\n",
    "    test_blocs:List[int] = patientdata[~train_flag][bloc_name]\n",
    "\n",
    "    # Doing the same with the patient ids\n",
    "    train_id_list:pd.DataFrame = patientdata[train_flag][id_name]\n",
    "    # We modify a column later to use for the test_id_list, not necesssary here.\n",
    "\n",
    "    # Grabbing the boolean values for the patients who died within 90 days in the training set\n",
    "    train_90d:pd.DataFrame = patientdata[train_flag][death_name]\n",
    "    test_90d:pd.DataFrame = patientdata[~train_flag][death_name]\n",
    "    # Next, we want to sample the existing training set to only pick cluster_sample percent to use\n",
    "    \n",
    "    # We want to flag all the data points in the train_zscores set that will be used to create the MDP\n",
    "    \n",
    "    # Note: len(train_zscores.index) is the fastest way to get the number of rows in a dataframe in pandas\n",
    "    \n",
    "    # The actual set to use\n",
    "    sample_train_set:pd.DataFrame = train_zscores[train_flag]\n",
    "        \n",
    "    if save_flag:\n",
    "        # Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('train_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(train_zscores, fp)\n",
    "        with open('test_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(test_zscores, fp)\n",
    "    return train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, train_90d, test_90d, sample_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# determine_optimal_clusters is a method that uses the modern curvature method to optimize clusters in the KMeans\n",
    "# clustering algorithm. The data for this is provided by euclidean variance calculated using Redhawk supercluster\n",
    "#\n",
    "# The input is:\n",
    "# variance_data: A list containing the total variance observed at each k-value (number of clusters)\n",
    "# show_graph: whether or not we would like to display the graph of the curvature\n",
    "# \n",
    "# The output is:\n",
    "# optimal_cluster_count: The int representing the optimal number of clusters\n",
    "\"\"\"\n",
    "def determine_optimal_clusters(variance_data:List[float], show_graph:bool): \n",
    "    # A large portion of this was provided/inspired by code from Dr. Giabbanelli's machine learning course\n",
    "    lower_bound:int = 1\n",
    "    upper_bound:int = len(variance_data) + 1\n",
    "    val_range = range(lower_bound, upper_bound)\n",
    "    # Approximating the values to a polynomial fit\n",
    "    coefs:List[float] = np.polyfit(val_range, variance_data, 3)\n",
    "    # Generate a list of values at each point\n",
    "    coefs_vals:List[float] = np.polyval(coefs[::1], val_range)\n",
    "    if show_graph:\n",
    "        # Generate the plot\n",
    "        plt.plot(val_range, variance_data)\n",
    "        plt.show()\n",
    "    # Curve values fluctuate based on geometric scaling, as such, it is required\n",
    "    # to test different k-values across different alphas\n",
    "    alphas:List[float] = [i/20.0 for i in range(0, 400)]\n",
    "    max_curve:float = -1\n",
    "    max_k:float = -1\n",
    "    # Test on a variety of Alphas and find the maximal result\n",
    "    for alpha in alphas:\n",
    "        # Scale the variation values, equation coefficients, and curve values based on various alphas\n",
    "        scaled_vars:List[float] = [alpha * variance_val for variance_val in variance_data]\n",
    "        scaled_coefs:List[float] = np.polyfit(val_range, scaled_vars, 3)\n",
    "        # Calculate the curvature of the line at each step\n",
    "        curve_vals = np.polyder(scaled_coefs)\n",
    "        curve_vals_mod = np.polyder(curve_vals)\n",
    "        scaled_curves:List[float] = []\n",
    "        # The authors of the curvature method use these two tranformations to calculate values\n",
    "        for k in val_range:\n",
    "            function_val_one:float = abs(np.polyval(curve_vals_mod, k))\n",
    "            function_val_two:float = 1 + np.polyval(curve_vals, k)**2\n",
    "            scaled_curves.append(function_val_one / (function_val_two**1.5))\n",
    "        # Iterate over all the scaled curves to determine the max\n",
    "        index_and_value = max(enumerate(scaled_curves), key=(lambda x: x[1]))\n",
    "        max_index:float = index_and_value[0]\n",
    "        max_value:float = index_and_value[1]\n",
    "        if(max_value > max_curve):\n",
    "            max_curve = max_value \n",
    "            max_k = max_index\n",
    "    # The k value needs to be scaled back to it's actual value, not its list index\n",
    "    true_k = max_k + 1\n",
    "    return true_k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters(data_set:pd.DataFrame, max_state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                max_num_iter:int=32):\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in range(1, max_state_count):\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(data_set)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "    return variance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_parallel(num:int):\n",
    "    data_set=sample_train_set\n",
    "    max_state_count=state_count\n",
    "    num_loops_per_iter=10000\n",
    "    max_num_iter=clustering_iter\n",
    "    total_needed_runs:List[int] = [i for i in range(1, 751)]\n",
    "    thread_needed_runs:List[int] = np.array_split(total_needed_runs, 24)[num]\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in thread_needed_runs:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(sample_train_set)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "        single_run = f'{state_count},{total_variance}'\n",
    "        with open('all_cluster_runs.csv', 'a') as f:\n",
    "            print(single_run, file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_driver(run_optimal_clusters:bool, run_multithread:bool, show_graph:bool) -> int:\n",
    "    # Cluster selection\n",
    "    optimal_cluster_count = 0 \n",
    "    # If we wish to find the optimal number of clusters\n",
    "    if run_optimal_clusters:\n",
    "        # This code is here for posterity, and cannot be run natively in juptyer. Use the stock Python CLI command or \n",
    "        # PyPi to actually run this. Note, it can take forever with large Nodes\n",
    "        if run_multithread:     \n",
    "            pool = multiprocessing.Pool()\n",
    "            pool.map(calculate_optimal_clusters_parallel, range(23))\n",
    "        else:\n",
    "            calculate_optimal_clusters(data_set=sample_train_set, max_state_count=state_count, num_loops_per_iter=10000, \n",
    "                                max_num_iter=clustering_iter)\n",
    "    # Load the data from the cluster variances into here\n",
    "    cluster_results = [0 for i in range(0, 749)]\n",
    "    with open('all_cluster_runs.csv', 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            cluster_results[int(row[0])-1] = float(row[1])\n",
    "    optimal_cluster_count = determine_optimal_clusters(cluster_results, show_graph=show_graph)\n",
    "    return optimal_cluster_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 5 arguments: \n",
    "# data_set:               The training dataset used to generate the various clusters\n",
    "# state_count:            Total number of clusters desired in the end result\n",
    "# num_loops_per_iter:     The number of loops ran per 1 iteration to approximate clusters\n",
    "# max_num_iter:           Number of iterations to run the algorithm\n",
    "# debug_flag:             True indicates print statements are included, False indicates no prints\n",
    "# regenerate_flag:        True indictates the kmeans will be run again. False means the cached/previous file value will be used\n",
    "# \n",
    "# and returns 4 values:\n",
    "# \n",
    "# cluster_values:         A value generated based on the Z-Scores of all the patient's data relative to each other\n",
    "# cluster_labels:         A number generated for the correspond state -> value relationship. Range is 0 to state_count\n",
    "# train_zscores:          The Z-Scores of the training set as created earlier\n",
    "# closest_clusters:       A list representing which state each patient datapoint is closest to\n",
    "# \n",
    "\"\"\"\n",
    "def kmeans_cluster_calculations(data_set:pd.DataFrame, train_zscores:pd.DataFrame, test_zscores:pd.DataFrame,\n",
    "                                max_num_iter:int=32, state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                debug_flag:bool=True, regenerate_flag=False, load_zscores=False) -> (List[List[np.float64]], List[int], pd.DataFrame, \n",
    "                                                                                                     np.ndarray, np.ndarray):\n",
    "    # In order to prepare a proper set of states, we want to use k-means clustering to group various patients into \n",
    "    # distinct states based on Z-Scores\n",
    "\n",
    "    # K-Means or K-Means++ is a technique used to condense very diverse and sparse data into similar groups called 'clusters'\n",
    "    # The K-means algorithm will create k clusters from N data points. In the case of this research,\n",
    "    # the algorithm divides patients into groups that have similar data (age, blood pressure, etc..) and creates a faux 'point'\n",
    "    # at the center of that particular clustering of data\n",
    "\n",
    "    # The KMeans takes three 'settings' arguments\n",
    "    # 1. n_clusters: The number of clusters (later to be used as states), that we desire the algorithm to produce\n",
    "    # this value has been preset to state_count which is 750\n",
    "    # 2. max_iter: How many times each round of k-means clustering will make adjustments, set at 10,000 in my case\n",
    "    # 3. n_init: The number of max_iter batches that will be conducted in a row. The best of these will be chosen\n",
    "    # and saved in the variable clusters_models\n",
    "    if regenerate_flag:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(data_set)\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('cluster_labels.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.labels_, fp)\n",
    "        with open('cluster_centers.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.cluster_centers_, fp)\n",
    "        if debug_flag:\n",
    "            print(clusters_models.labels_)\n",
    "            print(clusters_models.cluster_centers_)\n",
    "    \n",
    "\n",
    "    # Read these values back in from being saved to file\n",
    "    cluster_values:List[List[np.float64]] = []\n",
    "    cluster_labels:List[int] = [] \n",
    "\n",
    "    with open ('cluster_centers.txt', 'rb') as fp:\n",
    "        cluster_values = pickle.load(fp)\n",
    "    with open ('cluster_labels.txt', 'rb') as fp:\n",
    "        cluster_labels = pickle.load(fp)\n",
    "    if load_zscores:\n",
    "        with open ('train_zscores.txt', 'rb') as fp:\n",
    "            train_zscores = pickle.load(fp)\n",
    "        with open ('test_zscores.txt', 'rb') as fp:\n",
    "            test_zscores = pickle.load(fp)\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(cluster_values, \"\\n\", \"Dimensions: \", len(cluster_values),\" x \", len(cluster_values[0]), \"\\n\", train_zscores)\n",
    "    \n",
    "    # We now want to use the clusters to determine their nearest real data point neighbors\n",
    "    # As a visual of this. Suppose we have 4 flags of different colors scattered over a park. The K-Means++ algorithm\n",
    "    # is what planted the flags in the middle of groups of people that are similar. The KNN Search (K nearest neighbor search)\n",
    "    # can be used in MatLab as a simple point finder instead of as a more complicated Supervised Learning algorithm. In Python \n",
    "    # we can make use of the Vector Quanization (vq) package to assign each point to a centroid\n",
    "    print(train_zscores)\n",
    "    print(cluster_values)\n",
    "    closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "    closest_clusters_test:np.ndarray = vq(test_zscores, cluster_values)\n",
    "    \n",
    "\n",
    "    if debug_flag:\n",
    "        print(len(closest_clusters[0]))\n",
    "\n",
    "    # As an aside, closest_clusters[1] contains the distance between each point's values (in this case 50 of them)\n",
    "    # and their closest cluster's values.\n",
    "    # Ex: If a point is [1, 1, 1] and it's closest cluster is the point [3, 3, 3]  closest_clusters[1] would contain the vector\n",
    "    # [abs(3 - 1), abs(3 - 1), abs(3 - 1)] or [2, 2, 2]\n",
    "\n",
    "    # Validate that all the points are in the range 0-749 (since there are only 750 clusters as specified previously)\n",
    "    for i in closest_clusters[0]:\n",
    "        if(i > (state_count - 1) or i < 0):\n",
    "            print(\"The clusters you are searching for are not configured properly and are out of bounds\")\n",
    "            print(\"Did you modify the cluster_count variable without changing this error configuration?\")\n",
    "    \n",
    "    return cluster_values, cluster_labels, train_zscores, closest_clusters, closest_clusters_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cluster_number(cluster_distances):\n",
    "    total_variance = 0\n",
    "    for i in range(0, len(cluster_distances)):\n",
    "        total_variance = total_variance + cluster_distances[i]\n",
    "    return total_variance\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The generate_action_column function takes 4 arguments: \n",
    "#\n",
    "# column_values: A series of column values from a dataframe that we want to turn into action states\n",
    "# num_groups: How many groups or distinct actions we want to split the data into\n",
    "# column_name: The name of the column used for print debug statements\n",
    "# num_rows: The total number of rows in the full column before modifications (This is normally patientdata[column_name].size)\n",
    "# \n",
    "# This function returns column_actions, a series that represents the 'action', or group that each row of data falls under.\n",
    "#\n",
    "# An example is found down below, but in words, this function takes a full column of data, groups \n",
    "# the values for that data into num_groups distinct actions, and returns a series representing actions based on row\n",
    "# \n",
    "# Ex: Patients' blood pressure might be grouped into 5 categories (Action 1: < 20 mmHg, Action 2: > 20 mmHg && < 60 mmHg... etc)\n",
    "\"\"\"\n",
    "\n",
    "def generate_action_column(column_values:pd.DataFrame, num_groups:int, column_name:str, num_rows:int, debug_flag:bool=True) -> pd.Series:\n",
    "    # Determine minimum and maxium to scale data appropriately\n",
    "    if debug_flag:\n",
    "        print(\"Old Lowest \", column_name, \" Rank: \", min(column_values.rank()))\n",
    "        print(\"Old Highest \" , column_name,  \" Rank: \", max(column_values.rank()))\n",
    "    # Now we want to rank these actions in order of their value (lowest to highest)\n",
    "    # Normalizing according to lowest and highest rank\n",
    "    \n",
    "    # Moving the minimum to zero\n",
    "    column_ranks:pd.Series = (column_values.rank() - min(column_values.rank()))\n",
    "    # Shifting the max to approximately 1.0\n",
    "    column_ranks:pd.Series = column_ranks / max(column_ranks)\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Validate that the range is indeed 0 to 1\n",
    "        print(\"New Lowest \", column_name, \" Rank: \", min(column_ranks))\n",
    "        print(\"New Highest \", column_name, \" Rank: \", max(column_ranks))\n",
    "\n",
    "    # The Max of all column values needs to be nearly 1, and the min of all column\n",
    "    # values needs to be nearly 0 \n",
    "    if round(max(column_ranks), 3) != 1 or round(min(column_ranks), 3) != 0:\n",
    "        print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "        print(\"Current max: \", round(max(column_ranks), 3))\n",
    "        print(\"Curret min: \", round(min(column_ranks), 3))\n",
    "    # Normalize the rank values to even intevals of ranks\n",
    "    old_values:List[float] = np.sort(column_ranks.unique()).tolist()\n",
    "    even_intervals:List[float] = [i/column_ranks.unique().size for i in range(0, column_ranks.unique().size)]\n",
    "    # Iterate over the Series to apply the normalized values\n",
    "    for i in range(0, column_ranks.size):\n",
    "        old_value_index:float = old_values.index(column_ranks[i])\n",
    "        column_ranks[i] = even_intervals[old_value_index] \n",
    "    \n",
    "    # This is a mathematics trick to seperate all the values into {num_groups} distinct groups based on their rank.\n",
    "    # Given different columns of interest this can take different forms. For IV fluids, this number is 5.\n",
    "    column_groups:pd.Series = np.floor(((column_ranks + 1.0/float(num_groups)) * (num_groups - 1))) + 1\n",
    "    \n",
    "    # Validate that groups are all associated with desired group split\n",
    "    if not(column_groups.isin([i for i in range(1, num_groups + 1)]).any()):\n",
    "        print(\"Groups chosen fall outside the desired 1-\" + num_groups + \" window\")\n",
    "\n",
    "    column_actions:pd.Series = pd.Series([1 for i in range(0, num_rows)])\n",
    "\n",
    "    # If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "    for index in column_groups.index:\n",
    "        column_actions[index] = column_groups[index]\n",
    "        \n",
    "    return column_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This function takes two arguments:\n",
    "# actions_column: A column of action groups generated by the above function (generate_action_column())\n",
    "# real_values: The actual values from the dataset corresponding to the same column as actions_column\n",
    "# and returns a list that contains the real median values for each 'group' actions.\n",
    "#\n",
    "# Ex: We apply the function to the action_column \"IV_Fluid\", which has split the data into 4 different groups of \n",
    "# IV_Fluid actions. This function will produce a list containing the median amount of IV_Fluid administered for each of those\n",
    "# groups (Group 1 -> Adminster 20 mL, Group 2 -> Administer 40 mL, Group 3 -> Administer 60 mL, Group 4 -> Administer 80 mL\n",
    "\"\"\"\n",
    "\n",
    "def median_action_values(actions_column: pd.DataFrame, real_values:pd.DataFrame) -> List[np.float64]:\n",
    "    # Grab all the unique actions for a column and sort them\n",
    "    all_groups:List[np.float64] = np.sort(actions_column.unique())\n",
    "    # Concatanate the group number and real value for each row\n",
    "    action_set:pd.DataFrame = pd.concat([actions_column, real_values], axis=1, sort=False)\n",
    "    # Name the columns for accurate querying\n",
    "    action_set.columns = ['group_id', 'data_val']\n",
    "    # Grab the median value for each group based on group number using python list comprehension\n",
    "    median_values:List[np.float64] = [np.median(action_set[action_set['group_id'] == i]['data_val']) for i in all_groups]\n",
    "    return median_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# This function takes one argument\n",
    "# list_action_columns: This is a Pandas dataframe that contains all the action_columns we desir to be grouped by index\n",
    "# This can be retrieving using the previously defined 'median action' function \n",
    "# \n",
    "# and returns two items:\n",
    "# list_action columns: The 'keys' or integers that represent every permutation of actions\n",
    "# chosen_action: The key that was chosen based on the action values in each column\n",
    "\"\"\"\n",
    "def generate_action_matrix(list_action_columns:pd.DataFrame) -> (List[int], List[int]):\n",
    "    # Grabs the list of columns the user has provided for use\n",
    "    desired_columns:List[str] = [column for column in list_action_columns]\n",
    "    # Drops all group combinations that are duplicates\n",
    "    list_action_columns_indexes:pd.DataFrame = list_action_columns.drop_duplicates(desired_columns)\n",
    "    # Sorts all combinations in order\n",
    "    list_action_columns_indexes = list_action_columns_indexes.sort_values(desired_columns)\n",
    "    # Create a dictionary based on the values from the dataframe \n",
    "    list_action_columns_indexes:List[int] = list_action_columns_indexes.values.tolist() \n",
    "    # Determine which index in the list each row corresponds to \n",
    "    # Ex: For an 2-D action permutation list of [1,1] thru [5,5], there are 5 x 5 possibilities\n",
    "    # {1..5}, {1..5}, so there are 25 possible permutations, the indexes will run 1 - 25\n",
    "    chosen_action:List[int] = [list_action_columns_indexes.index(val_pair) for val_pair in list_action_columns.values.tolist()]\n",
    "    # Return the keys first, and then the true values for the dataset\n",
    "    return list_action_columns_indexes, chosen_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The build_dataset_actions function takes 7 arguments: \n",
    "# patientdata: A Dataframe representing all the patients and their data\n",
    "# first_column: Our first column that we desire to build our action matrix with (first_column x second_column) (SOFA by default)\n",
    "# second_column: Our second column that we desire to build or action matrix with (first_column x second_column) (qSOFA by default)\n",
    "# num_groups_first_column: The number of groups we want to divide our first column into (5 by default)\n",
    "# num_groups_second_column: The number of groups we want to divide our second column into (4 by default)\n",
    "# debug_flag: A boolean that determines whether we want debug prints presents\n",
    "# graph_flag: Whether we want to print the graph or not\n",
    "# \n",
    "# and returns 2 values:\n",
    "# \n",
    "# train_chosen_actions:        The actions for each row of the training dataset\n",
    "# train_action_values:         The matrix representing (first_column x second_column)\n",
    "# action_list:                 A list of actions taken at every given datapoint\n",
    "\"\"\"\n",
    "def build_dataset_actions(patientdata:pd.DataFrame, first_column:str=\"SOFA\", second_column:str=\"qSOFA\", num_groups_first_column:int=5,\n",
    "                         num_groups_second_column:int=4, debug_flag:bool=True, graph_flag:bool=True) -> (pd.Series, \n",
    "                                                                                                         List[List[int]],\n",
    "                                                                                                         List[int]):\n",
    "    # Generate the actions for a the first desired column of data\n",
    "    first_col:pd.DataFrame = patientdata[first_column]\n",
    "    first_col_actions:pd.Series = generate_action_column(column_values = first_col, num_groups = num_groups_first_column, \n",
    "                                                        column_name = first_column, \n",
    "                                                         num_rows = patientdata[first_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(first_col_actions.unique())    \n",
    "    # Now do the same for the second desired column of data\n",
    "    second_col:pd.DataFrame = patientdata[second_column]\n",
    "    second_col_actions:pd.Series = generate_action_column(column_values = second_col, num_groups = num_groups_second_column,  \n",
    "                                                         column_name = second_column, \n",
    "                                                         num_rows = patientdata[second_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(second_col_actions.unique())\n",
    "    \n",
    "    # Obtain the median real values that each division represents\n",
    "    first_median_actions:List[np.float64] = median_action_values(actions_column = first_col_actions, real_values = patientdata[first_column])\n",
    "    second_median_actions:List[np.float64] = median_action_values(actions_column = second_col_actions, real_values = patientdata[second_column])\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(first_column,\" Action Median Values:\", str(first_col_actions), \"\\n\" + second_column + \":\", second_median_actions, \"\\n\")\n",
    "    ###\n",
    "    # FINISH CONSTRUCTION OF ALL ACTIONS AND THEIR VALUES\n",
    "    ###\n",
    "    # Combine the columns that we desire to observe (iv_fluid_actions, vasopressor_actions)\n",
    "    combined_groups:pd.DataFrame = pd.concat([first_col_actions, second_col_actions], axis=1, sort=False)\n",
    "    # Name the columns for proper usage in the function\n",
    "    combined_groups.columns = [first_column, second_column]\n",
    "    \n",
    "    # The Key value pair for every datapoint and the corresponding action taken at that point\n",
    "    action_keys, action_list = generate_action_matrix(list_action_columns = combined_groups)\n",
    "    # Plot the distribution of actions\n",
    "    if graph_flag:\n",
    "        plt.hist(action_list, density=False, bins=20)  # `density=False` would make counts\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Index of Action Chosen: 1 through 24\")\n",
    "    train_chosen_actions:List[np.float64] = []\n",
    "    with open ('sample_train.txt', 'rb') as fp:\n",
    "        train_chosen_actions = pickle.load(fp)\n",
    "    test_chosen_actions:List[bool] = [not i for i in train_chosen_actions]\n",
    "    # Grab a Series representing the action taken by the train data only\n",
    "    train_chosen_actions:pd.Series = pd.Series(action_list)[train_chosen_actions]\n",
    "    # Grab a Series representing the action taken by the test data only\n",
    "    test_chosen_actions:pd.Series = pd.Series(action_list)[test_chosen_actions]\n",
    "\n",
    "    # Assign all action choices to their corresponding median values as shown previously\n",
    "    if debug_flag:\n",
    "        print(first_median_actions, second_median_actions)\n",
    "\n",
    "\n",
    "    # This gives us the representative median values for a patient's vitals present in various action groups\n",
    "    # action_keys[i] corresponds to train_action_values[i]\n",
    "    # So, if the patient falls into group [1, 1] or no iv fluid given, no vasopressor administered,\n",
    "    # The corresponding median values for this group will be represented by train_action_values (0.0, 0.0).\n",
    "    # A patient in group [1, 2] (no iv fluid, a little vasopressor) will have a median real value of (0.0, 0.04)\n",
    "    action_values_matrix:List[List[int]] = list(cartesian_prod(first_median_actions, second_median_actions))\n",
    "\n",
    "    if len(action_values_matrix) != len(first_median_actions) * len(second_median_actions):\n",
    "        print(\"Something went wrong in determining the Cartesian product\")\n",
    "    \n",
    "    return train_chosen_actions, test_chosen_actions, action_values_matrix, action_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_prestate_matrix_train function takes 5 arguments: \n",
    "# train_90d:              A column representing whether or not a patient was dead or alive at the end of 90days\n",
    "# train_blocs:            All the rows of data for a patient for each individual hospital stay\n",
    "# closest_clusters:       The closest data cluster that a given data point falls near\n",
    "# train_chosen_actions:   The action that is represented by two of the patient's characteristics, calculated previously\n",
    "# is_debug:               Whether or not statements are printed over time.\n",
    "# \n",
    "# and returns 1 value:\n",
    "# \n",
    "# qlearning_dataset_mod: The full training dataset configured to be converted into states and actions\n",
    "# all_lower_ranges:      The lower bound for the reward values\n",
    "# all_upper_ranges:      The upper bound for the reward values\n",
    "\"\"\"\n",
    "def construct_prestate_matrix_train(train_90d:pd.DataFrame, train_blocs:pd.DataFrame, closest_clusters:List[List[int]],\n",
    "                                    train_chosen_actions:pd.DataFrame, is_debug:bool = True, action_count:int = 20, \n",
    "                                    state_count:int = 750) -> (pd.DataFrame, List[int], List[int]):\n",
    "    ###\n",
    "    # BEGIN CONSTRUCTION OF PRE-STATE MATRIX\n",
    "    # This will be used to build the full state/action matrix\n",
    "    ### \n",
    "\n",
    "    # Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "    # If they have died, the range is [-100, 100]\n",
    "    # If they are alive, the range is [100, -100]\n",
    "    range_vals:List[int] = [100, -100]\n",
    "    # Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "    # This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "    train_90d_polarity:List[int] = (2 * (1 - train_90d) - 1)\n",
    "    range_matrix:List[int] = [np.multiply(polarity, range_vals) for polarity in train_90d_polarity]\n",
    "    # Grab the lower range limit and upper range limit seperately in order to build the\n",
    "    # full range of reward values\n",
    "    all_lower_ranges:List[int] = [i[0] for i in range_matrix]\n",
    "    all_upper_ranges:List[int] = [i[1] for i in range_matrix]\n",
    "        \n",
    "    # The qlearning_dataset prior to modification contains 6 columns and ~190885 rows (around 75% of the data)\n",
    "    # The columns are as follows:\n",
    "    #\n",
    "    # training_bloc: time_series stamps for a patient's state over time, very in range from {1..?}\n",
    "    #\n",
    "    # closest_cluster_index: The index of the nearest cluster to the z-scores of the patient's data, \n",
    "    # corresponding actual data for each cluster's index (i) can be found in cluster_values[i]\n",
    "    #\n",
    "    # chosen_action_index: The chosen action or representation of a patient's IV_Fluid and Vasopressor status [0 - 24]\n",
    "    # \n",
    "    # 90d_mortality_status: 0 means the patient is alive 90 days after discharge from ICU\n",
    "    #                      1 means the patient is dead  90 days after discharge from ICU\n",
    "    #\n",
    "    # lower_range + upper_range: An index to be used later on, gathered from the range index\n",
    "    if is_debug:\n",
    "        print(\"Training Blocs Length: \", str(len(train_blocs)), \"\\nClosest Clusters Length: \", str(len(closest_clusters[0])), \n",
    "              \"\\nAction List Length: \", str(len(train_chosen_actions)), \"\\nTrain 90d Length\", str(len(train_90d)), \n",
    "              \"\\nRange Matrix Length: \", len(range_matrix))\n",
    "    qlearning_dataset:pd.DataFrame = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters[0]), \n",
    "                                   pd.Series(train_chosen_actions.tolist()), \n",
    "                                   pd.Series(all_lower_ranges),\n",
    "                                   pd.Series(train_90d.tolist())], \n",
    "                                   axis=1, sort=False)\n",
    "    qlearning_dataset.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', '90d_mortality_status']\n",
    "    if is_debug: \n",
    "        print(qlearning_dataset)\n",
    "    \n",
    "    # Modify the set for the final time in order to construct the final life + death states for each patient\n",
    "    qlearning_dataset_mod = modify_qlearning_dataset(ql_dataset = qlearning_dataset)# Print some important details of the set\n",
    "    if is_debug:\n",
    "        # Total patients being observed in the test\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['training_bloc'] == 1]['training_bloc']))\n",
    "        # Show that we now have end states established \n",
    "        print(len(qlearning_dataset[qlearning_dataset['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == state_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] ==  state_count + 1]['chosen_action_index']))\n",
    "        print(qlearning_dataset_mod, \"\\n\")\n",
    "    return qlearning_dataset_mod, all_lower_ranges, all_upper_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# modify_qlearning_dataset is a function that takes a dataframe intended for qlearning and modifies\n",
    "# it in preparation for the ML. In essence, it runs through creating the life and and death states in preparation\n",
    "# for constructing the MDP\n",
    "#\n",
    "# Parameters: \n",
    "# ql_dataset:  The Dataset we want to modify in preparation\n",
    "# is_training: Whether we are building the training set or not.\n",
    "# is_debug:    Whether prints are included or not\n",
    "#       \n",
    "# \n",
    "# Returns: qlearning_dataset_mod - The modified dataset\n",
    "\"\"\"\n",
    "\n",
    "def modify_qlearning_dataset(ql_dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    # The base qlearning_dataset does not account for endpoints in either life or death\n",
    "    # These states have not been established yet, which is what this step corrects\n",
    "    qlearning_dataset:pd.DataFrame = ql_dataset.copy()\n",
    "    qlearning_dataset_len:int = len(qlearning_dataset.index)\n",
    "    # We need space to add a death/life state for every patient, about a 20% increase in size from the original MDP\n",
    "    # We will cut the excess off by the end of the loop\n",
    "    qlearning_dataset_len_mod:float = int(np.floor(qlearning_dataset_len * 1.2))\n",
    "    qlearning_dataset_mod:np.ndarray = []\n",
    "    qlearning_dataset_mod:np.ndarray = np.array([[0 for i in range(0, 5)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "    # Start construction of modified data\n",
    "    row:int = 0\n",
    "    # In Markov theory, an absorbing state is one which can be entered, but cannot be left. (Similar to the Hotel California)\n",
    "    # In the case of this experiment, those states are either life (state_count) or death (state_count + 1) per patient as\n",
    "    # defined by me (750, 751)\n",
    "    absorbing_states:List[int] = [state_count, state_count + 1]\n",
    "\n",
    "    # Start the loop to begin capping the markov chain off at life and death states\n",
    "    for i in range(0, qlearning_dataset_len - 1):\n",
    "        # Use the already gathered data for each row\n",
    "        qlearning_dataset_mod[row, :] = qlearning_dataset.iloc[i][0:5]\n",
    "        # If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\n",
    "        if qlearning_dataset.iloc[i + 1]['training_bloc'] <= qlearning_dataset.iloc[i]['training_bloc']:\n",
    "            # Grab the row\n",
    "            whole_row:pd.DataFrame = qlearning_dataset.iloc[i]\n",
    "            # Set most of the row to the original data's values, except set the action to be either state 750 or 751\n",
    "            # Life or death respectively\n",
    "            row = row + 1\n",
    "            # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "            print(whole_row['90d_mortality_status'])\n",
    "            qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status']]\n",
    "        row = row + 1\n",
    "    # Add in the last row\n",
    "    whole_row:pd.DataFrame = qlearning_dataset.iloc[len(qlearning_dataset.index) - 1]\n",
    "    qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status']]\n",
    "    \n",
    "    row = row + 1\n",
    "    # Get rid of the unneeded rows\n",
    "    qlearning_dataset_mod:pd.DataFrame = pd.DataFrame(qlearning_dataset_mod[0:row, :])\n",
    "    qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', 'death_state']\n",
    "    # Set all rows not in the terminal states to 0 reward to start\n",
    "    qlearning_dataset_mod.loc[qlearning_dataset_mod['chosen_action_index'] != -1,'reward_value'] = 0\n",
    "    return qlearning_dataset_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Now that we officially have some a valid bloc for actions, and a valid set of states, it's time \n",
    "#  to begin building the transitions matrix.\n",
    "###\n",
    "\n",
    "### If the matrix is bidirectional (S1 -> S2, S2 -> S1 are both valid, we can build two matrices)\n",
    "\n",
    "### \n",
    "# The MDP Toolbox we are going to be using requires Transition and Reward Matrices to be in the form\n",
    "# M(action, state1, state2)\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "# The create_transition_matrix method takes 4 arguments:\n",
    "# num_actions: The total number of possible actions (calculated by action_count ^ 2 or in py, action_count ** 2)\n",
    "# num_states:  Number of states the model uses\n",
    "# qlearning_dataset: The dataset that will be used for the qlearning process\n",
    "# transition_threshold: How many actions do we want to deem as scarce and not worth keeping (default = 5)\n",
    "# reverse: If false, the matrix that is created is represented as transition[A][S1][S2], if true: transition[A][S2][S1]\n",
    "# \n",
    "# and returns 2 values:\n",
    "# transition_matrix: The counts of which actions were chosen in which states\n",
    "# physician_policy:  The transition_matrix that has been turned into probabilties by dividing counts in each state by \n",
    "# total counts\n",
    "# \n",
    "\"\"\"\n",
    "def create_transition_matrix(num_actions:int, num_states:int, ql_data_input:pd.DataFrame, \n",
    "                             transition_threshold:int = 5, reverse:bool = False) -> (int_matrix2D, float_matrix2D):\n",
    "    # The transition matrix is a 3D construct, involving a transition between two states\n",
    "    # and an action. The dimensions for the matrix are (state_count * 2) * (state_count + 2) * action_count\n",
    "    transition_matrix:float_matrix2D = [[[0 for i in range(0, num_states + 2)] for i in range(0, num_states + 2)] for i in range(0, num_actions)]\n",
    "    # NP Arrays allow for more compact and efficient slicing\n",
    "    transition_matrix = np.array(transition_matrix).astype(float)\n",
    "    # We also need a matrix to denote the policy that corresponds with taken a particular action from a state\n",
    "    transition_policy_count:int_matrix2D = [[0 for i in range(0, num_states + 2 )] for i in range(0, num_actions)]\n",
    "    transition_policy_count = np.array(transition_policy_count).astype(float)\n",
    "    # Iterate over the actual data in order to form the actual states and their corresponding actions\n",
    "    # As soon as we hit the next patient (the next row has a training bloc value of 1), we stop processing actions for that patient\n",
    "    for i in range(0, len(ql_data_input) - 1):\n",
    "        # Since 1 is our 'endpoint' for each patient, there are no actions we can take from this point on\n",
    "        if ql_data_input.iloc[i + 1]['training_bloc'] > ql_data_input.iloc[i]['training_bloc']:\n",
    "            S1:int = ql_data_input.iloc[i]['closest_cluster_index']\n",
    "            S2:int = ql_data_input.iloc[i + 1]['closest_cluster_index'] \n",
    "            action_id:int = ql_data_input.iloc[i]['chosen_action_index']\n",
    "            if not(reverse):\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S1][S2] = transition_matrix[action_id][S1][S2] + 1\n",
    "            else:\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S2][S1] = transition_matrix[action_id][S2][S1] + 1\n",
    "                \n",
    "            # Count the number of times action A is used to transition from S1\n",
    "            transition_policy_count[action_id][S1] = transition_policy_count[action_id][S1] + 1        \n",
    "\n",
    "    # In order to avoid drastically altering our model, we fix a constant\n",
    "    # value (set by default to 5), in order to declare sparse actions \n",
    "    # as essentially not happening (make their count 0)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if transition_policy_count[i][j] <= transition_threshold:\n",
    "                transition_policy_count[i][j] = 0 \n",
    "    # Now, we want to prevent transitions from state -> state using\n",
    "    # a certain action if that action is sparse or nonexistant\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if not(reverse):\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,j,:] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,j,:] = transition_matrix[i,j,:]/np.float64(transition_policy_count[i][j])\n",
    "            else:\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,:,j] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,:,j] = transition_matrix[i,:,j]/np.float64(transition_policy_count[i][j])\n",
    "    \n",
    "    # Ensure no divisions create NaNs or infinities\n",
    "    transition_matrix = np.nan_to_num(transition_matrix)\n",
    "    # Determine the phyisican's policy based on total count\n",
    "    # This comes in handy later when comparing model ability\n",
    "    total_transitions:float = sum(transition_policy_count)\n",
    "    physician_policy:float_matrix2D = np.divide(transition_policy_count, total_transitions)\n",
    "    return transition_matrix, physician_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_Q_learning_eval is a method that takes 6 arguments and returns 2 items\n",
    "# This method evaluates the performance of the MDP determined previously\n",
    "# \n",
    "# Parameters:\n",
    "# ql_train_set_Q: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# alpha: A hyperparameter that weights our reward function at each step\n",
    "# numtraces: Number of Q-Learning iterations we would like to perform\n",
    "# num_actions: Total number of actions in the set (For Sepsis: 25)\n",
    "# num_clusters: Total number of states in the set (For Sepsis: 752)\n",
    "# is_training:  If False, this is the first phase (construction of the model). If True, the model is being Trained\n",
    "# \n",
    "# Returns:\n",
    "# Q_Equation = The set of Q-Values obtained by the algorithm's performance\n",
    "# sum_Q_values = The Q-Equation's performance at a given step in the algorithm\n",
    "\"\"\"\n",
    "def offpolicy_Q_learning_eval(ql_train_set_Q: pd.DataFrame, death_answer_key:List[int],gamma: float, alpha: float, \n",
    "                              numtraces: int, num_actions: int, \n",
    "                              num_clusters: int, stopping_difference: float, \n",
    "                              is_training=False, is_random=False,\n",
    "                              modulus_val:int=5000) -> (float_matrix2D, List[float]):\n",
    "    # We need to save the Q-value for each run \n",
    "    sum_Q_values:List[float] = np.zeros((numtraces))\n",
    "    # Where the Q-Values are saved at each given run\n",
    "    Q_Equation:int_matrix2D = np.zeros((num_actions, num_clusters))\n",
    "    # We need to save the Average Q value after so many iterations\n",
    "    previous_avg_Q:int = 0\n",
    "    # The list of all starting patient states \n",
    "    first_index_list:List[int] = ql_train_set_Q[ql_train_set_Q['training_bloc'] == 1].index\n",
    "    # A seperate index running in parallel with i for the sum_Q_values\n",
    "    jj:int = 0\n",
    "    # If we don't want a random data set from the existant set, only do each data point once\n",
    "    if not(is_random):\n",
    "        numtraces = len(first_index_list)\n",
    "    # We iterate for the total number of times we want to do this process\n",
    "    for i in range(0, numtraces):\n",
    "        # Select a random patient starting point from the data\n",
    "        patient_starter_index:int = 0\n",
    "        if is_random:\n",
    "            patient_starter_index:int = random.choice(first_index_list)\n",
    "        else:\n",
    "            patient_start_index = first_index_list[i]\n",
    "        # As Q-learning progreses, we need a data structure to track progress\n",
    "        full_trace:List[Tuple[float, int, int]] = []\n",
    "        # While we are still working on a single patient\n",
    "        num_ql_rows = len(ql_train_set_Q.index)\n",
    "        # We run until we hit the end of the patient or the end of the dataset\n",
    "        while patient_starter_index + 1 < num_ql_rows and ql_train_set_Q.iloc[patient_starter_index + 1]['training_bloc'] != 1:\n",
    "            # Grab state (Initial State at this point)\n",
    "            state_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['closest_cluster_index']\n",
    "            # Grab action taken from this point\n",
    "            action_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['chosen_action_index']\n",
    "            # Grab reward provided by taken an action from this state to the next\n",
    "            reward_value:float = ql_train_set_Q.iloc[patient_starter_index + 1]['reward_value']\n",
    "            # A 'step' in the trace, a single data point snapshot\n",
    "            trace_step:Tuple[float, int, int] = (reward_value, state_index, action_index)\n",
    "            # Add the step to the full trace\n",
    "            full_trace.append(trace_step)\n",
    "            # Increment the current data row\n",
    "            patient_starter_index = patient_starter_index + 1\n",
    "        # Full length of the trace path\n",
    "        trace_length:int = len(full_trace)\n",
    "        return_reward:float = ql_train_set_Q.iloc[patient_starter_index]['reward_value']\n",
    "        # Grab the final reward (final reward for last state)\n",
    "        if is_training:\n",
    "            # If the model guessed correctly and the patient died, weight the path -100 \n",
    "            if (reward_value < 0 and answer_key['death_state'].iloc[patient_starter_index] == 1):\n",
    "                return_reward = -100\n",
    "            # If the model guessed correctly and the patient lived, weight the path +100\n",
    "            elif (reward_value >= 0 and answer_key['death_state'].iloc[patient_starter_index] == 0):\n",
    "                return_reward = 100\n",
    "            # If the model guessed incorrectly and the patient lived, weight the path +100\n",
    "            elif (reward_value < 0 and answer_key['death_state'].iloc[patient_starter_index] == 0):\n",
    "                return_reward = 100\n",
    "            # If the model guessed incorrectly and the patient died, weight the path -100\n",
    "            elif (reward_value >= 0 and answer_key['death_state'].iloc[patient_starter_index] == 1):\n",
    "                return_reward = -100\n",
    "        # Walk the trace stack backwards\n",
    "        for j in range(trace_length - 2, -1, -1):\n",
    "            # Grab the state, action, and reward at each step\n",
    "            step_state:int = full_trace[j][1]\n",
    "            step_action:int = full_trace[j][2]\n",
    "            # Using alpha blending (where we take a portion of the old value and blend it with the new)\n",
    "            # We blend part of the old value of this state with the new value\n",
    "            Q_Equation[step_action, step_state] = (1 - alpha) * Q_Equation[step_action, step_state] + alpha * return_reward\n",
    "            # Cap the range for node values (-100, 100)\n",
    "            if Q_Equation[step_action, step_state] > 100:\n",
    "                Q_Equation[step_action, step_state] = 100\n",
    "            if Q_Equation[step_action, step_state] < -100:\n",
    "                Q_Equation[step_action, step_state] = -100\n",
    "            # Recall we have a gamma value to determine the impact of previous decisions on future ones\n",
    "            # Note: this is a Hyperparameter (a parameter on the model itself)\n",
    "            return_reward = return_reward * gamma  + full_trace[j][0]\n",
    "        # Save the overall value based on the current states and actions avaiable at the \n",
    "        # current iteration\n",
    "        sum_Q_values[jj] = np.sum(Q_Equation)\n",
    "        jj = jj + 1\n",
    "        # If we haven't hit our max iterations, we still want to see if we should keep pushing forward\n",
    "        # If there is no noticable progress, we want to stop\n",
    "        \n",
    "        # This is only applicable if we are not using all the training data set patients\n",
    "        if is_random:\n",
    "            # Perform a check every modulus_val runs\n",
    "            if i % modulus_val == 0:\n",
    "                # Grab the current slice of unchecked {modulus_val} values\n",
    "                slice_mean:float = np.mean(sum_Q_values[j - modulus_val:j])\n",
    "                # Calculate the difference between current and last average \n",
    "                max_difference:float =(slice_mean - previous_avg_Q)/previous_avg_Q\n",
    "                # Check if the average of this {modulus_val} values is less than 0.001 away from the previous\n",
    "                if abs(max_difference) < stopping_difference:\n",
    "                    break\n",
    "                previous_avg_Q = slice_mean\n",
    "            \n",
    "    # Trim off the portion of the list we did not use\n",
    "    sum_Q_values = sum_Q_values[0:jj]\n",
    "    return Q_Equation, sum_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def construct_trained_model(ql_final_dataset:pd.DataFrame, state_count:int, \n",
    "                            total_actions:int, weighted_probabilities:bool, \n",
    "                            physician_policy:List[List[float]]) -> pd.DataFrame:\n",
    "    # Make a final duplicate of the data\n",
    "    qlearning_train_final:pd.DataFrame = ql_final_dataset.copy()\n",
    "    \n",
    "    # If weights are considered, use the weighted rewards from the Q_Equation, otherwise \n",
    "    # use the Q_Equation as is \n",
    "    if weighted_probabilities:\n",
    "        # Weight Q-Value rewards according to their frequency of occuring\n",
    "        # The Q-Equation for a given state + action pair is equivalent to the reward value\n",
    "        # The Phyiscian Policy is the probability of that action ocurring given a state\n",
    "        # These weights prevent rare events from having massively scewed rewards\n",
    "        # For example, a path that occurs through a given state exactly once would have a much larger\n",
    "        # reward than a frequently traveled path, which could scew the data.\n",
    "        value_matrix = np.zeros((state_count, total_actions))\n",
    "        for i in range(0, state_count):\n",
    "            for j in range(0, total_actions):\n",
    "                value_matrix[i][j] = physician_policy[j][i] * Q_Equation[j][i]\n",
    "        for i in range(0, len(qlearning_train_final.index)):\n",
    "            row = qlearning_train_final.iloc[i]\n",
    "            if((row['closest_cluster_index'] != state_count) and (row['closest_cluster_index'] != state_count + 1)):\n",
    "                row['reward_value'] = value_matrix[row['closest_cluster_index']][row['chosen_action_index']]\n",
    "    else:\n",
    "        for i in range(0, len(qlearning_train_final.index)):\n",
    "            row = qlearning_train_final.iloc[i]\n",
    "            if((row['closest_cluster_index'] != state_count) and (row['closest_cluster_index'] != state_count + 1)):\n",
    "                row['reward_value'] = Q_Equation[row['chosen_action_index']][row['closest_cluster_index']]\n",
    "    \n",
    "    return qlearning_train_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(qlearning_train_final:pd.DataFrame, test_name:str):\n",
    "    # Iterate until the full set is done\n",
    "    i:int = 0\n",
    "    total_patients = 0\n",
    "    correct_guesses = 0\n",
    "    dead_instead_live = 0\n",
    "    live_instead_dead = 0\n",
    "    ql_dataframe_size = len(qlearning_train_final.index)\n",
    "    while i < ql_dataframe_size:\n",
    "        total_q_value = 0\n",
    "        while (i < ql_dataframe_size - 1) and (qlearning_train_final.iloc[i + 1]['chosen_action_index'] != -1):\n",
    "            total_q_value = total_q_value + qlearning_train_final.iloc[i + 1]['reward_value']\n",
    "            i = i + 1\n",
    "        # If we guess correctly, add one to correct guesses\n",
    "        if ((total_q_value >= 0 and qlearning_train_final.iloc[i]['death_state'] == 0) or \\\n",
    "             total_q_value < 0 and qlearning_train_final.iloc[i]['death_state'] == 1):\n",
    "                correct_guesses = correct_guesses + 1\n",
    "        if (total_q_value < 0 and qlearning_train_final.iloc[i]['death_state'] == 0):\n",
    "            dead_instead_live = dead_instead_live + 1\n",
    "        if (total_q_value >= 0 and qlearning_train_final.iloc[i]['death_state'] == 1):\n",
    "            live_instead_dead = live_instead_dead + 1\n",
    "            \n",
    "        total_patients = total_patients + 1\n",
    "        i = i + 1\n",
    "    print(\"Test Name: \" + test_name)\n",
    "    print(\"Accuracy: \" + str(correct_guesses/total_patients))\n",
    "    print(\"Living People Guessed Dead: \" + str(dead_instead_live))\n",
    "    print(\"Dead People Guessed Living: \" + str(live_instead_dead))\n",
    "    print(\"Total Guesses: \" + str(total_patients))\n",
    "    print(\"Correct Guesses: \" + str(correct_guesses))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # First Round of Functions\n",
    "    colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs = extract_init_column_data(patient_data=patientdata, debug_flag=False)\n",
    "    MIMIC_zscores:pd.DataFrame = construct_zscores(colbin, colnorm, collog, MIMIC_raw, debug_flag=False) \n",
    "    train_ids, test_ids, train_flag = split_training_and_test(id_count, icu_ids, debug_flag=False, save_to_file=True, patientdata=patientdata)\n",
    "    train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, train_90d, test_90d, sample_train_set = \\\n",
    "    zscores_for_train_and_test(train_flag, MIMIC_zscores, debug_flag=False, save_flag=True, \n",
    "                               patient_data=patientdata, bloc_name='bloc', id_name='icustayid', death_name='mortality_90d')\n",
    "    optimal_cluster_count = calculate_optimal_clusters_driver(run_optimal_clusters=False, run_multithread=False, show_graph=False)\n",
    "    cluster_values, cluster_labels, train_zscores, closest_clusters, closest_clusters_test = \\\n",
    "    kmeans_cluster_calculations(data_set=sample_train_set,  train_zscores=train_zscores, test_zscores=test_zscores,\n",
    "                                max_num_iter=clustering_iter, state_count=optimal_cluster_count, num_loops_per_iter=10000,\n",
    "                                debug_flag=False, regenerate_flag=False, load_zscores=False)\n",
    "    # Training and Testing Construction\n",
    "    train_chosen_actions, test_chosen_actions, action_values_matrix, action_list = build_dataset_actions(patientdata=patientdata, \n",
    "                      first_column=\"SOFA\", second_column=\"qSOFA\", \n",
    "                      num_groups_first_column=5, num_groups_second_column=4, \n",
    "                      debug_flag=False, graph_flag=False)\n",
    "    \n",
    "    qlearning_dataset_train_final, all_upper_ranges, all_lower_ranges = construct_prestate_matrix_train(train_90d = train_90d, \n",
    "                                                                                            train_blocs = train_blocs, \n",
    "                                                                                            closest_clusters = closest_clusters,\n",
    "                                                                                            train_chosen_actions = train_chosen_actions, \n",
    "                                                                                            is_debug=True,\n",
    "                                                                                            action_count=action_count, \n",
    "                                                                                            state_count=state_count)\n",
    "    # Constructing Transition Matrix(A, State1, State2)\n",
    "    total_actions:int = len(action_values_matrix)     \n",
    "    # Execute the function call\n",
    "    transition_mat, physician_policy = create_transition_matrix(num_actions = total_actions, num_states = state_count,ql_data_input = qlearning_dataset_train_final, transition_threshold = transition_threshold, reverse = False)\n",
    "    # Whether or not a patient in the training set actually lived or died, used for evaluation\n",
    "    answer_key=qlearning_dataset_train_final[qlearning_dataset_train_final['training_bloc'] == 1]\n",
    "    \n",
    "    # Normal value for iter_ql is 6, iter_wis is 750\n",
    "    # Q-Learning results is usually a positive value\n",
    "    # WIS Value should be Negative\n",
    "    Q_Equation, sum_Q_values = offpolicy_Q_learning_eval(\n",
    "            ql_train_set_Q=qlearning_dataset_train_final,\n",
    "            death_answer_key=answer_key,\n",
    "            gamma=0.99, \n",
    "            alpha=0.1,\n",
    "            numtraces=30000,\n",
    "            num_actions=total_actions,\n",
    "            num_clusters=state_count,\n",
    "            stopping_difference=0.001,\n",
    "            is_training=True,\n",
    "            is_random=False,\n",
    "            modulus_val=5000\n",
    "        )\n",
    "    qlearning_train_final = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy)\n",
    "    qlearning_train_final_no_weighting = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy)\n",
    "    evaluate_model_accuracy(qlearning_train_final=qlearning_train_final, test_name=\"Train, Weighted\")\n",
    "    evaluate_model_accuracy(qlearning_train_final=qlearning_train_final_no_weighting, test_name=\"Train, No Weighting\")\n",
    "    qlearning_dataset_test_final, _, _ = construct_prestate_matrix_train(train_90d = test_90d, \n",
    "                                                                     train_blocs = test_blocs, \n",
    "                                                                     closest_clusters = closest_clusters_test,\n",
    "                                                                     train_chosen_actions = test_chosen_actions, \n",
    "                                                                     is_debug=False,\n",
    "                                                                     action_count=action_count, \n",
    "                                                                     state_count=state_count)\n",
    "    qlearning_dataset_test_final_weighted = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy)\n",
    "    qlearning_dataset_test_final_nonweighted = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy)\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_weighted, test_name=\"Test, Weighted\")\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_nonweighted, test_name=\"Test, No Weighting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
