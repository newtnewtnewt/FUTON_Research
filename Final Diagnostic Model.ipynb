{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  FUTON Model MDP + Q-Learning Creation Script\n",
    "#  A Research Project conducted by Noah Dunn \n",
    "###\n",
    "\n",
    "# Import the standard tools for working with Pandas dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import ctypes\n",
    "import csv\n",
    "# Pickle provides easy Object Serialization for quick read + writes of data\n",
    "import pickle\n",
    "# Vector Quantization for Determining Cluster Centers\n",
    "from scipy.cluster.vq import vq\n",
    "# Skikit offers a solution to perform K-Means++ clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# Scipy provides a library to execute Z-Score Normalization\n",
    "from scipy.stats import zscore\n",
    "# We want to do type hinting for API clarification\n",
    "from typing import *\n",
    "# Import the MDP toolbox that contains a method for conducting Q-Learning\n",
    "# Tool can be found here: https://github.com/sawcordwell/pymdptoolbox\n",
    "# Documentation for the tool can be found here \n",
    "import mdptoolbox\n",
    "# Itertools provides an easy way to perform Cartesian product on multiple sets\n",
    "from itertools import product as cartesian_prod\n",
    "import multiprocessing\n",
    "# We need to perform 10 fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# For undersampling during the balancing phase\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# For using PCA to check variance\n",
    "from sklearn.decomposition import PCA\n",
    "### Some repetitive type hinting\n",
    "int_matrix2D = np.array\n",
    "float_matrix2D = np.array\n",
    "int_matrix3D = np.array\n",
    "float_matrix3D = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Data File that will be used to conduct the experiments\n",
    "patientdata:pd.DataFrame = pd.read_csv(\"G:/MIMIC-ALL/MIMIC-PATIENTS/patient_data_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "#  An MDP, or Markov Decision Process is used to model relationships between various states and actions.\n",
    "#  A state can be thought of in medical solution as a patient's diagnosis based on current vitals and state of being. \n",
    "#  An action can be thought of as a change in current diagnosis based on one of those vitals.\n",
    "#  The inspirations for the bulk of this code came from Komorowksi's AI Clinician which can be found \n",
    "#  here: https://github.com/matthieukomorowski/AI_Clinician/blob/master/AIClinician_core_160219.m\n",
    "###\n",
    "\n",
    "###\n",
    "# Begin by establishing some global variables for use in the MDP creation\n",
    "###\n",
    "mdp_count:int = 500            # The number of repititions we want/count of MDPs we need to create \n",
    "clustering_iter:int = 32       # The number of times clustering will be conducted\n",
    "cluster_sample:float = 0.25    # Proportion of the data used for clustering\n",
    "gamma:float = 0.99             # How close we desire clusters to be in similarity (Percentage)\n",
    "transition_threshold:int = 5   # The cutoff value for the transition matrix\n",
    "final_policies:int = 1         # The number of policies we would like to end up with\n",
    "state_count:int = 750          # The number of distinct states\n",
    "action_count:int = 5           # Number of actions per state (reccommended 2 to 10)\n",
    "crossval_iter:int = 5          # Number of crossvalidation runs (Default is 80% Train, 20% Test)\n",
    "# This will be replaced by the loop index at some point (Iterations of all the models)\n",
    "loop_index:int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Data structures to hold our interim data\n",
    "###\n",
    "\n",
    "###\n",
    "# The 30 and 15 constants here are used solely for the purpose of allotting enough space\n",
    "# to save information later on down in the pipeline\n",
    "## \n",
    "model_data:np.ndarray = np.empty((mdp_count*2, 30,))\n",
    "model_data[:] = np.nan\n",
    "\n",
    "bestmodels_data:np.ndarray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The extract_init_column_data function takes 6 arguments: \n",
    "# \n",
    "# patient_data:   The full DataFrame of all the patient data, raw and unfiltered\n",
    "# id_column:      The name of the column in the dataframe containing the patient IDs\n",
    "# binary_columns: A list containing the names of all the columns that are binary data (only 0s and 1s)\n",
    "# normal_columns: A list containing the names of all the columns that are regular data (require no function transformations)\n",
    "# log_columns:    A list containing the names of all the columns that are logarithmic data (Log has already been applied)\n",
    "# debug_flag:     A boolean flag that indicates whether or not print statements will be executed\n",
    "# \n",
    "# The function has 6 return values:\n",
    "#\n",
    "# colbin:       The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:      The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:       The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw:    The DataFrame containing the data of all desired columns and their values\n",
    "# id_count:     The total number of IDs to be used  \n",
    "# icu_ids:      The ids of all patients to be used\n",
    "# patient_idxs: \n",
    "\"\"\"\n",
    "\n",
    "def extract_init_column_data(patient_data:pd.DataFrame, id_column:str='icustayid', binary_columns:List[str]=None, normal_columns:List[str]=None,\n",
    "                            log_columns:List[str]=None, debug_flag:bool=False) -> (List[str], List[str], List[str], \n",
    "                                                                                   pd.DataFrame, pd.DataFrame, int, List[List[int]]):\n",
    "\n",
    "    # Grab list of unique patient ICU stay IDs\n",
    "    icu_ids:int = patient_data[id_column].unique()\n",
    "    # Number of patients to be used for states\n",
    "    id_count:int = icu_ids.size\n",
    "    if debug_flag:\n",
    "        print(id_count)\n",
    "\n",
    "    # Create a data structure to save patient data for report\n",
    "    patient_idxs:List[List[int]]= np.zeros((id_count, mdp_count,), dtype=np.int64)\n",
    "\n",
    "    # All our columns are broken up into 3 distinct categories:\n",
    "    # 1. Binary values (0 or 1)\n",
    "    # 2. Standard Ranges (Plain old Integers + Decimals)\n",
    "    # 3. Logarthmic Values (columnvalue = log(columnvalue))\n",
    "    colbin:List[str] = []\n",
    "    colnorm:List[str] = [] \n",
    "    collog:List[str] = []\n",
    "    \n",
    "    # Enables custom column selection\n",
    "    if binary_columns == None:\n",
    "        colbin = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag']\n",
    "    else:\n",
    "        colbin = binary_columns\n",
    "    \n",
    "    if normal_columns == None:\n",
    "        colnorm = ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', 'qSOFA'];\n",
    "    else:\n",
    "        colnorm = normal_columns\n",
    "    if log_columns == None:\n",
    "        collog = ['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly'];\n",
    "    else:\n",
    "        collog = log_columns\n",
    "    # Create seperate dataframes for each of the columns\n",
    "    colbin_df:pd.DataFrame = patient_data[colbin]\n",
    "    colnorm_df:pd.DataFrame = patient_data[colnorm]\n",
    "    collog_df:pd.DataFrame = patient_data[collog]\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Let's make sure we have what we need\n",
    "        print(colbin_df, \"\\n\", colnorm_df, \"\\n\", collog_df)\n",
    "    # Rearrange the dataframe in order of binary, normal, and log data from left to right\n",
    "    MIMIC_raw:pd.DataFrame = pd.concat([colbin_df, colnorm_df, collog_df], axis=1)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_raw) \n",
    "    return colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_zscores function takes 4 arguments: \n",
    "# \n",
    "# colbin:    The list of all columns representing binary values\n",
    "# colnorm:   The list of all columns representing normal values\n",
    "# collog:    The list of all columns representing log values\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "#\n",
    "# and returns \n",
    "# colbin:    The resulting list of binary data columns, or the default list if none is provided\n",
    "# colnorm:   The resulting list of normal data columns, or the default list if none is provided\n",
    "# collog:    The resulting list of log data columns, or the default list if none is provided\n",
    "# MIMIC_raw: The DataFrame containing the data of all desired columns and their values\n",
    "\"\"\"\n",
    "\n",
    "def construct_zscores(colbin:List[str], colnorm:List[str], collog:List[str], MIMIC_raw:pd.DataFrame, debug_flag:bool) -> pd.DataFrame:\n",
    "\n",
    "    # We want a Z-Score for every item. This a measure of variance to see how far a value is from the mean\n",
    "\n",
    "    # We need to normalize binaries to -0.5 and 0.5 for later use\n",
    "    MIMIC_zscores:pd.DataFrame = MIMIC_raw\n",
    "\n",
    "    # No need for the zscore algorithm here, -0.5 and 0.5 suffice\n",
    "    MIMIC_zscores[colbin] = MIMIC_zscores[colbin] - 0.5\n",
    "\n",
    "    # Recall these columns are logarithmic, so they needed converted back for proper Z-Scoring (+ 0.1 to avoid log(0))\n",
    "    # Note that log(0.1) is essentially 0, Mathematically proved\n",
    "    \n",
    "    # zscore is the function pulled from the stats library in the initial import calls\n",
    "    MIMIC_zscores[collog] = np.log(MIMIC_zscores[collog] + 0.1).apply(zscore)\n",
    "\n",
    "    # Normal column requires no modifications. Z-Scores are calculated as normal\n",
    "    MIMIC_zscores[colnorm] = MIMIC_zscores[colnorm].apply(zscore)\n",
    "    if debug_flag:\n",
    "        print(MIMIC_zscores)\n",
    "\n",
    "    # We want the Re_Admission and fluid intake scaled Similarly to the other variables\n",
    "    MIMIC_zscores['re_admission'] = np.log(MIMIC_zscores['re_admission'] + 0.6)\n",
    "    # Apply a scalar to fluid intake\n",
    "    MIMIC_zscores['input_total'] = 2 * MIMIC_zscores['input_total']\n",
    "    \n",
    "    return MIMIC_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to have a model severely skewed to over-predict death/life, it is desireable\n",
    "to include an equal number of patients who have lived or died. \n",
    "Already with data that has been stratified based on averages, the balance gives the cleanest\n",
    "possible situation to achieve balanced output\n",
    "\n",
    "Input: \n",
    "train_set_predict: A DataFrame containing the IDs and summary data (mean, q1, q3, max, min)\n",
    "for a given patient selected for the training set\n",
    "train_set_response: A DataFrame containing information on whether or not a patient lived or died\n",
    "is_debug: A flag that enables print statements in the functions\n",
    "\n",
    "Output:\n",
    "list_ids: The full list of IDs to be used for the training set\n",
    "\n",
    "\"\"\"\n",
    "def balance(train_set_predict:List[int], train_set_response:List[int], is_debug:bool) -> List[int]:\n",
    "    # This will undersample the majority class using a specific seed to keep consistency across runs\n",
    "    # In this case, that value is 47\n",
    "    rus = RandomUnderSampler(random_state=47, sampling_strategy='majority')\n",
    "    # train_set_predict = train_set_predict.reset_index()\n",
    "    # Duplicate column names like are present here do not enable the sampling alg to work properly\n",
    "    # Save the initial column headers to change back after the fact\n",
    "    # Also, the reset_index shoves all the indexes to a 'level_0' column that will be used to identify them\n",
    "    train_set_predict = train_set_predict.reset_index()\n",
    "    start_column_headers = train_set_predict.columns\n",
    "    train_set_modified = train_set_predict[:]\n",
    "    # The fake column headers are just place holders so the randomized undersampling is allowed to take place\n",
    "    train_set_modified.columns = [\"fake\" + str(i) for i in range(0, len(train_set_modified.columns))]\n",
    "    # Perform the Undersampling of the majority class (Patients that Live)\n",
    "    train_set_predict_bal, train_set_response_bal = rus.fit_resample(train_set_modified, train_set_response)\n",
    "    # Reset column headers back to where they were \n",
    "    train_set_predict_bal.columns = start_column_headers\n",
    "    # Validate that the number of columns is correct\n",
    "    if is_debug:\n",
    "        print(train_set_response_bal['death_state'].value_counts())\n",
    "    list_ids = np.sort(train_set_predict_bal['level_0'].values.tolist())\n",
    "    # Return the IDs that we intend to use for the training\n",
    "    return list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The golden standard for obtained ideal results for any machine learning exercise in the modern day is the\n",
    "K-Fold Cross-Validation, specifically the stratified version of this technique. The K-fold divides the data into 10\n",
    "seperate chunks chosen based on the feature set, and the stratified portion indicates that this data is relatively \n",
    "balanced in terms of the included data per fold. This function also calls the balance function for the training\n",
    "output, in order to insure an equal number of patients who lived and patients who died are chosen\n",
    "\n",
    "Input:\n",
    "stratified_data: Time-Series DataFrame that has been summarized by patient in order to be used in the K-Fold\n",
    "predictor_columns:  The columns to be used for stratification acting as predictors (I.E. the Summary Statistics for all Features)\n",
    "response_column: The column that concerns performance, in this case the life/death status\n",
    "is_debug: Enables print statements in the function\n",
    "\n",
    "Output: \n",
    "all_train_sets: The List of Lists of all IDs to be used for the training data each run\n",
    "all_test_sets: The List of lists of all IDs to be used for the testing data each run\n",
    "\"\"\"\n",
    "\n",
    "def crossval_split(stratified_data:pd.DataFrame, predictor_columns:List[str], \n",
    "                   response_column:List[str], is_debug:bool, id_correction_dict:Dict[int, int]) -> (int_matrix2D, int_matrix2D):\n",
    "    # The cross fold has an inner and outer component for proper division\n",
    "    # This is provided by the sklearn library\n",
    "    inner_cv = StratifiedKFold(10)\n",
    "    outer_cv = StratifiedKFold(10)\n",
    "    # Divy up the data into the new dataframes based on the selected columns\n",
    "    predict_data = stratified_data[predictor_columns]\n",
    "    response_data = stratified_data[response_column]\n",
    "    # Save these to be appended to after all the cross folds are done\n",
    "    all_train_sets = []\n",
    "    all_test_sets = []\n",
    "    # Iterate through using the cross-folds to build the correct IDs\n",
    "    for training_samples, test_samples in outer_cv.split(predict_data, response_data):\n",
    "        for inner_train, inner_test in inner_cv.split(predict_data.iloc[training_samples], response_data.iloc[training_samples]):\n",
    "            # Grab the training data indexes and corresponding reponses\n",
    "            train_predict = predict_data.iloc[training_samples].iloc[inner_train]\n",
    "            train_response = response_data.iloc[training_samples].iloc[inner_train]\n",
    "            # Grab the testing data indexes and corresponding responses\n",
    "            test_predict = predict_data.iloc[training_samples].iloc[inner_test]\n",
    "            # Balance the training dataset, grab the desired ids\n",
    "            train_ids = balance(train_predict, train_response, is_debug=False)\n",
    "            # Grab the ids to use for testing\n",
    "            test_ids = list(test_predict.index)\n",
    "            # Fix the IDs to the correct indexes\n",
    "            train_ids =list(map(id_correction_dict.get, train_ids))\n",
    "            test_ids = list(map(id_correction_dict.get, test_ids))\n",
    "            # Load them into a grand list\n",
    "            all_train_sets.append(train_ids)\n",
    "            all_test_sets.append(test_ids)\n",
    "    return all_train_sets, all_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normally in cross-fold validation, it is perfectly usable to insert a filtered dataset directly\n",
    "into the cross-validation split function. Due to the nature of this data being time-series, with a variety of recorded \n",
    "chart events for each patient, a trick to still benefit from the cross-validation step is to 'flatten' the data\n",
    "by providing summary statistics for each feature (mean, min, max, q1, q3), and form a stratification off that\n",
    "\n",
    "Input:\n",
    "patientdata: A DataFrame containing all the entire unflattened dataset\n",
    "icu_ids: A list of all the ids of the patients\n",
    "\n",
    "Output:\n",
    "all_patient_summaries_df: A DataFrame with flattened values for each of the patients\n",
    "\"\"\"\n",
    "def flatten_timeseries_data(patientdata:pd.DataFrame, icu_ids:List[int]) -> pd.DataFrame:\n",
    "    # The full list of factors to get values for\n",
    "    all_factors = ['gender','mechvent','max_dose_vaso','re_admission', 'qSOFAFlag', 'SOFAFlag',\n",
    "                       'age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "                        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "                        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "                        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance', \n",
    "                        'qSOFA', 'SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total',\n",
    "                        'input_4hourly','output_total','output_4hourly']\n",
    "    # Iterate over all the factors for all the patients\n",
    "    all_patient_summaries:List[pd.DataFrame] = []\n",
    "    for id_val in icu_ids:\n",
    "        # Grab each patient's full time-series run through the data\n",
    "        single_patient_data = patientdata[patientdata['icustayid'] == id_val]\n",
    "        # We can stratified time series data by 'flattening' it, constructing some summary statistic for each column\n",
    "        desired_patient_columns = pd.DataFrame(single_patient_data, columns=all_factors)\n",
    "        # Turn into dataframe and make each column seperate (Not 50 rows, 1 column)\n",
    "        patient_mean_values = pd.DataFrame(desired_patient_columns.mean()).transpose()\n",
    "        patient_min_values = pd.DataFrame(desired_patient_columns.min()).transpose()\n",
    "        patient_max_values = pd.DataFrame(desired_patient_columns.max()).transpose()\n",
    "        # Need to reset index on these due to funky interaction with concat\n",
    "        patient_q1_values = pd.DataFrame(desired_patient_columns.quantile(0.25)).transpose().reset_index()\n",
    "        patient_q3_values = pd.DataFrame(desired_patient_columns.quantile(0.75)).transpose().reset_index()\n",
    "        # Build the dataframe from the pieces\n",
    "        full_summary = pd.concat([patient_mean_values, patient_min_values, \n",
    "                                  patient_max_values, patient_q1_values, \n",
    "                                  patient_q3_values], axis=1, sort=False)\n",
    "        # Add it to the master set\n",
    "        all_patient_summaries.append(full_summary)\n",
    "    # Fix the List into a dataframe\n",
    "    # Also, set the indexes back to start at 1\n",
    "    all_patient_summaries_df = pd.concat(all_patient_summaries, axis=0, sort=False).reset_index().drop(columns=['level_0'])\n",
    "    return all_patient_summaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As discussed in the cross_validate and balance functions, this step takes in the standard, preprocessed\n",
    "dataframe of MIMIC data to be used for this experiment, and outputs Lists of balanced ID sets to be used.\n",
    "The cross-validation and balancing steps produce a much better, and less biased model\n",
    "\n",
    "Input:\n",
    "icu_ids: DataFrame of IDs of all patients\n",
    "debug_flag: Whether or not print statements are included\n",
    "save_to_file: Whether or not intermediate steps are written to a file\n",
    "regenerate_flag: When no files have been saved previously, set this to True to rerun all data\n",
    "patientdata: The standard input MIMIC DataFrame\n",
    "\n",
    "Output:\n",
    "train_ids_set: A 2D list of all the lists of IDs to be used for training\n",
    "test_ids_set:  A 2D list of all the lists of IDs to be used for testing\n",
    "train_flag_set: A boolean representation of all the lists of IDs to be used for training\n",
    "test_flag_set: A boolean representation of all the list of IDs to be used for testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cross_validate_and_balance(icu_ids:pd.DataFrame, debug_flag:bool, save_to_file:bool, \n",
    "                                     regenerate_flag:bool, patientdata:pd.DataFrame, regenerate_cross:bool) -> (pd.DataFrame, \n",
    "                                                                                                                pd.DataFrame, \n",
    "                                                                                                                pd.DataFrame, \n",
    "                                                                                                                pd.DataFrame):\n",
    "    if regenerate_flag:\n",
    "        if not(regenerate_cross):\n",
    "            # Flatten the full dataframe into summary statistics per patient\n",
    "            # This is used to stratify and balance appropriately\n",
    "            all_patient_summaries_df = flatten_timeseries_data(patientdata, icu_ids)\n",
    "            # This step messes up the indexes, we need to key the indexes to the correct ICU_IDS\n",
    "            incorrect_indices = [i for i in all_patient_summaries_df.index]\n",
    "            # Use dictionary comprehension to create a way of mapping the incorrect indices with \n",
    "            # their correct counterparts\n",
    "            id_correction_dict = {incorrect_indices[i]: icu_ids[i] for i in range(0, len(incorrect_indices))}\n",
    "            if debug_flag:\n",
    "                print(all_patient_summaries_df)\n",
    "            if save_to_file:\n",
    "                    with open('pre_cross_data.txt', 'wb') as fp:\n",
    "                        pickle.dump(all_patient_summaries_df, fp)\n",
    "            # Grab the patient's 90d death statuses\n",
    "            death_status = patientdata.drop_duplicates('icustayid')['mortality_90d']\n",
    "            # To ensure everything is lined up adjacently, apply two more reset_index's\n",
    "            # Also, the creation of the dataframe creates column indexes we don't desire. Get rid of these\n",
    "            id_table = pd.DataFrame(icu_ids)\n",
    "            id_table.columns = ['column_name']\n",
    "            death_status = pd.DataFrame(death_status.values.tolist()).reset_index()\n",
    "            death_status = death_status.drop(death_status.columns[0], axis=1)\n",
    "            death_status.columns = ['death_state']\n",
    "            icu_pair_set:pd.DataFrame = pd.concat([id_table, death_status, all_patient_summaries_df], axis=1, sort=False)\n",
    "            if save_to_file:\n",
    "                with open('interm_cross_data.txt', 'wb') as fp:\n",
    "                        pickle.dump(icu_pair_set, fp)\n",
    "                with open('id_correction_dictionary', 'wb') as fp:\n",
    "                        pickle.dump(id_correction_dict, fp)\n",
    "        # Remove duplicates from the columns list overall\n",
    "        icu_pair_set = None\n",
    "        id_correction_dict = None\n",
    "        with open('interm_cross_data.txt', 'rb') as fp:\n",
    "            icu_pair_set = pickle.load(fp)\n",
    "        with open('id_correction_dictionary', 'rb') as fp:\n",
    "            id_correction_dict = pickle.load(fp)\n",
    "        predictor_columns = list(set(icu_pair_set.columns))\n",
    "        # Death state is the response\n",
    "        predictor_columns.remove('death_state')\n",
    "        response_column = ['death_state']\n",
    "        # Split the data using 10-fold Cross Validation and Balance the Training Dataset\n",
    "        train_ids_set, test_ids_set = crossval_split(icu_pair_set, predictor_columns, response_column, \n",
    "                                                     is_debug=False, id_correction_dict=id_correction_dict)\n",
    "        train_flag_set = []\n",
    "        test_flag_set = []\n",
    "        for train_ids in train_ids_set:\n",
    "            train_flag_set.append(patientdata['icustayid'].isin(train_ids))\n",
    "        for test_ids in test_ids_set:\n",
    "            test_flag_set.append(patientdata['icustayid'].isin(test_ids))\n",
    "        # Save the full data into files to be reused\n",
    "        # There is no reason to redo the operations if nothing about the dataset has changed\n",
    "        if save_to_file:\n",
    "            with open('train_flags.txt', 'wb') as fp:\n",
    "                pickle.dump(train_flag_set, fp)\n",
    "            with open('test_flags.txt', 'wb') as fp:\n",
    "                pickle.dump(test_flag_set, fp)\n",
    "            with open('train_ids.txt', 'wb') as fp:\n",
    "                pickle.dump(train_ids_set, fp)\n",
    "            with open('test_ids.txt', 'wb') as fp:\n",
    "                pickle.dump(test_ids_set, fp)\n",
    "            \n",
    "        return train_ids_set, test_ids_set, train_flag_set, test_flag_set\n",
    "    else:\n",
    "        # If the data has already been processed, load it from file\n",
    "        train_ids_set = []\n",
    "        test_ids_set = []\n",
    "        train_flag_set = []\n",
    "        test_flag_set = []\n",
    "        with open('train_flags.txt', 'rb') as fp:\n",
    "            train_flag_set = pickle.load(fp)\n",
    "        with open('test_flags.txt', 'rb') as fp:\n",
    "            test_flag_set = pickle.load(fp)\n",
    "        with open('train_ids.txt', 'rb') as fp:\n",
    "            train_ids_set = pickle.load(fp)\n",
    "        with open('test_ids.txt', 'rb') as fp:\n",
    "            test_ids_set = pickle.load(fp)\n",
    "        return train_ids_set, test_ids_set, train_flag_set, test_flag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 8 arguments: \n",
    "# \n",
    "# train_flag:       The dataframe representing whether or not a row is used in training or not\n",
    "# test_flag:        The dataframe representing whether or not a row is used in testing or not\n",
    "# MIMIC_zscores:    The dataframe representing the zscores of the dataset\n",
    "# debug_flag:       A flag that determines if print statements are executed\n",
    "# save_flag:        A flag that determines if the training_zscores are saved\n",
    "# patient_data:     The raw MIMIC dataframe\n",
    "# bloc_name:        The name of the column that dictates a 'bloc' of time within an ICU visit\n",
    "# id_name:          The name of the column that dictates an ICU stay's ID\n",
    "# death_name:       The name of the column that dictates a patient's life status \n",
    "# \n",
    "# and returns 7 values:\n",
    "# \n",
    "# train_zscores:   The portion of MIMIC_zscores that is in the training set\n",
    "# test_zscores:    The portion of MIMIC_zscores that is in the testing set\n",
    "# train_blocs:     The rows of the MIMIC dataset that is in the training set\n",
    "# test_blocs:      The rows of the MIMIC dataset that is in the testing set\n",
    "# train_id_list:   The list of all IDs in the training set\n",
    "# train_90d:       A flag indicating if a given patient died in the training set after 90d\n",
    "# test_90d:        A flag indicating if a given patient died in the testing set after 90d\n",
    "\"\"\"\n",
    "\n",
    "def zscores_for_train_and_test(train_flag:pd.DataFrame, test_flag:pd.DataFrame, MIMIC_zscores:pd.DataFrame, \n",
    "                               debug_flag:bool, save_flag:bool, patient_data:pd.DataFrame, bloc_name:str, \n",
    "                               id_name:str, death_name:str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, \n",
    "                                                                pd.DataFrame, pd.DataFrame, pd.DataFrame, \n",
    "                                                                pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    # Seperate the Z-Scores for the training set and the testing set\n",
    "    train_zscores:pd.DataFrame = MIMIC_zscores[train_flag]\n",
    "    test_zscores:pd.DataFrame = MIMIC_zscores[test_flag]\n",
    "\n",
    "    # The blocs of relevance in order based on the train and test set\n",
    "    # These will be used to build relevant data frames later down\n",
    "    train_blocs:List[int] = patientdata[train_flag][bloc_name]\n",
    "    test_blocs:List[int] = patientdata[test_flag][bloc_name]\n",
    "\n",
    "    # Doing the same with the patient ids\n",
    "    train_id_list:pd.DataFrame = patientdata[train_flag][id_name].reset_index()['icustayid']\n",
    "    test_id_list:pd.DataFrame = patientdata[test_flag][id_name].reset_index()['icustayid']\n",
    "    # We modify a column later to use for the test_id_list, not necesssary here.\n",
    "\n",
    "    # Grabbing the boolean values for the patients who died within 90 days in the training set\n",
    "    train_90d:pd.DataFrame = patientdata[train_flag][death_name]\n",
    "    test_90d:pd.DataFrame = patientdata[test_flag][death_name]\n",
    "    \n",
    "\n",
    "    if save_flag:\n",
    "        # Python has object serialization to make write/reads fasters, in the form of pickle\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        with open('train_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(train_zscores, fp)\n",
    "        with open('test_zscores.txt', 'wb') as fp:\n",
    "            pickle.dump(test_zscores, fp)\n",
    "    return train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, test_id_list, train_90d, test_90d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# determine_optimal_clusters is a method that uses the modern curvature method to optimize clusters in the KMeans\n",
    "# clustering algorithm. The data for this is provided by euclidean variance calculated using Redhawk supercluster\n",
    "#\n",
    "# The input is:\n",
    "# variance_data: A list containing the total variance observed at each k-value (number of clusters)\n",
    "# show_graph: whether or not we would like to display the graph of the curvature\n",
    "# \n",
    "# The output is:\n",
    "# optimal_cluster_count: The int representing the optimal number of clusters\n",
    "\"\"\"\n",
    "def determine_optimal_clusters(variance_data:List[float], show_graph:bool): \n",
    "    # A large portion of this was provided/inspired by code from Dr. Giabbanelli's machine learning course\n",
    "    lower_bound:int = 1\n",
    "    upper_bound:int = len(variance_data) + 1\n",
    "    val_range = range(lower_bound, upper_bound)\n",
    "    # Approximating the values to a polynomial fit\n",
    "    coefs:List[float] = np.polyfit(val_range, variance_data, 3)\n",
    "    # Generate a list of values at each point\n",
    "    coefs_vals:List[float] = np.polyval(coefs[::1], val_range)\n",
    "    if show_graph:\n",
    "        # Generate the plot\n",
    "        plt.plot(val_range, variance_data)\n",
    "        plt.show()\n",
    "    # Curve values fluctuate based on geometric scaling, as such, it is required\n",
    "    # to test different k-values across different alphas\n",
    "    alphas:List[float] = [i/20.0 for i in range(0, 400)]\n",
    "    max_curve:float = -1\n",
    "    max_k:float = -1\n",
    "    # Test on a variety of Alphas and find the maximal result\n",
    "    for alpha in alphas:\n",
    "        # Scale the variation values, equation coefficients, and curve values based on various alphas\n",
    "        scaled_vars:List[float] = [alpha * variance_val for variance_val in variance_data]\n",
    "        scaled_coefs:List[float] = np.polyfit(val_range, scaled_vars, 3)\n",
    "        # Calculate the curvature of the line at each step\n",
    "        curve_vals = np.polyder(scaled_coefs)\n",
    "        curve_vals_mod = np.polyder(curve_vals)\n",
    "        scaled_curves:List[float] = []\n",
    "        # The authors of the curvature method use these two tranformations to calculate values\n",
    "        for k in val_range:\n",
    "            function_val_one:float = abs(np.polyval(curve_vals_mod, k))\n",
    "            function_val_two:float = 1 + np.polyval(curve_vals, k)**2\n",
    "            scaled_curves.append(function_val_one / (function_val_two**1.5))\n",
    "        # Iterate over all the scaled curves to determine the max\n",
    "        index_and_value = max(enumerate(scaled_curves), key=(lambda x: x[1]))\n",
    "        max_index:float = index_and_value[0]\n",
    "        max_value:float = index_and_value[1]\n",
    "        if(max_value > max_curve):\n",
    "            max_curve = max_value \n",
    "            max_k = max_index\n",
    "    # The k value needs to be scaled back to it's actual value, not its list index\n",
    "    true_k = max_k + 1\n",
    "    return true_k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters(data_set:pd.DataFrame, max_state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                max_num_iter:int=32):\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in range(1, max_state_count):\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(data_set)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "    return variance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_parallel(num:int):\n",
    "    data_set=sample_train_set\n",
    "    max_state_count=state_count\n",
    "    num_loops_per_iter=10000\n",
    "    max_num_iter=clustering_iter\n",
    "    total_needed_runs:List[int] = [i for i in range(1, 751)]\n",
    "    thread_needed_runs:List[int] = np.array_split(total_needed_runs, 24)[num]\n",
    "    # Code for this sample was provided largely by Dr. Giabbanelli \n",
    "    # This makes use of the new curvature method for calculating optimal clusters\n",
    "    # For K-Means sampling\n",
    "    variance_results:List[float] = np.zeros((max_state_count))\n",
    "    for state_count in thread_needed_runs:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(sample_train_set)\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "        cluster_distances = closest_clusters[1]\n",
    "        total_variance = 0\n",
    "        for i in range(0, len(cluster_distances)):\n",
    "            total_variance = total_variance + cluster_distances[i]\n",
    "        variance_results[state_count] = total_variance    \n",
    "        print(f'Finished with {state_count} at a variance of {total_variance}')\n",
    "        single_run = f'{state_count},{total_variance}'\n",
    "        with open('all_cluster_runs.csv', 'a') as f:\n",
    "            print(single_run, file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters_driver(run_optimal_clusters:bool, run_multithread:bool, show_graph:bool) -> int:\n",
    "    # Cluster selection\n",
    "    optimal_cluster_count = 0 \n",
    "    # If we wish to find the optimal number of clusters\n",
    "    if run_optimal_clusters:\n",
    "        # This code is here for posterity, and cannot be run natively in juptyer. Use the stock Python CLI command or \n",
    "        # PyPi to actually run this. Note, it can take forever with large Nodes\n",
    "        if run_multithread:     \n",
    "            pool = multiprocessing.Pool()\n",
    "            pool.map(calculate_optimal_clusters_parallel, range(23))\n",
    "        else:\n",
    "            calculate_optimal_clusters(data_set=sample_train_set, max_state_count=state_count, num_loops_per_iter=10000, \n",
    "                                max_num_iter=clustering_iter)\n",
    "    # Load the data from the cluster variances into here\n",
    "    cluster_results = [0 for i in range(0, 749)]\n",
    "    with open('all_cluster_runs.csv', 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            cluster_results[int(row[0])-1] = float(row[1])\n",
    "    optimal_cluster_count = determine_optimal_clusters(cluster_results, show_graph=show_graph)\n",
    "    return optimal_cluster_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The zscores_for_train_and_test function takes 5 arguments: \n",
    "# data_set:               The training dataset used to generate the various clusters\n",
    "# state_count:            Total number of clusters desired in the end result\n",
    "# num_loops_per_iter:     The number of loops ran per 1 iteration to approximate clusters\n",
    "# max_num_iter:           Number of iterations to run the algorithm\n",
    "# debug_flag:             True indicates print statements are included, False indicates no prints\n",
    "# regenerate_flag:        True indictates the kmeans will be run again. False means the cached/previous file value will be used\n",
    "# \n",
    "# and returns 4 values:\n",
    "# \n",
    "# cluster_values:         A value generated based on the Z-Scores of all the patient's data relative to each other\n",
    "# cluster_labels:         A number generated for the correspond state -> value relationship. Range is 0 to state_count\n",
    "# train_zscores:          The Z-Scores of the training set as created earlier\n",
    "# closest_clusters:       A list representing which state each patient datapoint is closest to\n",
    "# \n",
    "\"\"\"\n",
    "def kmeans_cluster_calculations(train_zscores:pd.DataFrame, test_zscores:pd.DataFrame,\n",
    "                                max_num_iter:int=32, state_count:int=750, num_loops_per_iter:int=10000,\n",
    "                                debug_flag:bool=True, regenerate_flag=False, load_zscores=False) -> (List[List[np.float64]], List[int], pd.DataFrame, \n",
    "                                                                                                     np.ndarray, np.ndarray):\n",
    "    # In order to prepare a proper set of states, we want to use k-means clustering to group various patients into \n",
    "    # distinct states based on Z-Scores\n",
    "\n",
    "    # K-Means or K-Means++ is a technique used to condense very diverse and sparse data into similar groups called 'clusters'\n",
    "    # The K-means algorithm will create k clusters from N data points. In the case of this research,\n",
    "    # the algorithm divides patients into groups that have similar data (age, blood pressure, etc..) and creates a faux 'point'\n",
    "    # at the center of that particular clustering of data\n",
    "\n",
    "    # The KMeans takes three 'settings' arguments\n",
    "    # 1. n_clusters: The number of clusters (later to be used as states), that we desire the algorithm to produce\n",
    "    # this value has been preset to state_count which is 750\n",
    "    # 2. max_iter: How many times each round of k-means clustering will make adjustments, set at 10,000 in my case\n",
    "    # 3. n_init: The number of max_iter batches that will be conducted in a row. The best of these will be chosen\n",
    "    # and saved in the variable clusters_models\n",
    "    cluster_models = []\n",
    "    cluster_labels:List[int] = [] \n",
    "    cluster_values:List[List[np.float64]] = []\n",
    "    if regenerate_flag:\n",
    "        clusters_models = KMeans(n_clusters=state_count, max_iter=num_loops_per_iter, n_init=max_num_iter).fit(train_zscores)\n",
    "        # Save the important data (clusters created as a result of the K-Means operations)\n",
    "        # This process takes quite a while. This will provide a checkpoint to decrease compute time\n",
    "        # until the code is put into dev.\n",
    "        cluster_labels = clusters_models.labels_\n",
    "        cluster_values = clusters_models.cluster_centers_\n",
    "        with open('cluster_labels.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.labels_, fp)\n",
    "        with open('cluster_centers.txt', 'wb') as fp:\n",
    "            pickle.dump(clusters_models.cluster_centers_, fp)\n",
    "        if debug_flag:\n",
    "            print(clusters_models.labels_)\n",
    "            print(clusters_models.cluster_centers_)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    with open ('cluster_centers.txt', 'rb') as fp:\n",
    "        cluster_values = pickle.load(fp)\n",
    "    with open ('cluster_labels.txt', 'rb') as fp:\n",
    "        cluster_labels = pickle.load(fp)\n",
    "    if load_zscores:\n",
    "        with open ('train_zscores.txt', 'rb') as fp:\n",
    "            train_zscores = pickle.load(fp)\n",
    "        with open ('test_zscores.txt', 'rb') as fp:\n",
    "            test_zscores = pickle.load(fp)\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(cluster_values, \"\\n\", \"Dimensions: \", len(cluster_values),\" x \", len(cluster_values[0]), \"\\n\", train_zscores)\n",
    "    \n",
    "    # We now want to use the clusters to determine their nearest real data point neighbors\n",
    "    # As a visual of this. Suppose we have 4 flags of different colors scattered over a park. The K-Means++ algorithm\n",
    "    # is what planted the flags in the middle of groups of people that are similar. The KNN Search (K nearest neighbor search)\n",
    "    # can be used in MatLab as a simple point finder instead of as a more complicated Supervised Learning algorithm. In Python \n",
    "    # we can make use of the Vector Quanization (vq) package to assign each point to a centroid\n",
    "    closest_clusters:np.ndarray = vq(train_zscores, cluster_values)\n",
    "    closest_clusters_test:np.ndarray = vq(test_zscores, cluster_values)\n",
    "    \n",
    "\n",
    "    if debug_flag:\n",
    "        print(len(closest_clusters[0]))\n",
    "\n",
    "    # As an aside, closest_clusters[1] contains the distance between each point's values (in this case 50 of them)\n",
    "    # and their closest cluster's values.\n",
    "    # Ex: If a point is [1, 1, 1] and it's closest cluster is the point [3, 3, 3]  closest_clusters[1] would contain the vector\n",
    "    # [abs(3 - 1), abs(3 - 1), abs(3 - 1)] or [2, 2, 2]\n",
    "\n",
    "    # Validate that all the points are in the range 0-749 (since there are only 750 clusters as specified previously)\n",
    "    for i in closest_clusters[0]:\n",
    "        if(i > (state_count - 1) or i < 0):\n",
    "            print(\"The clusters you are searching for are not configured properly and are out of bounds\")\n",
    "            print(\"Did you modify the cluster_count variable without changing this error configuration?\")\n",
    "    \n",
    "    return cluster_values, cluster_labels, train_zscores, closest_clusters, closest_clusters_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cluster_number(cluster_distances):\n",
    "    total_variance = 0\n",
    "    for i in range(0, len(cluster_distances)):\n",
    "        total_variance = total_variance + cluster_distances[i]\n",
    "    return total_variance\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The generate_action_column function takes 4 arguments: \n",
    "#\n",
    "# column_values: A series of column values from a dataframe that we want to turn into action states\n",
    "# num_groups: How many groups or distinct actions we want to split the data into\n",
    "# column_name: The name of the column used for print debug statements\n",
    "# num_rows: The total number of rows in the full column before modifications (This is normally patientdata[column_name].size)\n",
    "# \n",
    "# This function returns column_actions, a series that represents the 'action', or group that each row of data falls under.\n",
    "#\n",
    "# An example is found down below, but in words, this function takes a full column of data, groups \n",
    "# the values for that data into num_groups distinct actions, and returns a series representing actions based on row\n",
    "# \n",
    "# Ex: Patients' blood pressure might be grouped into 5 categories (Action 1: < 20 mmHg, Action 2: > 20 mmHg && < 60 mmHg... etc)\n",
    "\"\"\"\n",
    "\n",
    "def generate_action_column(column_values:pd.DataFrame, num_groups:int, column_name:str, num_rows:int, debug_flag:bool=True) -> pd.Series:\n",
    "    # Determine minimum and maxium to scale data appropriately\n",
    "    if debug_flag:\n",
    "        print(\"Old Lowest \", column_name, \" Rank: \", min(column_values.rank()))\n",
    "        print(\"Old Highest \" , column_name,  \" Rank: \", max(column_values.rank()))\n",
    "    # Now we want to rank these actions in order of their value (lowest to highest)\n",
    "    # Normalizing according to lowest and highest rank\n",
    "    \n",
    "    # Moving the minimum to zero\n",
    "    column_ranks:pd.Series = (column_values.rank() - min(column_values.rank()))\n",
    "    # Shifting the max to approximately 1.0\n",
    "    column_ranks:pd.Series = column_ranks / max(column_ranks)\n",
    "    \n",
    "    if debug_flag:\n",
    "        # Validate that the range is indeed 0 to 1\n",
    "        print(\"New Lowest \", column_name, \" Rank: \", min(column_ranks))\n",
    "        print(\"New Highest \", column_name, \" Rank: \", max(column_ranks))\n",
    "\n",
    "    # The Max of all column values needs to be nearly 1, and the min of all column\n",
    "    # values needs to be nearly 0 \n",
    "    if round(max(column_ranks), 3) != 1 or round(min(column_ranks), 3) != 0:\n",
    "        print(\"The ranks are not normalized correctly, either the max is too high, or the minium is too low\")\n",
    "        print(\"Current max: \", round(max(column_ranks), 3))\n",
    "        print(\"Curret min: \", round(min(column_ranks), 3))\n",
    "    # Normalize the rank values to even intevals of ranks\n",
    "    old_values:List[float] = np.sort(column_ranks.unique()).tolist()\n",
    "    even_intervals:List[float] = [i/column_ranks.unique().size for i in range(0, column_ranks.unique().size)]\n",
    "    # Iterate over the Series to apply the normalized values\n",
    "    for i in range(0, column_ranks.size):\n",
    "        old_value_index:float = old_values.index(column_ranks[i])\n",
    "        column_ranks[i] = even_intervals[old_value_index] \n",
    "    \n",
    "    # This is a mathematics trick to seperate all the values into {num_groups} distinct groups based on their rank.\n",
    "    # Given different columns of interest this can take different forms. For IV fluids, this number is 5.\n",
    "    column_groups:pd.Series = np.floor(((column_ranks + 1.0/float(num_groups)) * (num_groups - 1))) + 1\n",
    "    \n",
    "    # Validate that groups are all associated with desired group split\n",
    "    if not(column_groups.isin([i for i in range(1, num_groups + 1)]).any()):\n",
    "        print(\"Groups chosen fall outside the desired 1-\" + num_groups + \" window\")\n",
    "\n",
    "    column_actions:pd.Series = pd.Series([1 for i in range(0, num_rows)])\n",
    "\n",
    "    # If the value was non-zero and grouped in the 1 - 4 groups, we grab its value to save as an action\n",
    "    for index in column_groups.index:\n",
    "        column_actions[index] = column_groups[index]\n",
    "        \n",
    "    return column_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This function takes two arguments:\n",
    "# actions_column: A column of action groups generated by the above function (generate_action_column())\n",
    "# real_values: The actual values from the dataset corresponding to the same column as actions_column\n",
    "# and returns a list that contains the real median values for each 'group' actions.\n",
    "#\n",
    "# Ex: We apply the function to the action_column \"IV_Fluid\", which has split the data into 4 different groups of \n",
    "# IV_Fluid actions. This function will produce a list containing the median amount of IV_Fluid administered for each of those\n",
    "# groups (Group 1 -> Adminster 20 mL, Group 2 -> Administer 40 mL, Group 3 -> Administer 60 mL, Group 4 -> Administer 80 mL\n",
    "\"\"\"\n",
    "\n",
    "def median_action_values(actions_column: pd.DataFrame, real_values:pd.DataFrame) -> List[np.float64]:\n",
    "    # Grab all the unique actions for a column and sort them\n",
    "    all_groups:List[np.float64] = np.sort(actions_column.unique())\n",
    "    # Concatanate the group number and real value for each row\n",
    "    action_set:pd.DataFrame = pd.concat([actions_column, real_values], axis=1, sort=False)\n",
    "    # Name the columns for accurate querying\n",
    "    action_set.columns = ['group_id', 'data_val']\n",
    "    # Grab the median value for each group based on group number using python list comprehension\n",
    "    median_values:List[np.float64] = [np.median(action_set[action_set['group_id'] == i]['data_val']) for i in all_groups]\n",
    "    return median_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# This function takes one argument\n",
    "# list_action_columns: This is a Pandas dataframe that contains all the action_columns we desir to be grouped by index\n",
    "# This can be retrieving using the previously defined 'median action' function \n",
    "# \n",
    "# and returns two items:\n",
    "# list_action columns: The 'keys' or integers that represent every permutation of actions\n",
    "# chosen_action: The key that was chosen based on the action values in each column\n",
    "\"\"\"\n",
    "def generate_action_matrix(list_action_columns:pd.DataFrame) -> (List[int], List[int]):\n",
    "    # Grabs the list of columns the user has provided for use\n",
    "    desired_columns:List[str] = [column for column in list_action_columns]\n",
    "    # Drops all group combinations that are duplicates\n",
    "    list_action_columns_indexes:pd.DataFrame = list_action_columns.drop_duplicates(desired_columns)\n",
    "    # Sorts all combinations in order\n",
    "    list_action_columns_indexes = list_action_columns_indexes.sort_values(desired_columns)\n",
    "    # Create a dictionary based on the values from the dataframe \n",
    "    list_action_columns_indexes:List[int] = list_action_columns_indexes.values.tolist() \n",
    "    # Determine which index in the list each row corresponds to \n",
    "    # Ex: For an 2-D action permutation list of [1,1] thru [5,5], there are 5 x 5 possibilities\n",
    "    # {1..5}, {1..5}, so there are 25 possible permutations, the indexes will run 1 - 25\n",
    "    chosen_action:List[int] = [list_action_columns_indexes.index(val_pair) for val_pair in list_action_columns.values.tolist()]\n",
    "    # Return the keys first, and then the true values for the dataset\n",
    "    return list_action_columns_indexes, chosen_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The build_dataset_actions function takes 7 arguments: \n",
    "# patientdata: A Dataframe representing all the patients and their data\n",
    "# first_column: Our first column that we desire to build our action matrix with (first_column x second_column) (SOFA by default)\n",
    "# second_column: Our second column that we desire to build or action matrix with (first_column x second_column) (qSOFA by default)\n",
    "# num_groups_first_column: The number of groups we want to divide our first column into (5 by default)\n",
    "# num_groups_second_column: The number of groups we want to divide our second column into (4 by default)\n",
    "# debug_flag: A boolean that determines whether we want debug prints presents\n",
    "# graph_flag: Whether we want to print the graph or not\n",
    "# \n",
    "# and returns 2 values:\n",
    "# \n",
    "# train_chosen_actions:        The actions for each row of the training dataset\n",
    "# train_action_values:         The matrix representing (first_column x second_column)\n",
    "# action_list:                 A list of actions taken at every given datapoint\n",
    "\"\"\"\n",
    "def build_dataset_actions(patientdata:pd.DataFrame, first_column:str=\"SOFA\", second_column:str=\"qSOFA\", num_groups_first_column:int=5,\n",
    "                         num_groups_second_column:int=4, debug_flag:bool=True, graph_flag:bool=True,\n",
    "                         train_flag:pd.Series=[]) -> (pd.Series, List[List[int]], List[int]):\n",
    "    # Generate the actions for a the first desired column of data\n",
    "    first_col:pd.DataFrame = patientdata[first_column]\n",
    "    first_col_actions:pd.Series = generate_action_column(column_values = first_col, num_groups = num_groups_first_column, \n",
    "                                                        column_name = first_column, \n",
    "                                                         num_rows = patientdata[first_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(first_col_actions.unique())    \n",
    "    # Now do the same for the second desired column of data\n",
    "    second_col:pd.DataFrame = patientdata[second_column]\n",
    "    second_col_actions:pd.Series = generate_action_column(column_values = second_col, num_groups = num_groups_second_column,  \n",
    "                                                         column_name = second_column, \n",
    "                                                         num_rows = patientdata[second_column].size, debug_flag=debug_flag)\n",
    "    if debug_flag:\n",
    "        print(second_col_actions.unique())\n",
    "    \n",
    "    # Obtain the median real values that each division represents\n",
    "    first_median_actions:List[np.float64] = median_action_values(actions_column = first_col_actions, real_values = patientdata[first_column])\n",
    "    second_median_actions:List[np.float64] = median_action_values(actions_column = second_col_actions, real_values = patientdata[second_column])\n",
    "    \n",
    "    if debug_flag:\n",
    "        print(first_column,\" Action Median Values:\", str(first_col_actions), \"\\n\" + second_column + \":\", second_median_actions, \"\\n\")\n",
    "    ###\n",
    "    # FINISH CONSTRUCTION OF ALL ACTIONS AND THEIR VALUES\n",
    "    ###\n",
    "    # Combine the columns that we desire to observe (iv_fluid_actions, vasopressor_actions)\n",
    "    combined_groups:pd.DataFrame = pd.concat([first_col_actions, second_col_actions], axis=1, sort=False)\n",
    "    # Name the columns for proper usage in the function\n",
    "    combined_groups.columns = [first_column, second_column]\n",
    "    \n",
    "    # The Key value pair for every datapoint and the corresponding action taken at that point\n",
    "    action_keys, action_list = generate_action_matrix(list_action_columns = combined_groups)\n",
    "    # Plot the distribution of actions\n",
    "    if graph_flag:\n",
    "        plt.hist(action_list, density=False, bins=20)  # `density=False` would make counts\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Index of Action Chosen: 1 through 24\")\n",
    "    # Grab a Series representing the action taken by the train data only\n",
    "    train_chosen_actions:pd.Series = pd.Series(action_list)[train_flag]\n",
    "    # Grab a Series representing the action taken by the test data only\n",
    "    test_chosen_actions:pd.Series = pd.Series(action_list)[test_flag]\n",
    "\n",
    "    # Assign all action choices to their corresponding median values as shown previously\n",
    "    if debug_flag:\n",
    "        print(first_median_actions, second_median_actions)\n",
    "\n",
    "\n",
    "    # This gives us the representative median values for a patient's vitals present in various action groups\n",
    "    # action_keys[i] corresponds to train_action_values[i]\n",
    "    # So, if the patient falls into group [1, 1] or no iv fluid given, no vasopressor administered,\n",
    "    # The corresponding median values for this group will be represented by train_action_values (0.0, 0.0).\n",
    "    # A patient in group [1, 2] (no iv fluid, a little vasopressor) will have a median real value of (0.0, 0.04)\n",
    "    action_values_matrix:List[List[int]] = list(cartesian_prod(first_median_actions, second_median_actions))\n",
    "\n",
    "    if len(action_values_matrix) != len(first_median_actions) * len(second_median_actions):\n",
    "        print(\"Something went wrong in determining the Cartesian product\")\n",
    "    \n",
    "    return train_chosen_actions, test_chosen_actions, action_values_matrix, action_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The construct_prestate_matrix_train function takes 5 arguments: \n",
    "# train_90d:              A column representing whether or not a patient was dead or alive at the end of 90days\n",
    "# train_blocs:            All the rows of data for a patient for each individual hospital stay\n",
    "# closest_clusters:       The closest data cluster that a given data point falls near\n",
    "# train_chosen_actions:   The action that is represented by two of the patient's characteristics, calculated previously\n",
    "# is_debug:               Whether or not statements are printed over time.\n",
    "# \n",
    "# and returns 1 value:\n",
    "# \n",
    "# qlearning_dataset_mod: The full training dataset configured to be converted into states and actions\n",
    "# all_lower_ranges:      The lower bound for the reward values\n",
    "# all_upper_ranges:      The upper bound for the reward values\n",
    "\"\"\"\n",
    "def construct_prestate_matrix_train(train_90d:pd.DataFrame, train_blocs:pd.DataFrame, closest_clusters:List[List[int]],\n",
    "                                    train_chosen_actions:pd.DataFrame, train_id_list:pd.DataFrame, \n",
    "                                    is_debug:bool = True, action_count:int = 20, \n",
    "                                    state_count:int = 750) -> (pd.DataFrame, List[int], List[int]):\n",
    "    ###\n",
    "    # BEGIN CONSTRUCTION OF PRE-STATE MATRIX\n",
    "    # This will be used to build the full state/action matrix\n",
    "    ### \n",
    "\n",
    "    # Based on whether or not a patient is dead, we establish the range of possible values:\n",
    "    # If they have died, the range is [-100, 100]\n",
    "    # If they are alive, the range is [100, -100]\n",
    "    range_vals:List[int] = [100, -100]\n",
    "    # Convert the range of values for a patient's status (dead or alive) from 0 or 1 to -1 or 1\n",
    "    # This will enable ranges to suit the above criteria [-100, 100] or [100, -100]\n",
    "    train_90d_polarity:List[int] = (2 * (1 - train_90d) - 1)\n",
    "    range_matrix:List[int] = [np.multiply(polarity, range_vals) for polarity in train_90d_polarity]\n",
    "    # Grab the lower range limit and upper range limit seperately in order to build the\n",
    "    # full range of reward values\n",
    "    all_lower_ranges:List[int] = [i[0] for i in range_matrix]\n",
    "    all_upper_ranges:List[int] = [i[1] for i in range_matrix]\n",
    "        \n",
    "    # The qlearning_dataset prior to modification contains 6 columns and ~190885 rows (around 75% of the data)\n",
    "    # The columns are as follows:\n",
    "    #\n",
    "    # training_bloc: time_series stamps for a patient's state over time, very in range from {1..?}\n",
    "    #\n",
    "    # closest_cluster_index: The index of the nearest cluster to the z-scores of the patient's data, \n",
    "    # corresponding actual data for each cluster's index (i) can be found in cluster_values[i]\n",
    "    #\n",
    "    # chosen_action_index: The chosen action or representation of a patient's IV_Fluid and Vasopressor status [0 - 24]\n",
    "    # \n",
    "    # 90d_mortality_status: 0 means the patient is alive 90 days after discharge from ICU\n",
    "    #                      1 means the patient is dead  90 days after discharge from ICU\n",
    "    #\n",
    "    # lower_range + upper_range: An index to be used later on, gathered from the range index\n",
    "    if is_debug:\n",
    "        print(\"Training Blocs Length: \", str(len(train_blocs)), \"\\nClosest Clusters Length: \", str(len(closest_clusters[0])), \n",
    "              \"\\nAction List Length: \", str(len(train_chosen_actions)), \"\\nTrain 90d Length\", str(len(train_90d)), \n",
    "              \"\\nRange Matrix Length: \", len(range_matrix), \"\\nTrain IDs Length\", str(len(train_id_list)))\n",
    "    qlearning_dataset:pd.DataFrame = pd.concat([pd.Series(train_blocs.tolist()), \n",
    "                                   pd.Series(closest_clusters[0]), \n",
    "                                   pd.Series(train_chosen_actions.tolist()), \n",
    "                                   pd.Series(all_lower_ranges),\n",
    "                                   pd.Series(train_90d.tolist()),\n",
    "                                   train_id_list], \n",
    "                                   axis=1, sort=False)\n",
    "    qlearning_dataset.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', '90d_mortality_status', 'patient_id']\n",
    "    if is_debug: \n",
    "        print(qlearning_dataset)\n",
    "    # Modify the set for the final time in order to construct the final life + death states for each patient\n",
    "    qlearning_dataset_mod = modify_qlearning_dataset(ql_dataset = qlearning_dataset)# Print some important details of the set\n",
    "    if is_debug:\n",
    "        # Total patients being observed in the test\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['training_bloc'] == 1]['training_bloc']))\n",
    "        # Show that we now have end states established \n",
    "        print(len(qlearning_dataset[qlearning_dataset['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['chosen_action_index'] == action_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] == state_count]['chosen_action_index']))\n",
    "        print(len(qlearning_dataset_mod[qlearning_dataset_mod['closest_cluster_index'] ==  state_count + 1]['chosen_action_index']))\n",
    "        print(qlearning_dataset_mod, \"\\n\")\n",
    "    return qlearning_dataset_mod, all_lower_ranges, all_upper_ranges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# modify_qlearning_dataset is a function that takes a dataframe intended for qlearning and modifies\n",
    "# it in preparation for the ML. In essence, it runs through creating the life and and death states in preparation\n",
    "# for constructing the MDP\n",
    "#\n",
    "# Parameters: \n",
    "# ql_dataset:  The Dataset we want to modify in preparation\n",
    "# is_training: Whether we are building the training set or not.\n",
    "# is_debug:    Whether prints are included or not\n",
    "#       \n",
    "# \n",
    "# Returns: qlearning_dataset_mod - The modified dataset\n",
    "\"\"\"\n",
    "\n",
    "def modify_qlearning_dataset(ql_dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    # The base qlearning_dataset does not account for endpoints in either life or death\n",
    "    # These states have not been established yet, which is what this step corrects\n",
    "    qlearning_dataset:pd.DataFrame = ql_dataset.copy()\n",
    "    qlearning_dataset_len:int = len(qlearning_dataset.index)\n",
    "    # We need space to add a death/life state for every patient, about a 20% increase in size from the original MDP\n",
    "    # We will cut the excess off by the end of the loop\n",
    "    qlearning_dataset_len_mod:float = int(np.floor(qlearning_dataset_len * 1.2))\n",
    "    qlearning_dataset_mod:np.ndarray = []\n",
    "    qlearning_dataset_mod:np.ndarray = np.array([[0 for i in range(0, 6)] for i in range(0, qlearning_dataset_len_mod)])\n",
    "    # Start construction of modified data\n",
    "    row:int = 0\n",
    "    # In Markov theory, an absorbing state is one which can be entered, but cannot be left. (Similar to the Hotel California)\n",
    "    # In the case of this experiment, those states are either life (state_count) or death (state_count + 1) per patient as\n",
    "    # defined by me (750, 751)\n",
    "    absorbing_states:List[int] = [state_count, state_count + 1]\n",
    "\n",
    "    # Start the loop to begin capping the markov chain off at life and death states\n",
    "    for i in range(0, qlearning_dataset_len - 1):\n",
    "        # Use the already gathered data for each row\n",
    "        qlearning_dataset_mod[row, :] = qlearning_dataset.iloc[i][0:6]\n",
    "        # If we arrive at the terminal point (end of patient data), we need to point the MDP to either the death or life state\n",
    "        if qlearning_dataset.iloc[i + 1]['training_bloc'] <= qlearning_dataset.iloc[i]['training_bloc']:\n",
    "            # Grab the row\n",
    "            whole_row:pd.DataFrame = qlearning_dataset.iloc[i]\n",
    "            # Set most of the row to the original data's values, except set the action to be either state 750 or 751\n",
    "            # Life or death respectively\n",
    "            row = row + 1\n",
    "            # We need bloc number, final state (life or death, 750 or 751), end action (-1), and the reward value (lower_range)\n",
    "            qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status'], whole_row['patient_id']]\n",
    "        row = row + 1\n",
    "    # Add in the last row\n",
    "    whole_row:pd.DataFrame = qlearning_dataset.iloc[len(qlearning_dataset.index) - 1]\n",
    "    qlearning_dataset_mod[row, :] = [whole_row['training_bloc'] + 1, absorbing_states[whole_row['90d_mortality_status']], -1,  whole_row['reward_value'], whole_row['90d_mortality_status'], whole_row['patient_id']]\n",
    "    \n",
    "    row = row + 1\n",
    "    # Get rid of the unneeded rows\n",
    "    qlearning_dataset_mod:pd.DataFrame = pd.DataFrame(qlearning_dataset_mod[0:row, :])\n",
    "    qlearning_dataset_mod.columns = ['training_bloc', 'closest_cluster_index', 'chosen_action_index', 'reward_value', 'death_state', 'patient_id']\n",
    "    # Set all rows not in the terminal states to 0 reward to start\n",
    "    qlearning_dataset_mod.loc[qlearning_dataset_mod['chosen_action_index'] != -1,'reward_value'] = 0\n",
    "    return qlearning_dataset_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Now that we officially have some a valid bloc for actions, and a valid set of states, it's time \n",
    "#  to begin building the transitions matrix.\n",
    "###\n",
    "\n",
    "### If the matrix is bidirectional (S1 -> S2, S2 -> S1 are both valid, we can build two matrices)\n",
    "\n",
    "### \n",
    "# The MDP Toolbox we are going to be using requires Transition and Reward Matrices to be in the form\n",
    "# M(action, state1, state2)\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "# The create_transition_matrix method takes 4 arguments:\n",
    "# num_actions: The total number of possible actions (calculated by action_count ^ 2 or in py, action_count ** 2)\n",
    "# num_states:  Number of states the model uses\n",
    "# qlearning_dataset: The dataset that will be used for the qlearning process\n",
    "# transition_threshold: How many actions do we want to deem as scarce and not worth keeping (default = 5)\n",
    "# reverse: If false, the matrix that is created is represented as transition[A][S1][S2], if true: transition[A][S2][S1]\n",
    "# \n",
    "# and returns 2 values:\n",
    "# transition_matrix: The counts of which actions were chosen in which states\n",
    "# physician_policy:  The transition_matrix that has been turned into probabilties by dividing counts in each state by \n",
    "# total counts\n",
    "# \n",
    "\"\"\"\n",
    "def create_transition_matrix(num_actions:int, num_states:int, ql_data_input:pd.DataFrame, \n",
    "                             transition_threshold:int = 5, reverse:bool = False) -> (int_matrix2D, float_matrix2D):\n",
    "    # The transition matrix is a 3D construct, involving a transition between two states\n",
    "    # and an action. The dimensions for the matrix are (state_count * 2) * (state_count + 2) * action_count\n",
    "    transition_matrix:float_matrix2D = [[[0 for i in range(0, num_states + 2)] for i in range(0, num_states + 2)] for i in range(0, num_actions)]\n",
    "    # NP Arrays allow for more compact and efficient slicing\n",
    "    transition_matrix = np.array(transition_matrix).astype(float)\n",
    "    # We also need a matrix to denote the policy that corresponds with taken a particular action from a state\n",
    "    transition_policy_count:int_matrix2D = [[0 for i in range(0, num_states + 2 )] for i in range(0, num_actions)]\n",
    "    transition_policy_count = np.array(transition_policy_count).astype(float)\n",
    "    # Iterate over the actual data in order to form the actual states and their corresponding actions\n",
    "    # As soon as we hit the next patient (the next row has a training bloc value of 1), we stop processing actions for that patient\n",
    "    for i in range(0, len(ql_data_input) - 1):\n",
    "        # Since 1 is our 'endpoint' for each patient, there are no actions we can take from this point on\n",
    "        if ql_data_input.iloc[i + 1]['training_bloc'] > ql_data_input.iloc[i]['training_bloc']:\n",
    "            S1:int = ql_data_input.iloc[i]['closest_cluster_index']\n",
    "            S2:int = ql_data_input.iloc[i + 1]['closest_cluster_index'] \n",
    "            action_id:int = ql_data_input.iloc[i]['chosen_action_index']\n",
    "            if not(reverse):\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S1][S2] = transition_matrix[action_id][S1][S2] + 1\n",
    "            else:\n",
    "                # Count the number of times S1 -> S2 is taken using action A\n",
    "                transition_matrix[action_id][S2][S1] = transition_matrix[action_id][S2][S1] + 1\n",
    "                \n",
    "            # Count the number of times action A is used to transition from S1\n",
    "            transition_policy_count[action_id][S1] = transition_policy_count[action_id][S1] + 1        \n",
    "\n",
    "    # In order to avoid drastically altering our model, we fix a constant\n",
    "    # value (set by default to 5), in order to declare sparse actions \n",
    "    # as essentially not happening (make their count 0)\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if transition_policy_count[i][j] <= transition_threshold:\n",
    "                transition_policy_count[i][j] = 0 \n",
    "    # Now, we want to prevent transitions from state -> state using\n",
    "    # a certain action if that action is sparse or nonexistant\n",
    "    for i in range(0, num_actions):\n",
    "        for j in range(0, num_states + 2):\n",
    "            if not(reverse):\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,j,:] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,j,:] = transition_matrix[i,j,:]/np.float64(transition_policy_count[i][j])\n",
    "            else:\n",
    "                # Declare the weight of an unachievable action to have a zero probability\n",
    "                if transition_policy_count[i][j] == 0:\n",
    "                    transition_matrix[i,:,j] = 0\n",
    "                    # All probabilities must be declared, even unreachable states, an easy work around \n",
    "                    # to this issue is to simply declare the same state to have a probability of 1\n",
    "                    # https://stackoverflow.com/questions/43665797/must-a-transition-matrix-from-a-markov-decision-process-be-stochastic\n",
    "                    transition_matrix[i,j,j] = 1\n",
    "                # This weights the MDP based on the probability of taking one action from a state\n",
    "                # As opposed to taking any other possible action from that state\n",
    "                # S1 -> S2 might be 50%, S1 -> S3 20%, and S1 -> S4 30%\n",
    "                else:\n",
    "                    transition_matrix[i,:,j] = transition_matrix[i,:,j]/np.float64(transition_policy_count[i][j])\n",
    "    \n",
    "    # Ensure no divisions create NaNs or infinities\n",
    "    transition_matrix = np.nan_to_num(transition_matrix)\n",
    "    # Determine the phyisican's policy based on total count\n",
    "    # This comes in handy later when comparing model ability\n",
    "    total_transitions:float = sum(transition_policy_count)\n",
    "    physician_policy:float_matrix2D = np.divide(transition_policy_count, total_transitions)\n",
    "    return transition_matrix, physician_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# offpolicy_Q_learning_eval is a method that takes 6 arguments and returns 2 items\n",
    "# This method evaluates the performance of the MDP determined previously\n",
    "# \n",
    "# Parameters:\n",
    "# ql_train_set_Q: The actual dataset that serves as our proto-MDP\n",
    "# phys_pol: A 2D (actions X states) matrix that shows what the phyisican chose according dataset probabilities\n",
    "# gamma: A hyperparameter for determining how much we value previous data\n",
    "# alpha: A hyperparameter that weights our reward function at each step\n",
    "# numtraces: Number of Q-Learning iterations we would like to perform\n",
    "# num_actions: Total number of actions in the set (For Sepsis: 25)\n",
    "# num_clusters: Total number of states in the set (For Sepsis: 752)\n",
    "# is_training:  If False, this is the first phase (construction of the model). If True, the model is being Trained\n",
    "# \n",
    "# Returns:\n",
    "# Q_Equation = The set of Q-Values obtained by the algorithm's performance\n",
    "# sum_Q_values = The Q-Equation's performance at a given step in the algorithm\n",
    "\"\"\"\n",
    "def offpolicy_Q_learning_eval(ql_train_set_Q: pd.DataFrame, death_answer_key:List[int],gamma: float, alpha: float, \n",
    "                              numtraces: int, num_actions: int, \n",
    "                              num_clusters: int, stopping_difference: float, \n",
    "                              is_training=False, is_random=False,\n",
    "                              modulus_val:int=5000) -> (float_matrix2D, List[float]):\n",
    "    # We need to save the Q-value for each run \n",
    "    sum_Q_values:List[float] = np.zeros((numtraces))\n",
    "    # Where the Q-Values are saved at each given run\n",
    "    Q_Equation:int_matrix2D = np.zeros((num_actions, num_clusters))\n",
    "    # We need to save the Average Q value after so many iterations\n",
    "    previous_avg_Q:int = 0\n",
    "    # The list of all starting patient states \n",
    "    first_index_list:List[int] = ql_train_set_Q[ql_train_set_Q['training_bloc'] == 1].index\n",
    "    # A seperate index running in parallel with i for the sum_Q_values\n",
    "    jj:int = 0\n",
    "    # If we don't want a random data set from the existant set, only do each data point once\n",
    "    if not(is_random):\n",
    "        numtraces = len(first_index_list)\n",
    "    # We iterate for the total number of times we want to do this process\n",
    "    for i in range(0, numtraces):\n",
    "        # Select a random patient starting point from the data\n",
    "        patient_starter_index:int = 0\n",
    "        if is_random:\n",
    "            patient_starter_index:int = random.choice(first_index_list)\n",
    "        else:\n",
    "            patient_start_index = first_index_list[i]\n",
    "        # As Q-learning progreses, we need a data structure to track progress\n",
    "        full_trace:List[Tuple[float, int, int]] = []\n",
    "        # While we are still working on a single patient\n",
    "        num_ql_rows = len(ql_train_set_Q.index)\n",
    "        # We run until we hit the end of the patient or the end of the dataset\n",
    "        while patient_starter_index + 1 < num_ql_rows and ql_train_set_Q.iloc[patient_starter_index + 1]['training_bloc'] != 1:\n",
    "            # Grab state (Initial State at this point)\n",
    "            state_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['closest_cluster_index']\n",
    "            # Grab action taken from this point\n",
    "            action_index:int = ql_train_set_Q.iloc[patient_starter_index + 1]['chosen_action_index']\n",
    "            # Grab reward provided by taken an action from this state to the next\n",
    "            reward_value:float = ql_train_set_Q.iloc[patient_starter_index + 1]['reward_value']\n",
    "            # A 'step' in the trace, a single data point snapshot\n",
    "            trace_step:Tuple[float, int, int] = (reward_value, state_index, action_index)\n",
    "            # Add the step to the full trace\n",
    "            full_trace.append(trace_step)\n",
    "            # Increment the current data row\n",
    "            patient_starter_index = patient_starter_index + 1\n",
    "        # Full length of the trace path\n",
    "        trace_length:int = len(full_trace)\n",
    "        return_reward:float = ql_train_set_Q.iloc[patient_starter_index]['reward_value']\n",
    "        # Grab the final reward (final reward for last state)\n",
    "        if is_training:\n",
    "            # If the model guessed correctly and the patient died, weight the path -100 \n",
    "            if (reward_value < 0 and answer_key['death_state'].iloc[patient_starter_index] == 1):\n",
    "                return_reward = -100\n",
    "            # If the model guessed correctly and the patient lived, weight the path +100\n",
    "            elif (reward_value >= 0 and answer_key['death_state'].iloc[patient_starter_index] == 0):\n",
    "                return_reward = 100\n",
    "            # If the model guessed incorrectly and the patient lived, weight the path +100\n",
    "            elif (reward_value < 0 and answer_key['death_state'].iloc[patient_starter_index] == 0):\n",
    "                return_reward = 100\n",
    "            # If the model guessed incorrectly and the patient died, weight the path -100\n",
    "            elif (reward_value >= 0 and answer_key['death_state'].iloc[patient_starter_index] == 1):\n",
    "                return_reward = -100\n",
    "        # Walk the trace stack backwards\n",
    "        for j in range(trace_length - 2, -1, -1):\n",
    "            # Grab the state, action, and reward at each step\n",
    "            step_state:int = full_trace[j][1]\n",
    "            step_action:int = full_trace[j][2]\n",
    "            # Using alpha blending (where we take a portion of the old value and blend it with the new)\n",
    "            # We blend part of the old value of this state with the new value\n",
    "            Q_Equation[step_action, step_state] = (1 - alpha) * Q_Equation[step_action, step_state] + alpha * return_reward\n",
    "            # Cap the range for node values (-100, 100)\n",
    "            if Q_Equation[step_action, step_state] > 100:\n",
    "                Q_Equation[step_action, step_state] = 100\n",
    "            if Q_Equation[step_action, step_state] < -100:\n",
    "                Q_Equation[step_action, step_state] = -100\n",
    "            # Recall we have a gamma value to determine the impact of previous decisions on future ones\n",
    "            # Note: this is a Hyperparameter (a parameter on the model itself)\n",
    "            return_reward = return_reward * gamma  + full_trace[j][0]\n",
    "        # Save the overall value based on the current states and actions avaiable at the \n",
    "        # current iteration\n",
    "        sum_Q_values[jj] = np.sum(Q_Equation)\n",
    "        jj = jj + 1\n",
    "        # If we haven't hit our max iterations, we still want to see if we should keep pushing forward\n",
    "        # If there is no noticable progress, we want to stop\n",
    "        \n",
    "        # This is only applicable if we are not using all the training data set patients\n",
    "        if is_random:\n",
    "            # Perform a check every modulus_val runs\n",
    "            if i % modulus_val == 0:\n",
    "                # Grab the current slice of unchecked {modulus_val} values\n",
    "                slice_mean:float = np.mean(sum_Q_values[j - modulus_val:j])\n",
    "                # Calculate the difference between current and last average \n",
    "                max_difference:float =(slice_mean - previous_avg_Q)/previous_avg_Q\n",
    "                # Check if the average of this {modulus_val} values is less than 0.001 away from the previous\n",
    "                if abs(max_difference) < stopping_difference:\n",
    "                    break\n",
    "                previous_avg_Q = slice_mean\n",
    "            \n",
    "    # Trim off the portion of the list we did not use\n",
    "    sum_Q_values = sum_Q_values[0:jj]\n",
    "    return Q_Equation, sum_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def construct_trained_model(ql_final_dataset:pd.DataFrame, state_count:int, \n",
    "                            total_actions:int, weighted_probabilities:bool, \n",
    "                            physician_policy:List[List[float]], Q_Equation:List[List[float]]) -> pd.DataFrame:\n",
    "    # Make a final duplicate of the data\n",
    "    qlearning_train_final:pd.DataFrame = ql_final_dataset.copy()\n",
    "    \n",
    "    # If weights are considered, use the weighted rewards from the Q_Equation, otherwise \n",
    "    # use the Q_Equation as is \n",
    "    if weighted_probabilities:\n",
    "        # Weight Q-Value rewards according to their frequency of occuring\n",
    "        # The Q-Equation for a given state + action pair is equivalent to the reward value\n",
    "        # The Phyiscian Policy is the probability of that action ocurring given a state\n",
    "        # These weights prevent rare events from having massively scewed rewards\n",
    "        # For example, a path that occurs through a given state exactly once would have a much larger\n",
    "        # reward than a frequently traveled path, which could scew the data.\n",
    "        value_matrix = np.zeros((state_count, total_actions))\n",
    "        for i in range(0, state_count):\n",
    "            for j in range(0, total_actions):\n",
    "                value_matrix[i][j] = physician_policy[j][i] * Q_Equation[j][i]\n",
    "        for i in range(0, len(qlearning_train_final.index)):\n",
    "            row = qlearning_train_final.iloc[i]\n",
    "            if((row['closest_cluster_index'] != state_count) and (row['closest_cluster_index'] != state_count + 1)):\n",
    "                row['reward_value'] = value_matrix[row['closest_cluster_index']][row['chosen_action_index']]\n",
    "    else:\n",
    "        for i in range(0, len(qlearning_train_final.index)):\n",
    "            row = qlearning_train_final.iloc[i]\n",
    "            if((row['closest_cluster_index'] != state_count) and (row['closest_cluster_index'] != state_count + 1)):\n",
    "                row['reward_value'] = Q_Equation[row['chosen_action_index']][row['closest_cluster_index']]\n",
    "    \n",
    "    return qlearning_train_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(qlearning_train_final:pd.DataFrame, test_name:str):\n",
    "    all_patients_ids = qlearning_train_final['patient_id'].drop_duplicates().tolist()\n",
    "    # Iterate until the full set is done\n",
    "    total_patients:int = len(all_patients_ids)\n",
    "    unique_total:pd.DataFrame = qlearning_train_final[['patient_id', 'death_state']].drop_duplicates()\n",
    "    total_alive:int = unique_total['death_state'].value_counts()[0]\n",
    "    total_dead:int = unique_total['death_state'].value_counts()[1]\n",
    "    correct_guesses:int = 0\n",
    "    dead_instead_live:int = 0\n",
    "    live_instead_dead:int = 0\n",
    "    zero_valued_reward:int = 0\n",
    "    for id_val in all_patients_ids:\n",
    "        all_rewards:pd.Series = qlearning_train_final[qlearning_train_final['patient_id'] == id_val]['reward_value']\n",
    "        # We are not supposed to know the terminal reward for evaluation, this is the life/death state\n",
    "        all_rewards:pd.Series = all_rewards[:-1]\n",
    "        # Sum the total reward value for a given column, determine the sign value\n",
    "        reward_sum:float = np.sign(np.sum(all_rewards))\n",
    "        # Treat 0s as Living\n",
    "        if reward_sum == 0:\n",
    "            zero_valued_reward = zero_valued_reward + 1\n",
    "            reward_sum = 1\n",
    "        # Grab the death state of the patient\n",
    "        death_state = qlearning_train_final[qlearning_train_final['patient_id'] == id_val]['death_state'].tolist()[0]\n",
    "        # Change Values to match reward system 0 -> +1 -> Patient Lived, 1 -> -1 -> Patient Died\n",
    "        PATIENT_LIVED = 0\n",
    "        PATIENT_DIED = 1\n",
    "        if death_state == PATIENT_LIVED:\n",
    "            death_state = 1\n",
    "        else:\n",
    "            death_state = -1\n",
    "        # If the prediction and actual values matched, the model predicted correctly\n",
    "        if reward_sum == death_state:\n",
    "            correct_guesses = correct_guesses + 1\n",
    "        # If the patient was presumed to be dead and lived, count it\n",
    "        elif reward_sum == -1:\n",
    "            live_instead_dead = live_instead_dead + 1\n",
    "        # If the patient was presumed to be alive and died, count it\n",
    "        else:\n",
    "            dead_instead_live = dead_instead_live + 1\n",
    "    print(\"Test Name: \" + test_name)\n",
    "    print(\"Accuracy: \" + str(correct_guesses/total_patients))\n",
    "    print(\"Living People Guessed Dead: \" + str(dead_instead_live))\n",
    "    print(\"Dead People Guessed Living: \" + str(live_instead_dead))\n",
    "    print(\"Total Guesses: \" + str(total_patients))\n",
    "    print(\"Alive People: \" + str(total_alive))\n",
    "    print(\"Dead People: \" + str(total_dead))\n",
    "    print(\"Correct Guesses: \" + str(correct_guesses))\n",
    "    print(\"Empty Paths: \" + str(zero_valued_reward))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Train, Weighted\n",
      "Accuracy: 0.43948267447535383\n",
      "Living People Guessed Dead: 3590\n",
      "Dead People Guessed Living: 1004\n",
      "Total Guesses: 8196\n",
      "Alive People: 4098\n",
      "Dead People: 4098\n",
      "Correct Guesses: 3602\n",
      "Empty Paths: 6684\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_accuracy(qlearning_train_final=qlearning_train_final, test_name=\"Train, Weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PCA is a technique to perform dimensionality reduction in large featured datasets (I.E)\n",
    "# \n",
    "# \n",
    "# \n",
    "\"\"\"\n",
    "def pca_variance_analysis(MIMIC_zscores:pd.DataFrame, variance_allowed:float, is_debug:bool, num_components:int) -> float_matrix2D:\n",
    "    if num_components == -1:\n",
    "        model = PCA()\n",
    "        model.fit(MIMIC_zscores.values)\n",
    "        total_variances = np.cumsum(model.explained_variance_ratio_)\n",
    "        num_components = 0\n",
    "        for variance in total_variances:\n",
    "            if variance < variance_allowed:\n",
    "                num_components = num_components + 1\n",
    "        if is_debug: \n",
    "            print(\"Number of Components Chosen: \", num_components)\n",
    "    real_model = PCA(n_components=num_components)\n",
    "    pca_dataset = real_model.fit_transform(MIMIC_zscores.values)\n",
    "    return pca_dataset, num_components\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Train, Weighted\n",
      "Accuracy: 0.439551055264121\n",
      "Living People Guessed Dead: 1004\n",
      "Dead People Guessed Living: 3590\n",
      "Total Guesses: 8197\n",
      "Alive People: 4098\n",
      "Dead People: 4098\n",
      "Correct Guesses: 3603\n",
      "Empty Paths: 6685\n",
      "\n",
      "\n",
      "Test Name: Train, No Weighting\n",
      "Accuracy: 0.439551055264121\n",
      "Living People Guessed Dead: 1004\n",
      "Dead People Guessed Living: 3590\n",
      "Total Guesses: 8197\n",
      "Alive People: 4098\n",
      "Dead People: 4098\n",
      "Correct Guesses: 3603\n",
      "Empty Paths: 6685\n",
      "\n",
      "\n",
      "Test Name: Test, Weighted\n",
      "Accuracy: 0.6207966890843248\n",
      "Living People Guessed Dead: 336\n",
      "Dead People Guessed Living: 397\n",
      "Total Guesses: 1933\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Correct Guesses: 1200\n",
      "Empty Paths: 1538\n",
      "\n",
      "\n",
      "Test Name: Test, No Weighting\n",
      "Accuracy: 0.6207966890843248\n",
      "Living People Guessed Dead: 336\n",
      "Dead People Guessed Living: 397\n",
      "Total Guesses: 1933\n",
      "Alive People: 1476\n",
      "Dead People: 456\n",
      "Correct Guesses: 1200\n",
      "Empty Paths: 1538\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # First Round of Functions\n",
    "    colbin, colnorm, collog, MIMIC_raw, id_count, icu_ids, patient_idxs = extract_init_column_data(patient_data=patientdata, debug_flag=False)\n",
    "    MIMIC_zscores:pd.DataFrame = construct_zscores(colbin, colnorm, collog, MIMIC_raw, debug_flag=False) \n",
    "    train_ids_set, test_ids_set, train_flag_set, test_flag_set = cross_validate_and_balance(icu_ids, debug_flag=False, \n",
    "                                                                             save_to_file=True, regenerate_flag=True, \n",
    "                                                                             patientdata=patientdata, regenerate_cross=True)\n",
    "    # If this line provides trouble, re-run with regenerate_flag set to false after it is set to true\n",
    "    train_flag = train_flag_set[0]\n",
    "    test_flag = test_flag_set[0]\n",
    "    \n",
    "    train_zscores, test_zscores, train_blocs, test_blocs, train_id_list, test_id_list, train_90d, test_90d = \\\n",
    "    zscores_for_train_and_test(train_flag, test_flag, MIMIC_zscores, debug_flag=False, save_flag=True, \n",
    "                               patient_data=patientdata, bloc_name='bloc', id_name='icustayid', death_name='mortality_90d')\n",
    "    train_zscores_pca, num_train_comp = pca_variance_analysis(MIMIC_zscores=train_zscores, \n",
    "                                                   variance_allowed=0.80, is_debug=False, num_components=-1)\n",
    "    test_zscores_pca, _ = pca_variance_analysis(MIMIC_zscores=test_zscores, variance_allowed=0.80, \n",
    "                                             is_debug=False, num_components=num_train_comp)\n",
    "    optimal_cluster_count = calculate_optimal_clusters_driver(run_optimal_clusters=False, run_multithread=False, show_graph=False)\n",
    "    optimal_cluster_count = 75\n",
    "    cluster_values, cluster_labels, train_zscores_pca, closest_clusters, closest_clusters_test = \\\n",
    "    kmeans_cluster_calculations(train_zscores=train_zscores_pca, test_zscores=test_zscores_pca,\n",
    "                                max_num_iter=clustering_iter, state_count=optimal_cluster_count, num_loops_per_iter=10000,\n",
    "                                debug_flag=False, regenerate_flag=False, load_zscores=False)\n",
    "    # Training and Testing Construction\n",
    "    train_chosen_actions, test_chosen_actions, action_values_matrix, action_list = build_dataset_actions(patientdata=patientdata, \n",
    "                      first_column=\"SOFA\", second_column=\"qSOFA\", \n",
    "                      num_groups_first_column=5, num_groups_second_column=4, \n",
    "                      debug_flag=False, graph_flag=False, train_flag=train_flag)\n",
    "    \n",
    "    qlearning_dataset_train_final, all_upper_ranges, all_lower_ranges = construct_prestate_matrix_train(train_90d = train_90d, \n",
    "                                                                                            train_blocs = train_blocs, \n",
    "                                                                                            closest_clusters = closest_clusters,\n",
    "                                                                                            train_chosen_actions = train_chosen_actions, \n",
    "                                                                                            train_id_list = train_id_list,\n",
    "                                                                                            is_debug=False,\n",
    "                                                                                            action_count=action_count, \n",
    "                                                                                            state_count=state_count)\n",
    "    # Constructing Transition Matrix(A, State1, State2)\n",
    "    total_actions:int = len(action_values_matrix)     \n",
    "    # Execute the function call\n",
    "    transition_mat, physician_policy = create_transition_matrix(num_actions = total_actions, \n",
    "                                                                num_states = state_count,ql_data_input = \n",
    "                                                                qlearning_dataset_train_final, \n",
    "                                                                transition_threshold = transition_threshold, \n",
    "                                                                reverse = False)\n",
    "    # Whether or not a patient in the training set actually lived or died, used for evaluation\n",
    "    answer_key=qlearning_dataset_train_final[qlearning_dataset_train_final['training_bloc'] == 1]\n",
    "    \n",
    "    # Normal value for iter_ql is 6, iter_wis is 750\n",
    "    # Q-Learning results is usually a positive value\n",
    "    # WIS Value should be Negative\n",
    "    Q_Equation, sum_Q_values = offpolicy_Q_learning_eval(\n",
    "            ql_train_set_Q=qlearning_dataset_train_final,\n",
    "            death_answer_key=answer_key,\n",
    "            gamma=0.99, \n",
    "            alpha=0.1,\n",
    "            numtraces=30000,\n",
    "            num_actions=total_actions,\n",
    "            num_clusters=state_count,\n",
    "            stopping_difference=0.001,\n",
    "            is_training=True,\n",
    "            is_random=False,\n",
    "            modulus_val=5000\n",
    "        )\n",
    "    qlearning_train_final = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy,\n",
    "                                                                             Q_Equation)\n",
    "    qlearning_train_final_no_weighting = construct_trained_model(ql_final_dataset=qlearning_dataset_train_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy,\n",
    "                                                                              Q_Equation)\n",
    "    evaluate_model_accuracy(qlearning_train_final=qlearning_train_final, test_name=\"Train, Weighted\")\n",
    "    evaluate_model_accuracy(qlearning_train_final=qlearning_train_final_no_weighting, test_name=\"Train, No Weighting\")\n",
    "    qlearning_dataset_test_final, _, _ = construct_prestate_matrix_train(train_90d = test_90d, \n",
    "                                                                     train_blocs = test_blocs, \n",
    "                                                                     closest_clusters = closest_clusters_test,\n",
    "                                                                     train_chosen_actions = test_chosen_actions,\n",
    "                                                                     train_id_list = test_id_list,\n",
    "                                                                     is_debug=False,\n",
    "                                                                     action_count=action_count, \n",
    "                                                                     state_count=state_count)\n",
    "    qlearning_dataset_test_final_weighted = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=True, physician_policy=physician_policy,\n",
    "                                                                             Q_Equation)\n",
    "    qlearning_dataset_test_final_nonweighted = construct_trained_model(ql_final_dataset=qlearning_dataset_test_final, \n",
    "                                                state_count=state_count, total_actions=total_actions, \n",
    "                                                weighted_probabilities=False, physician_policy=physician_policy,\n",
    "                                                                              Q_Equation)\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_weighted, test_name=\"Test, Weighted\")\n",
    "    evaluate_model_accuracy(qlearning_dataset_test_final_nonweighted, test_name=\"Test, No Weighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Train, Weighted\n",
      "Accuracy: 0.43967305111626204\n",
      "Living People Guessed Dead: 1003\n",
      "Dead People Guessed Living: 3590\n",
      "Total Guesses: 8197\n",
      "Alive People: 4098\n",
      "Dead People: 4098\n",
      "Correct Guesses: 3604\n",
      "Empty Paths: 6686\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_accuracy(qlearning_train_final=qlearning_train_final, test_name=\"Train, Weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_accuracy(qlearning_dataset_test_final_weighted, test_name=\"Test, Weighted\")\n",
    "evaluate_model_accuracy(qlearning_dataset_test_final_nonweighted, test_name=\"Test, No Weighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4098\n",
      "0    4098\n",
      "Name: mortality_90d, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(patientdata[train_flag][['icustayid', 'mortality_90d']].drop_duplicates()['mortality_90d'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
